# 오늘 내가 배운 것들(Today I Learned)

- 챗봇 수정 및 멀티스레드 멀티프로세스 탐구를 해보았다.

---

#### PagedAttention이 운영체제의 가상 메모리 페이징 방식에서 아이디어를 따왔다고 하는데 가상 메모리 페이징 방식이 뭔데?

- 운영체제(OS)는 프로그램이 실제 메모리(RAM)의 위치를 몰라도 되게 하기위해 **가상 메메모리(Virtual Memory)**를 사용함
- 하지만 메모리가 무한하지 않기 때문에, 가상 메모리를 효율적으로 실제 물리 메모리에 "대응”시키기 위해 페이지(Paging)기법을 쓴다

### 핵심 개념 요약

| **용어**         | **의미**                               |
| -------------- | ------------------------------------ |
| **가상 주소 공간**   | 프로그램이 사용하는 주소. 실제 RAM과는 별개           |
| **물리 주소 공간**   | 실제 RAM의 주소                           |
| **페이지(Page)**  | 가상 메모리를 **고정 크기** 블록(예: 4KB)으로 나눈 단위 |
| **프레임(Frame)** | 실제 물리 메모리를 **동일 크기**의 블록으로 나눈 단위     |
| **페이지 테이블**    | 가상 주소의 페이지를 물리 메모리의 프레임에 **매핑**하는 구조 |

###  비유를 해보자면:

- 책을 보는데, 책 전체를 한 번에 책상에 올릴 수 없어서 몇 쪽씩만 꺼내서 본다.
  - 전체 책: 프로그램 전체( 가상 메모리)
  - 책상: 실제 RAM (물리 메모리)
  - 한 번에 올릴 수 있는 몇 쪽: 페이지 단위(예:4kb)
  - 책깔피: 페이지 테이블 (어디 있는지 추적)

### 프로그램의 가상 주소와 물리 주소가 다를 수 밖에 없음

> → **가상 주소는 프로그램이 직접 “보는” 주소**,
> → **물리 주소는 실제 RAM에 “존재하는” 주소**

- 운영체제(OS)가 이 둘을 페이지 테이블을 통해 매핑(mapping)해주는 구조임.
- 왜냐? 모든 프로그램에 "독립적인 주소 공간"을 제공해 주려고

### 그럼 왜 매핑이 필요하냐?

- 가상주소는 논리적으로 연속적이지만
- 물리 주소는 조각나 있고 사용 중일 수 있음
- 그래서 가상 주소 공간을 물리 주소 공간에 "그대로” 올리는 건 거의 불가능함

\-> 그래서 페이지 기법으로 쪼개서, 가상 페이지 -> 물리 프래임으로 1:1 매핑

### 정리 하면:

- 사용자마다 입력(쿼리)이 다르고, 시쿼스 길이도 제각각 이니까 그에 맞춰서 Key/Value 캐시를 블록 단위로 유연하게 배치해서 메모리를 최적화 한게 PagedAttention임.
- 기존 LLM 방식:
  - 사용자마다 문장 길이가 다름
  - 생성이 언제 끝날지 모름(streaming 방식)
  - 그래서 KV캐시 공간을 고정 &  연속적으로 잡아야함 -> 낭비 및 파편화 발생
- 현재 vLLM 방식(PagedAttention):
  - KV 캐시를 고정 크기 블록(Page)단위로 쪼갬
  - 각 사용자 시퀀스는 필요한 만큼 Block을 동적으로 할당
  - 종료된 요청은 Block을 즉시 회수해서 재사용
  - Attention 연산 시 -> 필요한 Block만 찾아서 사용

| **효과**               | **설명**                                 |
| -------------------- | -------------------------------------- |
|  GPU 메모리 효율 향상      | 쓸데없는 캐시 미리 안 잡음. 필요한 만큼만 Block 사용      |
|  동시 사용자 수 증가        | 메모리 최적화 덕분에 더 많은 요청을 동시에 처리 가능         |
|  latency 감소         | Continuous batching과 함께 사용 시 효율적       |
|  streaming/chat에 유리 | 사용자마다 길이가 다르고 동적 생성이 일어나는 상황에서 매우 최적화됨 |

---

- curl 요청(/base-info)을 보내봤는데, 다음과 같이 출력돼었다.

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/9164CF54-DE46-4AD9-AB10-B716F1C6F6DB_2/nazn5CWxw5R8rjAbbx0xV8fpuIemiK3tHCcmHiOoH98z/Image.png)

- 내용도 내용이지만...넘 길다
- 프롬프트를 수정해보자
  - 아래 4,5번 추가 
        4. description은 반드시 한 문장으로만 작성하세요.(50자 이내)
        5. 전체 응답을 간결하게 유지하세요.
- 또한 토큰화가 단어가 아닌 문장으로 잡히는 이슈가 생겨서 아래와 같 조사를 추가

### 조사들 리스트

```python
'은', '는', '이', '가', '을', '를', '의', '에', '에서', '로', '으로', 
'와', '과', '도', '만', '부터', '까지', '나', '든지', '라도', '라서', 
'고', '며', '거나', '든가', '든'
```

- 주격 조사: 은, 는, 이, 가 (주어를 나타냄)
- 목적격 조사: 을, 를 (목적어를 나타냄)
- 소유격 조사: 의 (소유를 나타냄)
- 부사격 조사: 에, 에서, 로, 으로 (장소, 방향, 수단을 나타냄)
- 접속 조사: 와, 과, 나, 든지, 라도, 라서, 고, 며, 거나, 든가, 든 (문장을 연결)
- 보조사: 도, 만, 부터, 까지 (추가 의미를 나타냄)

### 결과

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/E32B1567-D018-4BFF-901B-FC6D3B5D13E3_2/yebotjrBFsjsr4yb13LUb2lNyS7qKtLbtR8u1sD0C5Mz/Image.png)

- 나쁘지 않다. 근데 답변 내용 수정이 필요해보인다.

---

- 근데 지금 각자 터미널 열어서 FastAPI 따로 vLLM 서버 따로 Redis 서버 따로 RQ 워커 따로 활성화 중이다….
  - 한번에 다 활성화 못시키나?
  - 혹시 프로세스가 달라서?
  - 그래서 알아봤다.

## 현재 아키텍처 분석

### 1. FastAPI 서버 (Uvicorn) - 멀티스레드

- 프로세스: 1개 (PID 67415)
- 스레드: 여러 개 (Uvicorn의 워커 스레드)
- 모델: vLLM 서버를 HTTP API로 호출
- 메모리: 공유 (같은 프로세스 내에서 스레드들이 메모리 공유)

### 2. 피드백 모델 - Redis + RQ 워커 (멀티프로세스)

- FastAPI 서버: 요청을 Redis 큐에 등록 → 즉시 202 응답
- RQ 워커: 별도 프로세스로 실행 (rq worker feedback)
- 모델: vLLM 서버를 HTTP API로 호출
- 메모리: 분리 (FastAPI와 RQ 워커는 다른 프로세스)

### 3. vLLM 서버 - 별도 프로세스

- 포트: 8800번
- 역할: 실제 모델 추론 담당
- 메모리: 독립적 (FastAPI, RQ 워커와 분리)

## 왜 이런 구조인가?

### Uvicorn (FastAPI) - 멀티스레드

```python
# 현재 실행 중인 프로세스
INFO:     Started server process [67415]  # 하나의 프로세스
INFO:     Uvicorn running on http://0.0.0.0:8000
```

- 장점: 메모리 공유, 빠른 통신
- 단점: CPU 바운드 작업 시 GIL(Global Interpreter Lock) 제약

### RQ 워커 - 멀티프로세스

```python
# 별도 터미널에서 실행
rq worker feedback  # 새로운 프로세스
```

- 장점: CPU 바운드 작업에 유리, 프로세스 격리
- 단점: 메모리 공유 불가, 프로세스 간 통신 오버헤드

### 근데 FastAPI는 멀티스레드이고 나머지는 왜 멀티프로세스임?

## FastAPI (Uvicorn) - 멀티스레드 선택 이유

1. I/O 바운드 작업에 최적화

```python
# 챗봇 요청 처리
async def get_llm_response(prompt: str, category: str):
    # HTTP 요청 (I/O 작업)
    with httpx.stream("POST", url, json=payload, timeout=60.0) as response:
        for line in response.iter_lines():  # 네트워크 I/O 대기
            yield {"event": "challenge", "data": data}
```

- 특징: 대부분의 시간을 네트워크 I/O 대기에 사용
- 멀티스레드 장점: I/O 대기 중 다른 스레드가 CPU 사용 가능 

2. 메모리 공유의 이점

```python
# 같은 프로세스 내에서 메모리 공유
shared_state = {}  # 모든 스레드가 공유
session_cache = {}  # 세션 정보 공유
```

- 장점: 세션 관리, 캐시 공유, 상태 관리 용이
- 비용: 메모리 공유로 인한 오버헤드 최소

3. 빠른 응답 시간

- 요구사항: 사용자가 실시간으로 응답을 받아야 함
- 멀티스레드: 컨텍스트 스위칭이 빠름 (마이크로초 단위)
- 결과: 100ms 이내 응답 가능

## RQ 워커 - 멀티프로세스 선택 이유

### 1. CPU 바운드 작업

```python
# 피드백 생성 (CPU 집약적)
def generate_feedback_task(data):
    # LLM 추론 (CPU/GPU 집약적)
    feedback_result = asyncio.run(feedback_model.generate_feedback(data))
    # JSON 파싱, 텍스트 처리 등
```

- 특징: 실제 계산 작업이 많음
- 멀티프로세스 장점: GIL(Global Interpreter Lock) 제약 없음

### 2. 장애 격리

```python
# 워커 프로세스가 죽어도 FastAPI 서버는 영향 없음
try:
    feedback_result = generate_feedback(data)
except Exception as e:
    # 워커만 재시작하면 됨
    print(f"워커 오류: {e}")
```

- 장점: 한 워커가 죽어도 다른 서비스에 영향 없음
- 복구: 워커만 재시작하면 됨

### 3. 긴 처리 시간

```python
# 피드백 생성은 10-30초 소요
# 사용자는 즉시 응답이 필요하지 않음
return JSONResponse(
    status_code=202,  # 즉시 202 응답
    content={"message": "피드백 요청이 접수되었습니다."}
)
```

- 특징: 백그라운드에서 천천히 처리
- 멀티프로세스: 안정적인 장시간 실행

## 실제 성능 비교

1. 멀티스레드 (FastAPI)

```python
# 동시 요청 처리
curl http://localhost:8000/ai/chatbot/recommendation/base-info  # 요청 1
curl http://localhost:8000/ai/chatbot/recommendation/base-info  # 요청 2
curl http://localhost:8000/ai/chatbot/recommendation/base-info  # 요청 3
# → 모두 동시에 처리 가능 (I/O 대기 시간 활용)
```

2. 멀티프로세스 (RQ 워커)

```python
# 여러 워커 실행
rq worker feedback  # 워커 1
rq worker feedback  # 워커 2 (새 터미널)
rq worker feedback  # 워커 3 (새 터미널)
# → 각각 독립적인 프로세스로 CPU 코어 활용
```

## 왜 이렇게 설계했나?

- 작업 특성에 따른 최적화하려고

| **구분** | **FastAPI (챗봇)** | **RQ 워커 (피드백)** |
| ------ | ---------------- | --------------- |
| 응답 시간  | 즉시 필요 (100ms)    | 지연 가능 (10-30초)  |
| 작업 유형  | I/O 바운드          | CPU 바운드         |
| 동시성    | 높음 (100+ 요청)     | 낮음 (10-20 요청)   |
| 장애 영향  | 전체 서비스 중단        | 부분 서비스 중단       |
| 메모리    | 공유 (효율적)         | 분리 (안정적)        |

- 즉, 각 작업 패턴에 따라서 다르게 설계함
  - FastAPI: 빠른 응답이 필요한 I/O 중심 작업-> 멀티스레드
  - RQ워커: 긴 시간이 필요한 CPU 중심 작업-> 멀티프로세스

### 실제 동작 시나리오

- 사용자가 동시요청 했을때:

```python
# 시간 0초: 3명의 사용자가 동시에 요청
User1: "환경 챌린지 추천해줘"  # 스레드 1 시작
User2: "비건 챌린지 추천해줘"  # 스레드 2 시작  
User3: "자전거 챌린지 추천해줘" # 스레드 3 시작

# 시간 0.1초: 각 스레드가 vLLM 서버에 요청
Thread1: POST http://localhost:8800/v1/chat/completions
Thread2: POST http://localhost:8800/v1/chat/completions  
Thread3: POST http://localhost:8800/v1/chat/completions

# 시간 0.2초: vLLM 서버가 3개 요청을 병렬 처리
vLLM: [User1 요청 처리 중] [User2 요청 처리 중] [User3 요청 처리 중]

# 시간 1초: 각각 스트리밍 응답 시작
Thread1: "생활용물품의" → "재가공이" → "쉬운" → ...
Thread2: "비건" → "식단" → "도전" → ...
Thread3: "자전거" → "타기" → "챌린지" → ...
```

### VLLM 서버의 병렬 처리 능력

- vLLM의 내부 구조  

```python
# vLLM 서버는 내부적으로 다음과 같이 동작
class vLLMServer:
    def __init__(self):
        self.model = load_model()  # GPU에 모델 로드
        self.request_queue = []    # 요청 큐
        self.workers = []          # 워커 스레드들
    
    def handle_request(self, request):
        # 여러 요청을 동시에 처리
        # GPU 메모리와 컴퓨팅 자원을 효율적으로 분배
        return streaming_response()
```

- 병렬 처리 방식
    1. 요청 큐잉: 들어오는 요청들을 큐에 저장
    2. 배치 처리: 여러 요청을 배치로 묶어서 GPU에서 병렬처리
    3. 스트리밍: 각 요청에 대해 토큰을 스트리밍으로 전송(여기서 나는 토큰화를 단어 별로 구분하도록 수정함)
    4. 리소스 관리: GPU 메모리와 컴퓨팅 자원을 분배했다ㅇㅇ
- 이전 "shared_model”방식의 문제점은 아무래도 같은 GPU 사용으로 인한 경합이다

```python
# shared_model.py - 모든 요청이 같은 모델 인스턴스를 공유
class SharedMistralModel:
    def __init__(self):
        self._model = AutoModelForCausalLM.from_pretrained(...)  # GPU에 직접 로드
        self._tokenizer = AutoTokenizer.from_pretrained(...)

# 문제: 동시 요청 시
User1: model.generate(inputs1)  # GPU 메모리 사용
User2: model.generate(inputs2)  # 같은 GPU 메모리에서 경합 발생!
User3: model.generate(inputs3)  # OOM 발생!
```

- 왜 OOM이 발생했었냐면
    1. 메모리 경합: 여러 스레드가 동시에 같은 GPU 메모리 사용
    2. KV캐시 중복: 각 요청마다 별도의 KV 캐시가 필요하지만 공유 불가
    3. 배치 처리 불가: 순차적으로만 처리 가능
    4. 메모리 단편화: 연속 요청으로 인한 메모리 단편

### 병렬처리 vs 멀티스레드

- 병렬처리(Parallel Processing)

```python
# 여러 작업을 동시에 실행
# CPU 코어나 GPU 코어를 동시에 활용

# 예시: vLLM의 배치 처리
batch = [request1, request2, request3, request4]
# GPU에서 4개 요청을 동시에 처리
outputs = model.generate_batch(batch)  # 진짜 병렬 처리
```

- 멀티스레드

```python
# 여러 스레드가 번갈아가며 실행
# I/O 대기 시간을 활용한 동시성

# 예시: FastAPI의 요청 처리
Thread1: HTTP 요청 처리 → I/O 대기 → 응답 전송
Thread2: HTTP 요청 처리 → I/O 대기 → 응답 전송  
Thread3: HTTP 요청 처리 → I/O 대기 → 응답 전송
# 실제로는 번갈아가며 실행 (concurrent, not parallel)
```

- 이전 방식에서는

```python
# FastAPI 멀티스레드로 요청은 받지만
Thread1: model.generate() → GPU 메모리 부족으로 대기
Thread2: model.generate() → GPU 메모리 부족으로 대기
Thread3: model.generate() → OOM 발생!

# 결과: 순차 처리 (병렬 처리 불가)
```

- 현재 방식(vLLM)은

```python
# FastAPI 멀티스레드로 요청 받음
Thread1: HTTP 요청 → vLLM 서버로 전송
Thread2: HTTP 요청 → vLLM 서버로 전송
Thread3: HTTP 요청 → vLLM 서버로 전송

# vLLM 서버에서 진짜 병렬 처리
vLLM: [Request1][Request2][Request3] → GPU에서 "배치" 처리
```

## 핵심 차이점

### 이전 방식

- 요청 처리: 멀티스레드 (FastAPI)
- 모델 추론: 순차 처리 (GPU 메모리 부족)
- 결과: 동시성은 있지만 병렬성 없음

### 현재 방식

- 요청 처리: 멀티스레드 (FastAPI)
- 모델 추론: 병렬 처리 (vLLM의 PagedAttention)
- 결과: 동시성 + 병렬성 모두 있음

---

- 스레드란 무엇일까?
  - CPU에 있는건가?
  - 아니다. 스레드는 메모리에 있다!
    - 스레드: 메모리(RAM)에 존재하는 실행 단위
    - CPU: 스레드를 실행하는 하드웨어
    - 관계: CPU가 메모리에 있는 스레드를 번갈아가며 실행