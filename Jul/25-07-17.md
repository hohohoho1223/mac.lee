# 오늘 내가 배운 것들(Today I Learned)

- 기존 모델(파인튜닝x) 테스트 도중, 아래와 같이 스트리밍이 출력되는 것을 확인 

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/F5551760-524A-49F0-829E-EAB01E018DA6_2/edmzryQIockSkKOLH0zmrmx0SOQ54YRiWQgT5Qc9l5wz/Image.png)

- 사용자에게 다 뜬다ㅇㅇ
- “json” & “recommend” 출력 검거!!

### 원인?

- LLM의 출력 구조
  - LLM 프롬프트에서 “아래 JSON형식으로 츨력하라고 시키면, 모델이 실재로 아래처럼 한글자/한단어씩 생성 
  - 이때 모델이 "json", "recommend" 등 필드명을 한 글자씩 생성해서 스트리밍으로 보내는 중간에 바로바로 클라이언트로 전달
- SSE 스트리밍 로직
  - get_llm_response 함수에서 토큰이 들어올 때마다 아래처럼 "정제"해서 바로바로 내보냄:

```python
  cleaned_text = word
  cleaned_text = re.sub(r'"(recommend|challenges|title|description)":\s*("|\')?', '', cleaned_text)
```

- 하지만 이 정제 로직이 토큰이 완전히 모이기 전에 "json", "recommend" 같은 단어가 단독으로 들어오면 그냥 그대로 내보내는 구조임
- 실제로 발생하는 현상
  - LLM이 "recommend"라는 필드명을 생성할 때 ", r, e, ... d, " 이런 식으로 한 글자씩 토큰이 들어오고 단어 경계에서 "recommend"가 한 번에 들어오면 정제 로직이 아직 완성된 JSON 구조가 아니기 때문에 "recommend"만 따로 내보내는 경우가 생김

---

### vLLM 파인튜닝 모델 서빙

- 명령어:

```plaintext
(.venv) ubuntu@leafresh-ai-chatbot:~/15-Leafresh-AI/Text/LLM$ python3 -m vllm.entrypoints.openai.api_server     --model /home/ubuntu/mistral_finetuned/models--maclee123--leafresh_merged_v1/snapshots/bbee3805be9f9c26a211c8baa2d3339f75f9c9b2     --host 0.0.0.0     --port 8800     --max-model
-len 8192     --gpu-memory-utilization 0.9
```

- 근데 에러남.

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/14DC43FC-5391-46B6-B994-0067A19A548A_2/z45UOkt87RqFBG1BtC9y4uaMAsxBnWYKc7DZy1xLPEUz/Image.png)

- 원인을 좀더 명확히 찾아보자

> transformers로 정상 로드되는지 테스트

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("/home/ubuntu/mistral_finetuned/models--maclee123--leafresh_merged_v1/snapshots/bbee3805be9f9c26a211c8baa2d3339f75f9c9b2")
tokenizer = AutoTokenizer.from_pretrained("/home/ubuntu/mistral_finetuned/models--maclee123--leafresh_merged_v1/snapshots/bbee3805be9f9c26a211c8baa2d3339f75f9c9b2")
```

    - 출력 결과

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/A47B5115-5E49-4E8F-BD81-F292C1A5996A_2/vHhx5XX6xNTmP53npL4xFWUe2pqvyNBhtmk9UKhIB7oz/Image.png)

- ....?
- 아래는 핵심 메세지이다.

```plaintext
Some weights of the model checkpoint at ... were not used when initializing MistralForCausalLM: [...]
Some weights of MistralForCausalLM were not initialized from the model checkpoint ... and are newly initialized: [...]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

### 무슨 뜻인가?

- 모델 체크포인트에 LoRA/PEFT 관련 weight(lora_A, lora_B, base_layer.weight)가 들어 있음
- 하지만 MistralForCausalLM은 이 weight를 사용하지 않음(즉, LoRA/PEFT 구조가 합쳐지지 않았거나, merge 과정이 불완전)

- 반대로, 필수 weight(q_proj.weight, v_proj.weight)는 체크포인트에 없어서 새로 초기화됨
- 즉, 실제 모델 파라미터가 완전히 일치하지 않음

- 모델이 "부분적으로만" 로드되고, 나머지는 랜덤 초기화됨(정상적인 추론 불가) 
  - vLLM에서는 아예 KeyError로 죽음

### 왜 이런현상이 생겼나?

- LoRA/PEFT 파인튜닝 후, merge_and_unload(합치기) 과정이 제대로 안 됨
- 또는, merge 없이 LoRA adapter weight만 저장된 모델을 그대로 사용
- 또는, transformers 버전/merge 방식이 vLLM이 기대하는 구조와 다름

> 누가봐도 내가 파인튜닝을 잘못한것 같다ㅇㅇ

### 문제의 핵심

1. merge방식

```python
state_dict = get_peft_model_state_dict(model)
model.base_model.model.load_state_dict(state_dict, strict=False)
model = model.base_model.model
model.save_pretrained(drive_save_path, safe_serialization=False)
```

- 이 방식은 LoRA adapter의 weight만 base model에 "덮어쓰기" 하는 방식임
- 하지만, LoRA 구조의 특수 파라미터(lora_A, lora_B, base_layer.weight 등)가 checkpoint에 남아있을 수 있음
- transformers에서는 warning만 뜨고 넘어가지만, vLLM은 "정확히 일치하는 weight만" 기대하기 때문에 KeyError가 남
    1. 정석적인 merge 방법
        - PEFT의 merge_and_unload()를 써야 LoRA 구조가 완전히 base model에 합쳐지고, checkpoint에 LoRA 관련 파라미터가 남지 않는다.

### 해결 방법: merge_and_unload()사용

- 올바른 코드 예시:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

base_model_path = "mistral-7b-instruct"
adapter_path = "./lora_adapter"
save_path = "./merged_model"

# 1. base model 로드
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype="auto",
    device_map="auto"
)

# 2. LoRA adapter 적용
lora_model = PeftModel.from_pretrained(base_model, adapter_path)

# 3. merge_and_unload()로 완전히 합치기
merged_model = lora_model.merge_and_unload()

# 4. 저장
merged_model.save_pretrained(save_path, safe_serialization=False)
tokenizer = AutoTokenizer.from_pretrained(base_model_path)
tokenizer.save_pretrained(save_path)
```

- 반드시 merge_and_unload()를 사용해야 함 → 이러면 LoRA 관련 파라미터가 checkpoint에 남지 않고, 완전히 "순수"한 base model 구조로 저장
- transformers, vLLM 모두에서 warning 없이 clean하게 로드됨ㅇㅇ

### 요약

- 기존 merge 방식(get_peft_model_state_dict + load_state_dict)은 LoRA 특수 파라미터가 checkpoint에 남아서 vLLM에서 KeyError 발생
- PEFT의 merge_and_unload()를 써서 완전히 합친 모델을 저장해야 vLLM에서 정상적으로 로드

---

### 파인튜닝 vLLM 모델 적재 

- vLLM에서 `TypeError: expected str, bytes or os.PathLike object, not NoneType 에러 발생`

### 문제점

- vLLM 실행 시 토크나이저 로딩에서 TypeError 발생

### 원인 분석

- tokenizer_config.json에 "tokenizer_file" 또는 "vocab_file" 항목이 없어서, transformers/vLLM이 tokenizer.model 파일을 자동으로 못 찾음

### 해결책

- tokenizer_config.json에 반드시 "tokenizer_file": "tokenizer.model" 또는 "vocab_file": "tokenizer.model" 추가

```json
"bos_token": "<s>",
  "chat_template": "{%- if messages[0][\"role\"] == \"system\" %}\n    {%- set system_message = messages[0][\"content\"] %}\n    {%- set loop_messages = messages[1:] %}\n{%- else %}\n    {%- set loop_messages = messages %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n{%- set user_messages = loop_messages | selectattr(\"role\", \"equalto\", \"user\") | list %}\n\n{#- This block checks for alternating user/assistant messages, skipping tool calling messages #}\n{%- set ns = namespace() %}\n{%- set ns.index = 0 %}\n{%- for message in loop_messages %}\n    {%- if not (message.role == \"tool\" or message.role == \"tool_results\" or (message.tool_calls is defined and message.tool_calls is not none)) %}\n        {%- if (message[\"role\"] == \"user\") != (ns.index % 2 == 0) %}\n            {{- raise_exception(\"After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\") }}\n        {%- endif %}\n        {%- set ns.index = ns.index + 1 %}\n    {%- endif %}\n{%- endfor %}\n\n{{- bos_token }}\n{%- for message in loop_messages %}\n    {%- if message[\"role\"] == \"user\" %}\n        {%- if tools is not none and (message == user_messages[-1]) %}\n            {{- \"[AVAILABLE_TOOLS] [\" }}\n            {%- for tool in tools %}\n                {%- set tool = tool.function %}\n                {{- '{\"type\": \"function\", \"function\": {' }}\n                {%- for key, val in tool.items() if key != \"return\" %}\n                    {%- if val is string %}\n                        {{- '\"' + key + '\": \"' + val + '\"' }}\n                    {%- else %}\n                        {{- '\"' + key + '\": ' + val|tojson }}\n                    {%- endif %}\n                    {%- if not loop.last %}\n                        {{- \", \" }}\n                    {%- endif %}\n                {%- endfor %}\n                {{- \"}}\" }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- else %}\n                    {{- \"]\" }}\n                {%- endif %}\n            {%- endfor %}\n            {{- \"[/AVAILABLE_TOOLS]\" }}\n            {%- endif %}\n        {%- if loop.last and system_message is defined %}\n            {{- \"[INST] \" + system_message + \"\\n\\n\" + message[\"content\"] + \"[/INST]\" }}\n        {%- else %}\n            {{- \"[INST] \" + message[\"content\"] + \"[/INST]\" }}\n        {%- endif %}\n    {%- elif message.tool_calls is defined and message.tool_calls is not none %}\n        {{- \"[TOOL_CALLS] [\" }}\n        {%- for tool_call in message.tool_calls %}\n            {%- set out = tool_call.function|tojson %}\n            {{- out[:-1] }}\n            {%- if not tool_call.id is defined or tool_call.id|length != 9 %}\n                {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n            {%- endif %}\n            {{- ', \"id\": \"' + tool_call.id + '\"}' }}\n            {%- if not loop.last %}\n                {{- \", \" }}\n            {%- else %}\n                {{- \"]\" + eos_token }}\n            {%- endif %}\n        {%- endfor %}\n    {%- elif message[\"role\"] == \"assistant\" %}\n        {{- \" \" + message[\"content\"]|trim + eos_token}}\n    {%- elif message[\"role\"] == \"tool_results\" or message[\"role\"] == \"tool\" %}\n        {%- if message.content is defined and message.content.content is defined %}\n            {%- set content = message.content.content %}\n        {%- else %}\n            {%- set content = message.content %}\n        {%- endif %}\n        {{- '[TOOL_RESULTS] {\"content\": ' + content|string + \", \" }}\n        {%- if not message.tool_call_id is defined or message.tool_call_id|length != 9 %}\n            {{- raise_exception(\"Tool call IDs should be alphanumeric strings with length 9!\") }}\n        {%- endif %}\n        {{- '\"call_id\": \"' + message.tool_call_id + '\"}[/TOOL_RESULTS]' }}\n    {%- else %}\n        {{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}\n    {%- endif %}\n{%- endfor %}\n",
  "clean_up_tokenization_spaces": false,
  "eos_token": "</s>",
  "legacy": false,
  "model_max_length": 1000000000000000019884624838656,
  "pad_token": null,
  "sp_model_kwargs": {},
  "spaces_between_special_tokens": false,
  "tokenizer_class": "LlamaTokenizer",
  "unk_token": "<unk>",
  "tokenizer_file": "tokenizer.model",
  "use_default_system_prompt": false
}
```

- tokenizer_config.json 파일 안에
- "tokenizer_file" 또는 "vocab_file" 항목에 들어가는 토크나이저 모델 파일의 경로를 의미

### 왜 이 경로가 중요할까?

- transformers나 vLLM이 토크나이저를 로드할 때 `tokenizer_config.json` 의 "tokenizer_file" 또는 "vocab_file" 값을 참고해서 해당 경로에 있는 파일을 읽어 토크나이저를 초기화 함
- 이 값이 없거나, 잘못된 경로(예: 오타, None, 실제 파일 없음)면 "expected str, bytes or os.PathLike object, not NoneType" 같은 에러가 발생함

### 결론

- 여기서의 경로란?

  - tokenizer_config.json에서 "tokenizer_file" 또는 "vocab_file"에 명시하는 토크나이저 모델 파일의 상대경로(파일명) 이다.
  - 실제 파일이 해당 폴더에 존재해야 하며, 파일명과 정확히 일치해야 함

---

### 근데 또 오류남…

- vLLM 서버는 실행됨
- 근데 vLLM 서버가 켜진 상태에서 curl 요청을 보내자마자 GPU에서 "device-side assert"가 터지는 상황

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/EFD306B7-C2D8-4A20-BD47-84729FD217BF_2/fpaRPgGj4HOFsA3Qw3pLlMEJDWb53uSdy7kaTTQrKA4z/Image.png)

> 미치겠다ㅠㅠ

# 오류의 핵심 요약

- CUDA error: device-side assert triggered
- vLLM 내부에서 unified_attention_with_output → flash_attn_varlen_func → CUDA 커널에서 에러
- 로그 상에서 index out of bounds: 0 <= tmp5 < 32768 등 인덱스 범위 초과가 반복적으로 발생

## 이런 에러가 발생하는 대표적 원인

### (1) 잘못된 입력 토큰

- 프롬프트가 토크나이저에 맞지 않거나,
- 토크나이저/모델이 mismatch(예: base 모델과 tokenizer가 다름)
- tokenizer.model이 파인튜닝 모델과 정확히 일치하지 않음

### (2) 모델/토크나이저 파일 불일치

- base 모델과 tokenizer가 서로 다른 버전/구조
- 파인튜닝 후 merge 과정에서 tokenizer를 잘못 덮어씀

### (3) vLLM/transformers/torch 버전 호환성

- vLLM, transformers, torch, flash-attn 등 버전 mismatch

### (4) 입력 prompt가 너무 길거나, 이상한 값

- max_seq_len, max_model_len 등 설정이 모델보다 큼
- prompt에 이상한 문자가 들어감

### 그럼 하나씩 확인해보자!

1. transformers로 직접 inference 해보기
    - 파인튜닝/merge된 모델 폴더에서 아래 코드 실행:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("/home/ubuntu/mistral_finetuned/models--maclee123--leafresh_merged_v1/snapshots/0fe572bd6dccfb84946e37fb253ccea74dff2599")
tokenizer = AutoTokenizer.from_pretrained("/home/ubuntu/mistral_finetuned/models--maclee123--leafresh_merged_v1/snapshots/0fe572bd6dccfb84946e37fb253ccea74dff2599")
out = model.generate(**tokenizer("안녕", return_tensors="pt").to(model.device))
print(tokenizer.decode(out[0]))
```

- 결과 확인

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/EDFB10B9-C2AA-42BC-BABB-1F39AFF53A73_2/RIsdblOBbTjJq40qk8MJClL0p1tya3nI6LLkfcFo56wz/Image.png)

- transformers로 직접 모델 로드 + inference가 정상적으로 동작한다는 건
- 모델과 토크나이저 파일이 완벽하게 일치한다는 뜻

## 그런데 vLLM만 device-side assert가 나는 이유?

- 이제 원인은 vLLM 환경/옵션/버전/입력 파라미터 쪽에 있다

### 원인찾음

- 원본 `config.json` 파일을 안 덮어씌움

+ config.json

```json
{
  "architectures": [
    "MistralForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 32768,
  "model_type": "mistral",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-05,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.42.0.dev0",
  "use_cache": true,
  "vocab_size": 32768
}

```

## 왜 config/tokenizer를 base와 동일하게 해야 하나?

### A. LoRA merge의 본질

- LoRA는 "가중치만" 수정하는 방식
- 모델 구조(config), 토크나이저는 그대로 유지
- merge_and_unload()는 weight만 base에 합치는 것

### B. vLLM의 엄격한 요구사항

- vLLM은 GPU 커널에서 정확한 구조 정보를 요구
- config.json의 vocab_size, hidden_size, num_attention_heads 등이 weight 파일과 1:1로 일치해야 함
  - 조금이라도 다르면 device-side assert 발생

### C. transformers vs vLLM 차이

- transformers: 내부적으로 fallback/에러 무시 가능
- vLLM: GPU 커널에서 엄격하게 체크 → 조금이라도 다르면 바로 죽음

---

### config/tokenizer가 무슨역할 하는데? 파라미터 구성?

- ㅇㅇ
- config/tokenizer 파일들은 모델의 "구조와 동작 방식"을 정의하는 파라미터 구성임

### config.json의 역할

- 모델 구조 파라미터

```json
{
  "vocab_size": 32000,        // 어휘 크기
  "hidden_size": 4096,        // 은닉층 크기
  "num_attention_heads": 32,  // 어텐션 헤드 수
  "num_hidden_layers": 32,    // 레이어 수
  "intermediate_size": 14336, // FFN 크기
  "max_position_embeddings": 32768  // 최대 위치 임베딩
}
```

- 이 값들이 GPU에서:
  - 메모리 할당: hidden_size × num_layers 등으로 GPU 메모리 계산
  - 텐서 shape: weight 파일의 shape와 1:1로 맞아야 함
  - 연산 구조: attention, FFN 등 각 레이어의 연산 방식 결정

## tokenizer 파일들의 역할

- tokenizer.model (SentencePiece 모델)
  - 텍스트 → 토큰 변환 규칙: "안녕하세요" → [1, 234, 567, 89]
  - 어휘 사전: 각 토큰 ID가 어떤 의미인지 정의
  - 특수 토큰: <s>, </s>, <pad>, <unk> 등

```json
{
  "tokenizer_file": "tokenizer.model",
  "bos_token": "<s>",
  "eos_token": "</s>",
  "pad_token": "<pad>",
  "chat_template": "..."
}
```

- special_tokens_map.json
  - 특수 토큰 매핑: 토큰 ID와 실제 문자열 연결

## vLLM에서 왜 이게 중요하나?

- GPU 커널에서 엄격한 체크

```plaintext
// vLLM 내부에서 이런 식으로 체크
if (token_id >= vocab_size) {
    // device-side assert 발생!
    assert(false, "index out of bounds");
}
```

- 실제 예시
  - config.json: "vocab_size": 32000
  - 실제 토큰 ID: 35000 (vocab_size 초과)
  - 결과: GPU에서 index out of bounds → device-side assert

## LoRA merge와의 관계

- LoRA는 weight만 수정

```plaintext
Base Model Weight + LoRA Weight = Merged Weight
     ↓
config/tokenizer는 그대로 유지해야 함
```

- 이유는?
  - LoRA: 특정 weight만 fine-tuning
  - config/tokenizer: 모델 구조/토크나이저 규칙 (변경 안 함)
  - merge: weight만 합치고, 구조는 base와 동일
  - 정리하면 아래와 같다

| **파일**                  | **역할**     | **vLLM에서의 중요성**      |
| ----------------------- | ---------- | -------------------- |
| config.json             | 모델 구조 파라미터 | GPU 메모리/텐서 shape 계산  |
| tokenizer.model         | 토큰화 규칙     | 텍스트→토큰 변환            |
| tokenizer_config.json   | 토크나이저 설정   | 특수 토큰, chat template |
| special_tokens_map.json | 특수 토큰 매핑   | 토큰 ID ↔ 문자열          |

- 즉, config/tokenizer는 "모델이 어떻게 동작할지"를 정의하는 설계도이고, vLLM은 이 설계도가 weight 파일과 1:1로 맞아야 GPU에서 안전하게 동작하므로 체크하는것!ㅇㅇ