# 오늘 내가 배운 것들(Today I Learned)

- 피드백 모델을 다시 vLLM 싱글톤으로 전환하였다.
- 크롤러 기능 수정하였다.

---

- 피드백 모델을 다시 vLLM 싱글톤으로 전환하였다.
- 처음에 왜 이렇게 모델을 분리해서 동시에 메모리에 적재하려했냐면:
    1. 챗봇 의 답변 구조화를 파인튜닝으로 실현하려했다.
        - 각 친환경 챌린지 추천 시 SSE 스트리밍 도중 답변 줄바꿈 구현
    2. 따라서 챗봇한테만 vLLM적용, 피드백 모델은 기존의 `shared_model.py` 파일에서 
        - FeedbackModel이 직접 로컬에서 4비트 양자화된 Mistral 모델을 메모리에 올림
        - `shared_model.py에서` 싱글톤으로 모델/토크나이저를 관리
        - generate_feedback에서 직접 토크나이저/모델로 inference
        - RQ 워커(task.py)도 shared_model을 import해서 같은 방식으로 직접 모델을 메모리에 올림
- 하지만 그에 따른 문제점들이 있었다.

### 문제점 

- 챗봇 SSE-streaming, 답변 구조화 파인튜닝 등 다양한 실험을 위해 두 모델을 동시에 적재하려 했으나, requirements.txt의 torch/transformers/accelerate 등 패키지 버전과 vLLM이 요구하는 버전이 달라 **심각한 패키지 충돌**이 발생.
- vLLM 설치 시 torch를 먼저 설치해야 하고, vLLM이 요구하는 버전과 기존 코드가 요구하는 버전이 다르면 pip가 충돌을 일으킴.
- vLLM 서버 실행 시 --gpu-memory-utilization, --max-model-len 등 옵션을 조정해도 메모리 적재 실패 및 환경 충돌이 반복됨.

### 구조 통합 및 vLLM 싱글톤 전환

- 위 문제로 인해 **최종적으로 vLLM 기반 싱글톤 구조(통합 모델 서버)로 전환**
- 챗봇과 피드백 모두 vLLM 서버(OpenAI 호환 API) 하나만 사용하도록 통합
- `shared_model.py` 및 로컬 4비트 양자화 모델 로딩 코드는 완전히 폐기
- 모든 AI 기능(챗봇/피드백)은 HTTP API를 통해 vLLM 서버에 요청하는 구조로 일원화

### GCP Pub/Sub 기반 비동기 메시지 처리 도입

- 기존에는 피드백 생성 비동기 처리를 위해 Redis 큐(RQ) 기반 구조만 사용했음
- **GCP Pub/Sub를 도입하여 피드백 요청/결과 메시지의 비동기 처리를 구현**
- Pub/Sub 도입 이유:
    1. 예외처리 및 누락된 메세지 큐 재처리 
    2. GCP Pub/Sub는 메시지 영속성, 재전송, 구독자 분리, 클라우드 네이티브 연동 등에서 강점
    3. BE(백엔드)와 AI 워커가 서로 다른 프로젝트/환경에 분산되어 있어, Pub/Sub를 통한 메시지 브로커 구조가 더 적합

### 통신 성공

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/519A64F7-9E5F-4660-866D-27882B8C22B2_2/r9214xIpnuxXhoG8HstgRTqlYbHyRJtd67YDAFNyIP8z/Image.png)

---

- `.gitignore` 파일에 pids/, logs/, *.pid, *.log 항목 추가

### env파일 변경!!!!

+ .env(dev용)
> 중요정보가 있어 생략.(메모장에 따로 존재)

---

### 크롤러 이걸 어떻게 해야할까?

- 현재는 단순히 스크립트로 실행되는 구조임

> `python generate_challenge_docs.py`처럼 직접 실행하면 크롤링 및 챌린지 문서 생성이 동작

### 서버에서 작동되게 하려면?

1. Flask/FastAPI 등으로 API 엔드포인트를 만들어서 요청 시 크롤링/생성 실행
2. 백그라운드 데몬/서비스로 주기적으로 실행

> 예: cron, systemd, supervisor 등으로 주기적 실행

### 차이점 요약

| **방식**            | **언제 실행?** | **누가 실행?**  | **예시 용도**      |
| ----------------- | ---------- | ----------- | -------------- |
| Flask/FastAPI API | 요청이 올 때마다  | 외부(사람/프로그램) | 필요할 때만, 수동 트리거 |
| 데몬/주기적 실행         | 정해진 시간마다   | 자동          | 뉴스/데이터 자동 수집   |

### 그럼 API 서버(Flask/FastAPI)앱이 시작될 때, 크롤러를 한 번 실행하는 코드만 추가하면 어떨까?

- 어차피 서버는 일 8시간만 가동되기 떄문에ㅇㅇ
- 서버 가동시킬 떄, 크롤러 한번 실행하도록 

## 그래서 API 엔드포인트 지정함ㅇㅇ

- 서버 시작 시 크롤러가 자동으로 실행됨 (백그라운드 스레드)
- `POST /ai/crawler/run` - 크롤러 수동 실행 가능

```Bash
curl -X POST http://localhost:8000/ai/crawler/run
```