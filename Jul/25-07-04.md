# 오늘 내가 배운 것들(Today I Learned)

## 현재 시스템 구조(vLLM 적용)

- `ps aux | grep python` : 각 프로세스 상세 정보 확인

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/4E225D9E-793D-4093-96D0-16DF3259982D_2/pyavHNuGehmi2ESryXUKHl2Vfamk6nE8Zrseggeo5eAz/Image.png)

- 아래는 해당 프로세스들 간 시스템 구조도 이다.

```python
시스템 구조:
├── FastAPI 서버 (uvicorn) - 포트 8000
├── vLLM API 서버 - 포트 8800
│   ├── 메인 프로세스 (PID 228479)
│   ├── 리소스 트래커 (PID 228818)
│   └── 워커 프로세스 (PID 228819) - 실제 추론
└── RQ Worker - feedback 큐 처리
```

## 현재 GPU메모리 사용량(nvidia-smi 기준)

> vLLM_원본 모델(양자화x) 로드

1. PID 225090 - FastAPI 서버 (uvicorn)
    - GPU 메모리: 324MiB
    - 이는 정상적인 웹 서버 메모리 사용량
2. PID 228819 - vLLM 워커 프로세스
    - GPU 메모리: 21,004MiB (약 20.5GB)
    - 이는 Mistral-7B 모델이 GPU에 로드되어 있음을 의미

## 메모리 사용량 분석

- 총 GPU 메모리 사용량: 약 20.8GB
  - FastAPI 서버: 324MB (1.5%)
  - LLM 모델: 20.5GB (98.5%) 

## 각 상황별 메모리 사용량 비교

> 시스템 RAM 확인은 `free -h` 명령어 실행

1. **원본 Mistral 7B모델 적재**
    - 16FP 적용
    - 4비트 양자화 미적용

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/A2DC9264-932A-4C2D-82A4-A43B413BB4AF_2/Rj9EiqSL9ADwwk80QDDybWnL8xHgaVud1QZYd9hoJpYz/Image.png)

- GPU RAM: 14.6GB 사용 중 / 23GB 총 용량
- 시스템 RAM: 2.3GB 사용 중/50GB 총 용량
- 뭐야 OOM(Out Of Memory)가 안나네?
- ㅇㅋ 그럼 curl로 동시요청(서로다른 sessionId 4개) 해봄
  > 오류남ㅋ

- 그래서 아래와 같이 동시 요청 처리를 위해 설정 추가해 보았다.

```python
# 동시 요청 처리를 위한 설정
self._model.config.pretraining_tp = 1  # 텐서 병렬화 비활성화
self._model.config.rope_scaling = None  # RoPE 스케일링 비활성화
```

1. pretraining_tp = 1
    - 텐서 병렬화(Tensor Parallelism) 비활성화
    - 기본값: 3보통 2나 4 (여러 GPU에 모델을 분산)
    - 문제: 텐서 병렬화가 활성화되면 여러 GPU 간 동기화가 필요한데, 단일 GPU에서는 불필요하고 오히려 동시성 문제를 일으킬 수 있음
    - 해결: 1로 설정해서 단일 GPU에서 단순하게 처리

### 2. rope_scaling = None

- RoPE(Rotary Position Embedding) 스케일링 비활성화
- 문제: RoPE 스케일링이 활성화되면 위치 인코딩을 동적으로 조정하는데, 여러 요청이 동시에 들어오면 이 값들이 꼬일 수 있음
- 해결: None으로 설정해서 기본 RoPE만 사용
- 다시 실행 결과 

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/C5865177-5898-4880-BE51-06DECE79CF1C_2/4CxLxwIbI3EPBoXAESpmqSIAtpcJ5AxA2c6cNbzifdkz/Image.png)

- ㅋㅋ OOM(Out Of Memory)은 안나지만 내부 로직이 터져 버린듯(이상한 말 내뱉음)

#### WHY?

- 여러 요청이 동시에 들어오면 모델 내부 상태가 꼬이거나, 토크나이저/모델이 스레드 세이프하지 않아서 응답이 완전히 깨지는 현상
  - 이건 float16(비양자화)든 4비트 양자화든 싱글턴 모델을 여러 스레드에서 동시에 generate()로 돌릴 때 발생하는 전형적인 현상
- Huggingface Transformers의 generate()는 기본적으로 스레드 세이프하지 않음.

> 스레드 세이프란?
> 여러 스레드가 동시에 같은 객체/함수를 사용해도 안전한 것\

- 현재 상황: 여러 요청이 동시에 `model/generate()` 를 호출하면 모델 내부 상태가 꼬여서 이상한 결과 나온듯
  - 여러 요청이 동시에 들어오면 내부 버퍼, 캐시, 토크나이저 상태가 꼬여서 이상한 결과가 나옴.
  - 특히 Python의 GIL과 별개로, GPU 연산은 병렬로 들어가도 모델 객체가 공유되면 이런 현상이 자주 발생.

    ### 해결책이 세마포어? 큐?

    - 세마포어: 동시에 N개 까지만 실행 허용(N=1이면 락과 같음)
    - 큐: 요청을 줄세워서 하나씩 처리

    ### 이러면 챗봇 사용자가 답변 받으려면 줄을 서야 하는 상황..ㅋ

    - 모델 복사본?
    > 되겠냐? 하나 올려도 14GB인데?
    - GPU 메모리 분할?
    > 되겠냐? 하나 올려도 14GB인데?

1. **4비트 양자화**
    - 16fp 적용
    - `quantization_config` 설정:

```python
quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,  # 4비트 양자화 활성화
                bnb_4bit_compute_dtype=torch.float16,  # 계산은 16비트로 수행
                bnb_4bit_use_double_quant=True,  # 이중 양자화로 메모리 추가 절약
                bnb_4bit_quant_type="nf4"  # Normalized Float 4-bit
            )
```

- 아래는 메모리 사용량
- 기존 원본 모델 메모리보다는 훨씬 적은 메모리가 적재 된것을 확인.

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/0CC55B76-93AF-4CDC-9B45-B8C5A3202DB0_2/SDrwijoabQct3ueA6PngyZo0k8VzdxjahZ4QHFiuFbIz/Image.png)

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/D6A54089-BCCF-4C1B-8D31-22DFE5DA7BAD_2/5GhFAFDySuKhrbaIqbYKsnFJKnL4GxvMjgceHV9zDO8z/Image.png)

- 근데 아무리 양자화 해서 메모리 최적화 해도 뭐하나
  - 여러 요청이 동시에 들어오는대로 각 요청마다 모델 인스턴스(복사본) 못 만드는 순간서로 상태가 꼬임
  - 락을 걸면 안전하지만, 결국 "순차 처리"만 가능

---

### 프로세스 작동 구조 변화

```plaintext
[과거 구조]
FastAPI ─┬─> 모델(GPU)
rq 워커 ─┘─> 모델(GPU)
(= GPU에 모델 여러 번 올라감)

[지금 구조]
FastAPI ─┬─> [vLLM 서버] ─> 모델(GPU)
rq 워커 ─┘─> [vLLM 서버] ─> 모델(GPU)
(= GPU에 모델 한 번만 올라감, 모두 vLLM 서버에 HTTP로 요청)
```

- 여기서 의문이 들었다.
- vLLM 적용시 FastAPI, rq 워커 모두 모델을 직접 로드 하지 않고, HTTP API로 vLLM서버에 요청만 보내게 되면서 vLLM 서버에서만 GPU에 모델을 한번만 올려서 모든 요청 처리한다는데, **애초에 rq 워커가 FastAPI에서 모델 로드한걸 가져다 쓰면 되는거 아니었나?**

- 파이썬/OS의 프로세스 구조상, FastAPI에서 로드한 모델을 RQ워커가 “직접 가져다 쓸 수는 없다.”

### why?

1. 프로세스 메모리 공간은 완전히 분리
    - FastAPI 서버와 RQ워커는 서로다른 프로 세스임.
    - 각 프로세스는 자기만의 메모리 공간을 가짐.
    - 한 프로세스에서 만든 파이썬 객체(모델 인스턴스)는 다른 프로세스에서 직접 참조 할 수 없다.
2. GPU 메모리와 파이썬 객체
    - GPU 메모리에 모델이 올라가 있어도,
    - 그걸 조작하는 파이썬 객체(예: shared_model)는 각 프로세스 마다 별도로 생성 되어야함
    - 즉, FastAPI에서 모델을 올려도, RQ 워커는 그걸 "참조”할 방법이 없음  ->   그래서 다시 로드 해야함.
3. 프로세스 간 객체 공유는 불가능
    - 파이썬의 기본 구조에서는 프로세스 간에 객체는 직접 공유 불가.
    > (IPC, shared memory, socket 등 특수한 방법을 쓰지 않는 한)

### 프로세스 간 모델 공유가 왜 안되는 걸까?

1. 프로세스 간 메모리 공간의 근본적 분리

## 1.1. 가상 메모리 시스템의 특성

```python
# 각 프로세스는 독립적인 가상 메모리 공간을 가짐
Process A (FastAPI): 0x00000000 ~ 0x7FFFFFFF (가상 주소)
Process B (RQ Worker): 0x00000000 ~ 0x7FFFFFFF (동일한 가상 주소지만 물리적으로 다른 메모리)

# 실제로는:
# FastAPI: shared_model._model → 0x7f8b2c001234 (물리 메모리 A)
# RQ Worker: shared_model._model → 0x7f8b2c001234 (물리 메모리 B) - 완전히 다른 메모리!
```

- 문제점:
  - 같은 가상 주소 0x10000000 이 각 프로세스에서 완전히 다른 물리 메모리를 가르킴
  - PyTorch 모델의 텐서들이 가리키는 메모리 주소가 프로세스마다 다름
  - GPU 메모리도 프로세스별로 분리된 컨텍스트를 가짐

## 1.2. Python 객체의 메모리 레이아웃

```python
# shared_model.py에서 실제로 일어나는 일:
class SharedMistralModel:
    def _initialize_model(self):
        # 1. Python 객체 생성 (프로세스 메모리에)
        self._model = AutoModelForCausalLM.from_pretrained(...)
        
        # 2. 이 모델 객체는 다음과 같은 구조를 가짐:
        # - Python 객체 헤더 (타입, 참조 카운트 등)
        # - 모델 파라미터 텐서들의 참조 리스트
        # - 각 텐서는 GPU 메모리의 특정 주소를 가리킴
        # - 이 주소들은 프로세스별로 고유함
         
        # 3. GPU 메모리 할당 (CUDA 컨텍스트에)
        # - device_map="auto"로 인해 GPU 메모리 할당
        # - 이 할당은 현재 프로세스의 CUDA 컨텍스트에만 유효
```

2. GPU 메모리와 CUDA 컨텍스트의 분리

## 2.1. 실제 CUDA 초기화 가정

```python
# shared_model.py에서 torch import 시 자동으로 일어나는 일:
import torch  # PyTorch가 자동으로 CUDA 초기화

# 각 프로세스에서:
# 1. CUDA 드라이버 초기화
# 2. CUDA 컨텍스트 생성 (프로세스별로 독립적)
# 3. GPU 메모리 관리자 초기화

# FastAPI 프로세스: CUDA 컨텍스트 A 생성
# RQ Worker 프로세스: CUDA 컨텍스트 B 생성 (완전히 분리됨)
```

- 핵심문제:
            - CUDA 컨텍스트는 프로세스별로 완전리 분리됨
            - 한 프로세스에서 할당한 GPU 메모리를 다른 프로세스에서 직접 접근이 불가
            - GPU 메모리 주소도 프로세스 별로 가상화됨 
        2. GPU 메모리 할당의프로세스 별 독립성 

```python
# shared_model.py에서 실제 GPU 메모리 할당:
gpu_memory = torch.cuda.get_device_properties(0).total_memory
# 이 명령은 현재 프로세스의 CUDA 컨텍스트에서만 유효

self._model = AutoModelForCausalLM.from_pretrained(
    # ... 설정들
    device_map="auto",  # 현재 프로세스의 GPU에만 할당
)

# 결과:
# FastAPI: GPU 메모리 주소 0x10000000 ~ 0x20000000 할당
# RQ Worker: GPU 메모리 주소 0x30000000 ~ 0x40000000 할당 (완전히 다른 영역)
```

### IPC(Inter-Process Communication)? 그게 뭔데? 

>> 프로세스간 통신을 의미

    - IPC란?
        1. 기본 개념
            - 프로세스: 실행중인 프로그램의 인스턴스
            - 통신: 데이터나 신호를 주고받는 것
            - IPC: 서로 다른 프로세스 들이 데이터를 주고 받거나 동기화 하는 방법

        2. 왜 IPC가 필요한가?

```python
# 현재 시스템에서 실제 상황:
# FastAPI 서버 (프로세스 A)
# RQ Worker (프로세스 B)
# vLLM 서버 (프로세스 C)

# 각 프로세스는 독립적인 메모리 공간을 가짐
# → 서로 직접 데이터를 공유할 수 없음
# → IPC를 통해 통신해야 함
```

    - 이전 코드에서는 모델 객체 자체를 프로세스간 공유를 하려고 했기 때문에, 현실적으로 IPC 적용이 불가능 했다.
    - 하지만 현재 시스템에서 실제로 사용하는 IPC는 "작업 요청/응답" 을 주고 받는 통신이어라
        1.  Redis를 통한 IPC(메세지 큐)

>>>> 메세지 큐란?

>>>> 프로세스 간에 메세지를 주고 받는 시스템

>>>> 메세지 = 데이터 + 메타데이터

```python
# feedback_router.py에서 실제 사용:
from rq import Queue
from redis import Redis

redis_conn = Redis(host='localhost', port=6379, db=0)
feedback_queue = Queue('feedback', connection=redis_conn)

# FastAPI → RQ Worker로 작업 전달 (IPC)
job = feedback_queue.enqueue('Text.LLM.model.feedback.tasks.generate_feedback_task', request.model_dump())
```

        2. HTTP를 통한 IPC(소켓 기반)

```python
# LLM_chatbot_free_text_model.py에서 실제 사용:
import httpx

# FastAPI/RQ Worker → vLLM Server로 추론 요청 (IPC)
async with httpx.AsyncClient() as client:
    response = await client.post(
        "http://localhost:8800/v1/chat/completions",
        json=payload,
        timeout=60.0
    )
```

#### 프로세스 간 통신 구조

```python
# 현재 시스템의 프로세스들:
# 1. FastAPI 서버 (포트 8000)
# 2. RQ Worker (별도 프로세스)
# 3. vLLM 서버 (포트 8800)
# 4. Redis 서버 (포트 6379)

# IPC 통신 흐름:
FastAPI → Redis(IPC) → RQ Worker
FastAPI/RQ Worker → HTTP(IPC) → vLLM Server
```

    -  현재 vLLM 시스템의 실제구조

```python
# 현재 시스템에서 사용하는 IPC:

# 1. Redis 큐 (작업 요청)
FastAPI → Redis → RQ Worker
"피드백 생성해줘" (작은 JSON)

# 2. HTTP API (추론 요청)  
FastAPI/RQ Worker → vLLM Server
"이 프롬프트로 추론해줘" (작은 JSON)

# 3. HTTP 응답 (결과)
vLLM Server → FastAPI/RQ Worker
"추론 결과" (텍스트)
```

    #### 결론:

    - 모델 공유용 IPC: 현실적으로 불가능(너무 크고 느림)
    - 작업 통신용 IPC: 현재 시스템에서 사용중

---

    ### 메세지 큐 와 테스크 큐?

    1.  메세지 큐(=Redis)

>>> Redis는 메세지를 저장하고 전달하는 메세지 큐 시스템

```python
# 메시지 큐는 "프로세스 간에 메시지를 주고받는 시스템"
# 메시지 = 데이터 + 메타데이터

# Redis는 메시지를 저장하고 전달하는 메시지 큐 시스템
# 현재 시스템에서:
redis_conn = Redis(host='localhost', port=6379, db=0)
feedback_queue = Queue('feedback', connection=redis_conn)
```

    1. 태스크 큐(=QR)

>>>  RQ는 테스크를 관리하는 테스크 큐 시스템

```python
# 태스크 큐는 "실행할 작업들을 관리하는 시스템"
# 태스크 = 실행할 함수 + 파라미터

# RQ는 태스크를 관리하는 태스크 큐 시스템
# RQ가 Redis를 백엔드로 사용해서 메시지를 저장함
from rq import Queue
feedback_queue = Queue('feedback', connection=redis_conn)
```

### 현재  시스템 구조^^

```python
# 흐름:
FastAPI → RQ(태스크 큐) → Redis(메시지 큐) → RQ Worker
```

    1. Redis (메시지 큐) - 메시지 저장소
    2. RQ (태스크 큐) - 태스크 관리 시스템
    3. RQ가 Redis를 사용해서 메세지를 저장!