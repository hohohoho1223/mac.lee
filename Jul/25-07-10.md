# 오늘 내가 배운 것들(Today I Learned)

- 크롤러 파이프라인 수정 & 리펙토링 + 홍보글 작성해봄ㅎ

---

- 크롤러 변경사항

### 1. FastAPI 통합

- **변경사항:** 크롤러가 `Text/LLM/main.py`의 FastAPI 서버에 통합됨
- **엔드포인트:** `/ai/crawler/run` (POST) - 크롤러 수동 실행
- **위치:** `Text/LLM/router/crawler_router.py`

### 2. 백그라운드 스레딩

- **변경사항:** main.py 시작 시 크롤러가 백그라운드 스레드에서 실행
- **동작:** FastAPI 서버가 즉시 시작되고, 크롤러가 동시에 실행됨
- **코드 위치:** `Text/LLM/main.py` - `threading.Thread(target=generate_challenge_docs, daemon=True).start()`

### 3. 서버 시작 플로우

- **이전:** 크롤러가 끝날 때까지 기다린 후 서버 시작
- **현재:** 서버가 즉시 시작되고, 크롤러는 백그라운드에서 실행
- **장점:** 서버 시작 지연 없음, API 엔드포인트 즉시 사용 가능

### 4. [25-07-10] Crawler/Embed 파이프라인 구조 분리 및 개선

- **변경사항:**
- `generate_challenge_docs.py`는 백그라운드 스레드에서 싱행되어 챌린지 데이터(고정+크롤링)만 생성-> `challenge_docs.txt`에 저장 (임베딩 X)
- `embed_init.py`는 오직 `challenge_docs.txt` 파일을 읽어서 임베딩 및 Qdrant 저장만 수행 (크롤링/데이터 생성 X)
- `embed_init.py`에서 크롤링/데이터 생성 관련 코드 완전 제거
- 파일 기반 파이프라인으로 역할이 명확히 분리되어 유지보수, 자동화, 테스트 용이
- 중복 실행/무한루프 위험 해소

## 파일 구조

```other
Text/
├── Crawler/
│ ├── generate_challenge_docs.py # 메인 크롤러 로직
│ ├── embed_init.py # 임베딩 및 Qdrant 저장
│ └── challenge_docs.txt # 생성된 챌린지 데이터 (gitignored)
└── LLM/
├── main.py # 크롤러가 통합된 FastAPI 서버
└── router/
└── crawler_router.py # 크롤러 API 엔드포인트
```

## API 엔드포인트

- 서버 시작 시 크롤러가 자동으로 실행됨 (백그라운드 스레드)
- `POST /ai/crawler/run` - 크롤러 수동 실행 가능

```Bash
curl -X POST http://localhost:8000/ai/crawler/run
```

## 의존성

- FastAPI
- threading (내장)
- requests, BeautifulSoup (크롤링용)
- SentenceTransformer, Qdrant (임베딩용)

## 참고사항

- main.py 시작 시 크롤러가 자동으로 실행됨
- challenge_docs.txt는 생성된 데이터를 커밋하지 않도록 gitignored됨
- 백그라운드 스레딩으로 서버가 즉시 시작되면서 크롤러가 동시에 실행됨
- 추후에 수동으로 크롤러 실행 하고 싶으면 `curl -X POST http://localhost:8000/ai/crawler/run` 입력

---

### 챗봇 동시요청시 vLLM 서버 셧다운 이슈 

- 아래와 같은 에러 로그 확인

```plaintext
Exception: Invalid prefix encountered
...
raise EngineGenerateError() from e
vllm.v1.engine.exceptions.EngineGenerateError
```

- vllm 내부에서 detokenizer가 토큰을 디코딩하는 과정에서 "Invalid prefix encountered"라는 예외가 발생
- 이로 인해 전체 서버가 셧다운됨

### 원인 분석

### 1. Invalid prefix encountered

- 이 에러는 vllm의 detokenizer가 토큰 스트림을 디코딩할 때, 예상하지 못한(모델이 지원하지 않는) prefix가 등장했을 때 발생합니다.
- 주로 다음과 같은 경우에 발생할 수 있습니다:
- prompt나 generation 결과에 모델이 지원하지 않는 특수 토큰/문자열이 포함된 경우
- 모델/토크나이저 파일이 손상되었거나, 버전이 맞지 않는 경우
- 동시 요청 처리 중 내부 상태가 꼬인 경우 (특히 여러 세션에서 동시에 요청 시)

### 2. max_tokens 설정

- 로그에서 `max_tokens=32247` 로 설정되어 있음을 확인

> 이 값이 너무 크면, 내부적으로 비정상적인 토큰 시퀀스가 생성될 가능성이 높아짐

- 32,247 토큰은 모델이 지원하는 최대 context window를 초과할 수 있음

### 해결 방안

- `max_tokens: 2048` 로 지정
- base-info, free-text에 적용

```python
payload = {
    "model": "/home/ubuntu/mistral/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db",
    "messages": [{"role": "user", "content": prompt}],
    "stream": True,
    "max_tokens": 2048
}
```

### max_tokens vs max-model-len 차이

1. max-model-len (vLLM 서버 시작 시):
    - 전체 시퀀스 길이 제한 (입력 + 출력)
    - GPU 메모리 한계로 인한 서버 시작 문제 해결
    - `max-model-len: 8192` 토큰 = 입력 + 출력 총합 제한
2. max_tokens (FastAPI 요청 시):
    - 생성할 토큰 수 제한 (출력만)
    - EngineGenerateError (Invalid prefix encountered) 방지
    - 2048 토큰 = 응답 생성 제한

### 왜 두 개를 다 설정하는가?

1. max-model-len 8192:
    - 서버가 시작될 수 있도록 GPU 메모리 한계 설정
    - 입력(1000) + 출력(2048) = 3048 < 8192 → 안전
2. max_tokens 2048:
    - 응답 품질 제어 (너무 긴 응답 방지)
    - EngineGenerateError 방지 (비정상 토큰 시퀀스 방지)
    - 메모리 안전성 (동시 요청 시 OOM 방지)