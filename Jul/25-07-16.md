# 오늘 내가 배운 것들(Today I Learned)

- 파인튜닝 이어서 진행.

---

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/7973B2CB-A9F9-42FA-89BB-7A831AEAB3E9_2/rLf8pGZCyXhH5GvbLpzXGXlEuFA0ayaJj5AA5CPy9FIz/Image.png)

- Loss값이 4밑으로 안내려가고 요동치고 있디.(데이터셋 2000개)
- Loss값을 줄이기위해 개선 방법

## **Loss 감소를 위한 개선 방법**

| **방법**                             | **설명**                                                                                                                                                                                                                                |
| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1️⃣ 데이터 증강**                     | \- **Leafresh 과제의 핵심**은 JSON 포맷으로 챌린지를 추천하는 것이므로, 다양한 패턴의 입력/출력을 학습해야 함.- **현재 2000개**는 7B 모델 기준으로 극단적으로 적음.- 보통 **5B~7B 모델**은 LoRA라도 최소 **1만~3만 샘플**이 있어야 안정적으로 수렴함.- **다양한 location, job, situation 조합**으로 **synthetic data 생성** 추천 |
| **2️⃣ 학습 epoch 증가**                | \- 데이터가 적으면 반복해서 많이 보여주는 게 필요함.- 현재 2~3 epoch → **최소 10 epoch 이상 권장**- LoRA는 파라미터가 적어서 **과적합보다는 부족학습이 문제**가 됨.                                                                                                                        |
| **3️⃣ Learning Rate 줄이기**          | \- 현재 learning rate 2e-4는 LoRA에서는 많이 쓰는 값이지만, **데이터가 적을 땐 과하게 큼**.- **1e-4로 줄이면** 더 천천히 수렴하고, Loss 감소 안정성이 높아짐.- 특히 **“structured output task(JSON)”**는 높은 learning rate일수록 모델이 잘못된 패턴을 학습함.                                          |
| **4️⃣ LoRA Rank 줄이기 (r=16 → r=8)** | \- Rank가 크면 모델의 표현력이 좋아지지만, **VRAM 사용량도 많이 올라감**.- Rank=8로 낮추면 **VRAM 감소 + 과적합 방지 + 더 안정적 수렴**.- Mistral의 경우 r=8~16이 일반적이고, 데이터 적으면 **8부터 시도하는 게 안전**                                                                                 |

## **정리하면(7B 기준)**

| **항목**        | **현재 상태** | **개선안**       |
| ------------- | --------- | ------------- |
| 데이터 개수        | 2,000개    | 최소 10,000개 이상 |
| epoch         | 2~3       | 10 이상         |
| learning rate | **5e-5**  | 2e-4          |
| LoRA rank     | 16        | 8             |

- 참고:
- “Loss가 줄지 않는다” → 모델이 학습할 데이터 패턴이 부족하거나, 너무 급하게 학습하고 있어서 발생하는 현상
- JSON기반 출력은 로그퍼플렉서티도 높게 나오는게 정상 ← ??

---

    ### 2e-4의 의미

| **표기**   | **뜻**                 |
| -------- | --------------------- |
| **5e-5** | 5 × 10⁻5 = **0.0005** |
| 2e-4     | 2 × 10⁻⁴ = **0.0002** |

- Learning Rate가 뭘까?
- 학습률 (Learning Rate):
  - 모델의 가중치(W)를 얼마나 크게 업데이트 할지 결정하는 값

```python
W_new = W_old - learning_rate × gradient
```

- 줄이면 좋은걸까?

| **학습률이 크면**          | **학습률이 작으면**         |
| -------------------- | -------------------- |
| 빠르게 갱신되지만 **발산** 가능  | 천천히 갱신되지만 **안정적 수렴** |
| 데이터 적을 때 **Loss 진동** | 데이터 적을 때 **완만하게 감소** |

- 학습률이 작으면 더 오래걸리지만 안정적인 최소점을 찾아 내려갈 수 있다.
- 데이터가 작으므로(현재 2000개) 학습률을 **5e-5** → 2e-4 낮춰서 진행해보자