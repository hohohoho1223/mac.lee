# 오늘 내가 배운 것들(Today I Learned)

- 멀티태스킹 파인튜닝 진행

---

- 현재 하나의 vLLM서버에서 L4환경으로 챗봇, 피드백 요청을 다 받고 있다.
- 각 모델별 자체서빙을 현실적으로 하기 힘든 상황(GPU NVIDIA L4 1개)
- 그래서 “멀티테스킹 파인튜닝”을 진행 할 예정

1. 멀티태스킹 파인튜닝이란?
    - 여러 업무를 하나의 모델이 처리하도록 파인튜닝 하는 것
    - 각 태스크(예: 챌린지 추천, 피듭백, 검열 등)에 맞는 프롬프트/지시문을 함께 학습시킴
2. 멀티태스킹 데이터셋 예시
    - 챌린지 추천 + 피드백 + 자유대화까지 한 모델에!

```json
[
  {
    "instruction": "너는 챌린지 추천 챗봇이야. 사용자가 챌린지 추천을 요청하면 아래 JSON 포맷으로 답해.",
    "input": "제로웨이스트 챌린지 추천해줘",
    "output": "{\"recommend\": \"이런 챌린지를 추천합니다.\", \"challenges\": [{\"title\": \"1. 텀블러 사용하기\", \"description\": \"일회용 컵 대신 텀블러를 사용하세요.\", \"category\": \"ZERO_WASTE\"}, ...]}"
  },
  {
    "instruction": "너는 피드백 어시스턴트야. 사용자의 주간 활동을 요약해서 칭찬과 격려를 해줘.",
    "input": "이번 주에 3번 플로깅을 했어.",
    "output": "이번 주에도 환경을 위해 열심히 실천하셨네요! 정말 멋져요 😊"
  },
  {
    "instruction": "너는 사용자와 자유롭게 대화하며 대화의 맥락에 맞는 친환경 챌린지 3가지를 JSON 형식으로 추천하는 챗봇이야.",
    "input": "스트레스 받아서 쇼핑으로 풀어야겠어!",
    "output": "{\"recommend\": \"쇼핑으로 스트레스를 푸는 것도 좋죠! 기분 전환도 하면서 환경까지 생각하는 의식있는 쇼핑 챌린지는 어떠세요?\", \"challenges\": [{\"title\": \"1. 중고 거래 앱으로 보물찾기\", \"description\": \"새 제품 대신 중고 거래 앱에서 나에게 꼭 필요한 물건을 저렴하게 찾아보세요.\"}, {\"title\": \"2. 구매 전 3번 생각하기\", \"description\": \"정말 필요한 물건인지 3번 고민하고 구매하여 충동구매를 줄여요.\"}, {\"title\": \"3. 친환경 브랜드 제품 구매하기\", \"description\": \"하나를 사더라도 환경을 생각하는 브랜드의 제품을 선택해 가치 있는 소비를 해요.\"}]}"
  }
]
```

- instruction: 역할/업무를 명확히 지시
- input: 실제 사용자 질문
- output: 그에 맞는 답변(포맷/톤/내용 다르게)

3. 데이터 증강(다양성 확보)

## (1) 증강 전략

- 질문 다양화:
  - 같은 의미, 다양한 표현(예: "추천해줘", "뭐가 좋아?", "뭐할까?")
- 상황/대상 다양화:
  - 직장인, 학생, 가족, 친구, 혼자 등
- 챌린지/피드백/자유대화:
  - 각 태스크별로 다양한 상황/질문/답변 추가
- 오타/구어체/반말/존댓말:
  - 실제 사용자 스타일 반영

## (2) 증강 예시

```json
{
  "instruction": "너는 챌린지 추천 챗봇이야. ...",
  "input": "플라스틱 줄이는 방법 뭐 있어?",
  "output": "{\"recommend\": \"플라스틱 줄이기에 도움이 되는 챌린지를 추천합니다.\", ...}"
},
{
  "instruction": "너는 챌린지 추천 챗봇이야. ...",
  "input": "친구랑 할만한 환경 챌린지 추천해줘",
  "output": "{\"recommend\": \"친구와 함께하면 더 좋은 챌린지를 추천합니다.\", ...}"
},
{
  "instruction": "너는 피드백 어시스턴트야. ...",
  "input": "이번 주에 분리수거를 5번 했어.",
  "output": "분리수거를 꾸준히 실천하셨군요! 환경을 위한 멋진 행동이에요."
},
{
  "instruction": "너는 자유대화 챗봇이야. ...",
  "input": "너 오늘 뭐했어?",
  "output": "저는 오늘도 여러분을 도울 준비를 하고 있었어요!"
}
```

1. 프롬프트 설계 팁
    - 역할을 명확히:
        - "너는 챌린지 추천 챗봇이야", "너는 피드백 어시스턴트야", "너는 자유대화 챗봇이야" 등
    - 포맷/톤/언어를 구체적으로 명시:
        - "아래 JSON 포맷으로만 답해", "한글로만 답해", "항상 3개 챌린지", "자연스럽고 친근하게" 등
    - 예시를 프롬프트에 포함:
        - "출력 예시: { ... }"
        - "반드시 위 예시처럼만 답해"

### 피드백 데이터 셋 설정

- 성공: "{} 성공! 정말 멋져요. {}"
- 실패: "{}는(은) 조금 아쉬웠지만, 도전 자체가 의미 있어요. {}"
- 그룹 챌린지 참여: "{} 챌린지에도 꾸준히 참여하셨네요! 대단해요. {}"
- 아무것도 없을 때: "이번 주는 챌린지 활동이 없었네요. 다음엔 새로운 도전을 기대할게요! {}"

---

### 트레인 코드

```python
import os, json, threading
import torch
import matplotlib.pyplot as plt
from transformers import (
    AutoModelForCausalLM, AutoTokenizer,
    TrainingArguments, Trainer,
    EarlyStoppingCallback, TrainerCallback
)
from peft import LoraConfig, get_peft_model
from datasets import Dataset

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.exceptions import RequestValidationError
from prometheus_client import start_http_server

# 1. 데이터 로드 및 토크나이징
with open("/content/drive/MyDrive/Colab Notebooks/[카카오 부트캠프]/Leafresh/multitask_dataset.json", "r") as f:
    data = json.load(f)

dataset = Dataset.from_list(data)

model_path = "/content/drive/MyDrive/Colab Notebooks/[카카오 부트캠프]/Leafresh/mistral-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_path)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map={"": "cuda:0"}
)

# 2. LoRA 구성
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# 3. 토크나이즈 함수
def tokenize_fn(examples):
    texts = [
        f"<s>[INST] {inst}\n{inp} [/INST] {out} </s>"
        for inst, inp, out in zip(examples['instruction'], examples['input'], examples['output'])
    ]
    return tokenizer(texts, truncation=True, padding="max_length", max_length=1024)

tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=dataset.column_names)

# 4. Loss Callback
class LossHistoryCallback(TrainerCallback):
    def __init__(self):
        self.losses = []
        self.steps = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None and "loss" in logs:
            self.losses.append(logs["loss"])
            self.steps.append(state.global_step)

loss_callback = LossHistoryCallback()
early_stopping = EarlyStoppingCallback(early_stopping_patience=4)

# 5. Train/Validation Split
split = tokenized.train_test_split(test_size=0.1, seed=42)

# 6. Trainer 설정
args = TrainingArguments(
    output_dir="./lora_adapter",
    num_train_epochs=4, # 전체 데이터셋 2번 반복학습
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    fp16=True,
    save_steps=50,
    logging_steps=5,
    report_to="wandb",
    eval_strategy="steps",   # ✅ 반드시 추가 (validation 해야함)
    eval_steps=50,                  # ✅ evaluation 주기
    load_best_model_at_end=True,    # ✅ EarlyStopping 필수
    metric_for_best_model="eval_loss"  # ✅ EarlyStopping 필수
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=split['train'],
    eval_dataset=split['test'],
    data_collator=lambda data: {
        "input_ids": torch.stack([torch.tensor(f["input_ids"]) for f in data]),
        "attention_mask": torch.stack([torch.tensor(f["attention_mask"]) for f in data]),
        "labels": torch.stack([torch.tensor(f["input_ids"]) for f in data]),
    },
    callbacks=[loss_callback, early_stopping]
)

# 7. 학습 실행
trainer.train()

# 8. 저장 및 Loss 그래프
model.save_pretrained("./lora_adapter")
tokenizer.save_pretrained("./lora_adapter")

plt.plot(loss_callback.steps, loss_callback.losses)
plt.xlabel("Step")
plt.ylabel("Training Loss")
plt.title("Training Loss Curve")
plt.show()
```

- 여기서 `gradient_accumulation_steps=8` 이게 2개씩 8번(=16개) 샘플을 처리한 후에야  optimizer가 한 번 파라미터를 업데이트
  - 실질적 batch size = 2 × 8 = 16
  - 메모리 부족할 때 batch size를 키우는 효과

### "메모리 부족할 때 batch size를 키우는 효과"이게 뭐임? 뭔 상관관계가 있음?

- gradient_accumulation_steps와 batch size의 관계를 이해해보자.
    1. GPU 메모리와 batch size의 관계
        - batch size를 키우면 한 번에 더 많은 데이터를 처리 → 더 많은 메모리가 필요
        - 하지만 GPU 메모리가 부족하면 batch size를 크게 할 수 없음
    2. gradient_accumulation_steps의 역할
        - gradient_accumulation_steps는"작은 batch size로 여러 번 gradient를 누적해서, 마치 큰 batch size로 학습하는 효과"를 준다
    3. 예시
        - 원하는 실질적 batch size: 32
        - GPU 메모리 한계로 per_device_train_batch_size=4만 가능
        - gradient_accumulation_steps=8로 설정
    4. 동작 방식
        - 4개씩 8번(총 32개) 데이터를 처리하면서 gradient를 누적
        - 8번째 step에서 한 번 optimizer step(가중치 업데이트)
        - → 실질적으로 batch size=32로 학습한 것과 거의 동일한 효과

---

### Train 테스트

1. 변수 설정 

```python
    num_train_epochs=7, # 전체 데이터셋 반복학습 횟수
    per_device_train_batch_size=1, # GPU 환경 여건상 1로 고정
    gradient_accumulation_steps=8, 
```

2. 데이터 셋:

- 138개
- Loss값 변화

| **Step** | **Training Loss** | **Validation Loss** |
| -------- | ----------------- | ------------------- |
| 50       | 4.401700          | 4.598206            |
| 100      | 4.384800          | 4.559369            |
| 150      | 4.367800          | 4.551898            |

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/A4F3B519-39D8-480E-B22A-518ECCFCF775_2/RocxNMP6D5QaZoNMGsmcxAp3675vF1RjF9TpY9HBaesz/Image.png)

- 500개
- Loss값 변화

| **Step** | **Training Loss** | **Validation Loss** |
| -------- | ----------------- | ------------------- |
| 50       | 4.645700          | 4.492012            |
| 100      | 4.556200          | 4.426577            |
| 150      | 4.432800          | 4.408548            |
| 200      | 4.433300          | 4.403012            |
| 250      | 4.598700          | 4.401348            |
| 300      | 4.277900          | 4.400284            |
| 350      | 4.472500          | 4.400099            |

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/62502B2B-EFEE-482A-8CA8-0111949AAA62_2/iN7hweYbSsLQOgLPkWSpXr68TqfQTDLql0vYrqLsCPgz/Image.png)

- 2000개
- Loss값 변화(청록색)

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/7973B2CB-A9F9-42FA-89BB-7A831AEAB3E9_2/rLf8pGZCyXhH5GvbLpzXGXlEuFA0ayaJj5AA5CPy9FIz/Image.png)

| **Step** | **Training Loss** | **Validation Loss** |
| -------- | ----------------- | ------------------- |
| 50       | 4.620100          | 4.637240            |
| 100      | 4.611300          | 4.567899            |
| 150      | 4.586400          | 4.554155            |
| 200      | 4.444700          | 4.549914            |
| 250      | 4.744000          | 4.546744            |
| 300      | 4.646400          | 4.544673            |
| 350      | 4.588600          | 4.542412            |
| 400      | 4.587200          | 4.540801            |
| 450      | 4.697900          | 4.539388            |
| 500      | 4.494200          | 4.537243            |
| 550      | 4.405500          | 4.535868            |
| 600      | 4.634600          | 4.534741            |

- 변화량이 적어 조기중단함.
  > Loss값이 안줄어듦.