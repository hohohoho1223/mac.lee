# ì˜¤ëŠ˜ ë‚´ê°€ ë°°ìš´ ê²ƒë“¤(Today I Learned)

- ë©€í‹°íƒœìŠ¤í‚¹ íŒŒì¸íŠœë‹ ì§„í–‰

---

- í˜„ì¬ í•˜ë‚˜ì˜ vLLMì„œë²„ì—ì„œ L4í™˜ê²½ìœ¼ë¡œ ì±—ë´‡, í”¼ë“œë°± ìš”ì²­ì„ ë‹¤ ë°›ê³  ìˆë‹¤.
- ê° ëª¨ë¸ë³„ ìì²´ì„œë¹™ì„ í˜„ì‹¤ì ìœ¼ë¡œ í•˜ê¸° í˜ë“  ìƒí™©(GPU NVIDIA L4 1ê°œ)
- ê·¸ë˜ì„œ â€œë©€í‹°í…ŒìŠ¤í‚¹ íŒŒì¸íŠœë‹â€ì„ ì§„í–‰ í•  ì˜ˆì •

1. ë©€í‹°íƒœìŠ¤í‚¹ íŒŒì¸íŠœë‹ì´ë€?
    - ì—¬ëŸ¬ ì—…ë¬´ë¥¼ í•˜ë‚˜ì˜ ëª¨ë¸ì´ ì²˜ë¦¬í•˜ë„ë¡ íŒŒì¸íŠœë‹ í•˜ëŠ” ê²ƒ
    - ê° íƒœìŠ¤í¬(ì˜ˆ: ì±Œë¦°ì§€ ì¶”ì²œ, í”¼ë“­ë°±, ê²€ì—´ ë“±)ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸/ì§€ì‹œë¬¸ì„ í•¨ê»˜ í•™ìŠµì‹œí‚´
2. ë©€í‹°íƒœìŠ¤í‚¹ ë°ì´í„°ì…‹ ì˜ˆì‹œ
    - ì±Œë¦°ì§€ ì¶”ì²œ + í”¼ë“œë°± + ììœ ëŒ€í™”ê¹Œì§€ í•œ ëª¨ë¸ì—!

```json
[
  {
    "instruction": "ë„ˆëŠ” ì±Œë¦°ì§€ ì¶”ì²œ ì±—ë´‡ì´ì•¼. ì‚¬ìš©ìê°€ ì±Œë¦°ì§€ ì¶”ì²œì„ ìš”ì²­í•˜ë©´ ì•„ë˜ JSON í¬ë§·ìœ¼ë¡œ ë‹µí•´.",
    "input": "ì œë¡œì›¨ì´ìŠ¤íŠ¸ ì±Œë¦°ì§€ ì¶”ì²œí•´ì¤˜",
    "output": "{\"recommend\": \"ì´ëŸ° ì±Œë¦°ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.\", \"challenges\": [{\"title\": \"1. í…€ë¸”ëŸ¬ ì‚¬ìš©í•˜ê¸°\", \"description\": \"ì¼íšŒìš© ì»µ ëŒ€ì‹  í…€ë¸”ëŸ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\", \"category\": \"ZERO_WASTE\"}, ...]}"
  },
  {
    "instruction": "ë„ˆëŠ” í”¼ë“œë°± ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. ì‚¬ìš©ìì˜ ì£¼ê°„ í™œë™ì„ ìš”ì•½í•´ì„œ ì¹­ì°¬ê³¼ ê²©ë ¤ë¥¼ í•´ì¤˜.",
    "input": "ì´ë²ˆ ì£¼ì— 3ë²ˆ í”Œë¡œê¹…ì„ í–ˆì–´.",
    "output": "ì´ë²ˆ ì£¼ì—ë„ í™˜ê²½ì„ ìœ„í•´ ì—´ì‹¬íˆ ì‹¤ì²œí•˜ì…¨ë„¤ìš”! ì •ë§ ë©‹ì ¸ìš” ğŸ˜Š"
  },
  {
    "instruction": "ë„ˆëŠ” ì‚¬ìš©ìì™€ ììœ ë¡­ê²Œ ëŒ€í™”í•˜ë©° ëŒ€í™”ì˜ ë§¥ë½ì— ë§ëŠ” ì¹œí™˜ê²½ ì±Œë¦°ì§€ 3ê°€ì§€ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶”ì²œí•˜ëŠ” ì±—ë´‡ì´ì•¼.",
    "input": "ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ì•„ì„œ ì‡¼í•‘ìœ¼ë¡œ í’€ì–´ì•¼ê² ì–´!",
    "output": "{\"recommend\": \"ì‡¼í•‘ìœ¼ë¡œ ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í‘¸ëŠ” ê²ƒë„ ì¢‹ì£ ! ê¸°ë¶„ ì „í™˜ë„ í•˜ë©´ì„œ í™˜ê²½ê¹Œì§€ ìƒê°í•˜ëŠ” ì˜ì‹ìˆëŠ” ì‡¼í•‘ ì±Œë¦°ì§€ëŠ” ì–´ë– ì„¸ìš”?\", \"challenges\": [{\"title\": \"1. ì¤‘ê³  ê±°ë˜ ì•±ìœ¼ë¡œ ë³´ë¬¼ì°¾ê¸°\", \"description\": \"ìƒˆ ì œí’ˆ ëŒ€ì‹  ì¤‘ê³  ê±°ë˜ ì•±ì—ì„œ ë‚˜ì—ê²Œ ê¼­ í•„ìš”í•œ ë¬¼ê±´ì„ ì €ë ´í•˜ê²Œ ì°¾ì•„ë³´ì„¸ìš”.\"}, {\"title\": \"2. êµ¬ë§¤ ì „ 3ë²ˆ ìƒê°í•˜ê¸°\", \"description\": \"ì •ë§ í•„ìš”í•œ ë¬¼ê±´ì¸ì§€ 3ë²ˆ ê³ ë¯¼í•˜ê³  êµ¬ë§¤í•˜ì—¬ ì¶©ë™êµ¬ë§¤ë¥¼ ì¤„ì—¬ìš”.\"}, {\"title\": \"3. ì¹œí™˜ê²½ ë¸Œëœë“œ ì œí’ˆ êµ¬ë§¤í•˜ê¸°\", \"description\": \"í•˜ë‚˜ë¥¼ ì‚¬ë”ë¼ë„ í™˜ê²½ì„ ìƒê°í•˜ëŠ” ë¸Œëœë“œì˜ ì œí’ˆì„ ì„ íƒí•´ ê°€ì¹˜ ìˆëŠ” ì†Œë¹„ë¥¼ í•´ìš”.\"}]}"
  }
]
```

- instruction: ì—­í• /ì—…ë¬´ë¥¼ ëª…í™•íˆ ì§€ì‹œ
- input: ì‹¤ì œ ì‚¬ìš©ì ì§ˆë¬¸
- output: ê·¸ì— ë§ëŠ” ë‹µë³€(í¬ë§·/í†¤/ë‚´ìš© ë‹¤ë¥´ê²Œ)

3. ë°ì´í„° ì¦ê°•(ë‹¤ì–‘ì„± í™•ë³´)

## (1) ì¦ê°• ì „ëµ

- ì§ˆë¬¸ ë‹¤ì–‘í™”:
  - ê°™ì€ ì˜ë¯¸, ë‹¤ì–‘í•œ í‘œí˜„(ì˜ˆ: "ì¶”ì²œí•´ì¤˜", "ë­ê°€ ì¢‹ì•„?", "ë­í• ê¹Œ?")
- ìƒí™©/ëŒ€ìƒ ë‹¤ì–‘í™”:
  - ì§ì¥ì¸, í•™ìƒ, ê°€ì¡±, ì¹œêµ¬, í˜¼ì ë“±
- ì±Œë¦°ì§€/í”¼ë“œë°±/ììœ ëŒ€í™”:
  - ê° íƒœìŠ¤í¬ë³„ë¡œ ë‹¤ì–‘í•œ ìƒí™©/ì§ˆë¬¸/ë‹µë³€ ì¶”ê°€
- ì˜¤íƒ€/êµ¬ì–´ì²´/ë°˜ë§/ì¡´ëŒ“ë§:
  - ì‹¤ì œ ì‚¬ìš©ì ìŠ¤íƒ€ì¼ ë°˜ì˜

## (2) ì¦ê°• ì˜ˆì‹œ

```json
{
  "instruction": "ë„ˆëŠ” ì±Œë¦°ì§€ ì¶”ì²œ ì±—ë´‡ì´ì•¼. ...",
  "input": "í”Œë¼ìŠ¤í‹± ì¤„ì´ëŠ” ë°©ë²• ë­ ìˆì–´?",
  "output": "{\"recommend\": \"í”Œë¼ìŠ¤í‹± ì¤„ì´ê¸°ì— ë„ì›€ì´ ë˜ëŠ” ì±Œë¦°ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.\", ...}"
},
{
  "instruction": "ë„ˆëŠ” ì±Œë¦°ì§€ ì¶”ì²œ ì±—ë´‡ì´ì•¼. ...",
  "input": "ì¹œêµ¬ë‘ í• ë§Œí•œ í™˜ê²½ ì±Œë¦°ì§€ ì¶”ì²œí•´ì¤˜",
  "output": "{\"recommend\": \"ì¹œêµ¬ì™€ í•¨ê»˜í•˜ë©´ ë” ì¢‹ì€ ì±Œë¦°ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.\", ...}"
},
{
  "instruction": "ë„ˆëŠ” í”¼ë“œë°± ì–´ì‹œìŠ¤í„´íŠ¸ì•¼. ...",
  "input": "ì´ë²ˆ ì£¼ì— ë¶„ë¦¬ìˆ˜ê±°ë¥¼ 5ë²ˆ í–ˆì–´.",
  "output": "ë¶„ë¦¬ìˆ˜ê±°ë¥¼ ê¾¸ì¤€íˆ ì‹¤ì²œí•˜ì…¨êµ°ìš”! í™˜ê²½ì„ ìœ„í•œ ë©‹ì§„ í–‰ë™ì´ì—ìš”."
},
{
  "instruction": "ë„ˆëŠ” ììœ ëŒ€í™” ì±—ë´‡ì´ì•¼. ...",
  "input": "ë„ˆ ì˜¤ëŠ˜ ë­í–ˆì–´?",
  "output": "ì €ëŠ” ì˜¤ëŠ˜ë„ ì—¬ëŸ¬ë¶„ì„ ë„ìš¸ ì¤€ë¹„ë¥¼ í•˜ê³  ìˆì—ˆì–´ìš”!"
}
```

1. í”„ë¡¬í”„íŠ¸ ì„¤ê³„ íŒ
    - ì—­í• ì„ ëª…í™•íˆ:
        - "ë„ˆëŠ” ì±Œë¦°ì§€ ì¶”ì²œ ì±—ë´‡ì´ì•¼", "ë„ˆëŠ” í”¼ë“œë°± ì–´ì‹œìŠ¤í„´íŠ¸ì•¼", "ë„ˆëŠ” ììœ ëŒ€í™” ì±—ë´‡ì´ì•¼" ë“±
    - í¬ë§·/í†¤/ì–¸ì–´ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ:
        - "ì•„ë˜ JSON í¬ë§·ìœ¼ë¡œë§Œ ë‹µí•´", "í•œê¸€ë¡œë§Œ ë‹µí•´", "í•­ìƒ 3ê°œ ì±Œë¦°ì§€", "ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ" ë“±
    - ì˜ˆì‹œë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨:
        - "ì¶œë ¥ ì˜ˆì‹œ: { ... }"
        - "ë°˜ë“œì‹œ ìœ„ ì˜ˆì‹œì²˜ëŸ¼ë§Œ ë‹µí•´"

### í”¼ë“œë°± ë°ì´í„° ì…‹ ì„¤ì •

- ì„±ê³µ: "{} ì„±ê³µ! ì •ë§ ë©‹ì ¸ìš”. {}"
- ì‹¤íŒ¨: "{}ëŠ”(ì€) ì¡°ê¸ˆ ì•„ì‰¬ì› ì§€ë§Œ, ë„ì „ ìì²´ê°€ ì˜ë¯¸ ìˆì–´ìš”. {}"
- ê·¸ë£¹ ì±Œë¦°ì§€ ì°¸ì—¬: "{} ì±Œë¦°ì§€ì—ë„ ê¾¸ì¤€íˆ ì°¸ì—¬í•˜ì…¨ë„¤ìš”! ëŒ€ë‹¨í•´ìš”. {}"
- ì•„ë¬´ê²ƒë„ ì—†ì„ ë•Œ: "ì´ë²ˆ ì£¼ëŠ” ì±Œë¦°ì§€ í™œë™ì´ ì—†ì—ˆë„¤ìš”. ë‹¤ìŒì—” ìƒˆë¡œìš´ ë„ì „ì„ ê¸°ëŒ€í• ê²Œìš”! {}"

---

### íŠ¸ë ˆì¸ ì½”ë“œ

```python
import os, json, threading
import torch
import matplotlib.pyplot as plt
from transformers import (
    AutoModelForCausalLM, AutoTokenizer,
    TrainingArguments, Trainer,
    EarlyStoppingCallback, TrainerCallback
)
from peft import LoraConfig, get_peft_model
from datasets import Dataset

from fastapi import FastAPI, Request, HTTPException
from fastapi.responses import JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.exceptions import RequestValidationError
from prometheus_client import start_http_server

# 1. ë°ì´í„° ë¡œë“œ ë° í† í¬ë‚˜ì´ì§•
with open("/content/drive/MyDrive/Colab Notebooks/[ì¹´ì¹´ì˜¤ ë¶€íŠ¸ìº í”„]/Leafresh/multitask_dataset.json", "r") as f:
    data = json.load(f)

dataset = Dataset.from_list(data)

model_path = "/content/drive/MyDrive/Colab Notebooks/[ì¹´ì¹´ì˜¤ ë¶€íŠ¸ìº í”„]/Leafresh/mistral-7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_path)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.float16,
    device_map={"": "cuda:0"}
)

# 2. LoRA êµ¬ì„±
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_config)

# 3. í† í¬ë‚˜ì´ì¦ˆ í•¨ìˆ˜
def tokenize_fn(examples):
    texts = [
        f"<s>[INST] {inst}\n{inp} [/INST] {out} </s>"
        for inst, inp, out in zip(examples['instruction'], examples['input'], examples['output'])
    ]
    return tokenizer(texts, truncation=True, padding="max_length", max_length=1024)

tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=dataset.column_names)

# 4. Loss Callback
class LossHistoryCallback(TrainerCallback):
    def __init__(self):
        self.losses = []
        self.steps = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None and "loss" in logs:
            self.losses.append(logs["loss"])
            self.steps.append(state.global_step)

loss_callback = LossHistoryCallback()
early_stopping = EarlyStoppingCallback(early_stopping_patience=4)

# 5. Train/Validation Split
split = tokenized.train_test_split(test_size=0.1, seed=42)

# 6. Trainer ì„¤ì •
args = TrainingArguments(
    output_dir="./lora_adapter",
    num_train_epochs=4, # ì „ì²´ ë°ì´í„°ì…‹ 2ë²ˆ ë°˜ë³µí•™ìŠµ
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    fp16=True,
    save_steps=50,
    logging_steps=5,
    report_to="wandb",
    eval_strategy="steps",   # âœ… ë°˜ë“œì‹œ ì¶”ê°€ (validation í•´ì•¼í•¨)
    eval_steps=50,                  # âœ… evaluation ì£¼ê¸°
    load_best_model_at_end=True,    # âœ… EarlyStopping í•„ìˆ˜
    metric_for_best_model="eval_loss"  # âœ… EarlyStopping í•„ìˆ˜
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=split['train'],
    eval_dataset=split['test'],
    data_collator=lambda data: {
        "input_ids": torch.stack([torch.tensor(f["input_ids"]) for f in data]),
        "attention_mask": torch.stack([torch.tensor(f["attention_mask"]) for f in data]),
        "labels": torch.stack([torch.tensor(f["input_ids"]) for f in data]),
    },
    callbacks=[loss_callback, early_stopping]
)

# 7. í•™ìŠµ ì‹¤í–‰
trainer.train()

# 8. ì €ì¥ ë° Loss ê·¸ë˜í”„
model.save_pretrained("./lora_adapter")
tokenizer.save_pretrained("./lora_adapter")

<div align="center">

plt.plot(loss_callback.steps, loss_callback.losses)
plt.xlabel("Step")
plt.ylabel("Training Loss")
plt.title("Training Loss Curve")
plt.show()

</div>
```

- ì—¬ê¸°ì„œ `gradient_accumulation_steps=8` ì´ê²Œ 2ê°œì”© 8ë²ˆ(=16ê°œ) ìƒ˜í”Œì„ ì²˜ë¦¬í•œ í›„ì—ì•¼  optimizerê°€ í•œ ë²ˆ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸
  - ì‹¤ì§ˆì  batch size = 2 Ã— 8 = 16
  - ë©”ëª¨ë¦¬ ë¶€ì¡±í•  ë•Œ batch sizeë¥¼ í‚¤ìš°ëŠ” íš¨ê³¼

### "ë©”ëª¨ë¦¬ ë¶€ì¡±í•  ë•Œ batch sizeë¥¼ í‚¤ìš°ëŠ” íš¨ê³¼"ì´ê²Œ ë­ì„? ë­” ìƒê´€ê´€ê³„ê°€ ìˆìŒ?

- gradient_accumulation_stepsì™€ batch sizeì˜ ê´€ê³„ë¥¼ ì´í•´í•´ë³´ì.
    1. GPU ë©”ëª¨ë¦¬ì™€ batch sizeì˜ ê´€ê³„
        - batch sizeë¥¼ í‚¤ìš°ë©´ í•œ ë²ˆì— ë” ë§ì€ ë°ì´í„°ë¥¼ ì²˜ë¦¬ â†’ ë” ë§ì€ ë©”ëª¨ë¦¬ê°€ í•„ìš”
        - í•˜ì§€ë§Œ GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•˜ë©´ batch sizeë¥¼ í¬ê²Œ í•  ìˆ˜ ì—†ìŒ
    2. gradient_accumulation_stepsì˜ ì—­í• 
        - gradient_accumulation_stepsëŠ”"ì‘ì€ batch sizeë¡œ ì—¬ëŸ¬ ë²ˆ gradientë¥¼ ëˆ„ì í•´ì„œ, ë§ˆì¹˜ í° batch sizeë¡œ í•™ìŠµí•˜ëŠ” íš¨ê³¼"ë¥¼ ì¤€ë‹¤
    3. ì˜ˆì‹œ
        - ì›í•˜ëŠ” ì‹¤ì§ˆì  batch size: 32
        - GPU ë©”ëª¨ë¦¬ í•œê³„ë¡œ per_device_train_batch_size=4ë§Œ ê°€ëŠ¥
        - gradient_accumulation_steps=8ë¡œ ì„¤ì •
    4. ë™ì‘ ë°©ì‹
        - 4ê°œì”© 8ë²ˆ(ì´ 32ê°œ) ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ë©´ì„œ gradientë¥¼ ëˆ„ì 
        - 8ë²ˆì§¸ stepì—ì„œ í•œ ë²ˆ optimizer step(ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸)
        - â†’ ì‹¤ì§ˆì ìœ¼ë¡œ batch size=32ë¡œ í•™ìŠµí•œ ê²ƒê³¼ ê±°ì˜ ë™ì¼í•œ íš¨ê³¼

---

### Train í…ŒìŠ¤íŠ¸

1. ë³€ìˆ˜ ì„¤ì • 

```python
    num_train_epochs=7, # ì „ì²´ ë°ì´í„°ì…‹ ë°˜ë³µí•™ìŠµ íšŸìˆ˜
    per_device_train_batch_size=1, # GPU í™˜ê²½ ì—¬ê±´ìƒ 1ë¡œ ê³ ì •
    gradient_accumulation_steps=8, 
```

2. ë°ì´í„° ì…‹:

- 138ê°œ
- Lossê°’ ë³€í™”

<div align="center">

| **Step** | **Training Loss** | **Validation Loss** |
| -------- | ----------------- | ------------------- |
| 50       | 4.401700          | 4.598206            |
| 100      | 4.384800          | 4.559369            |
| 150      | 4.367800          | 4.551898            |

</div>

<div align="center">

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/A4F3B519-39D8-480E-B22A-518ECCFCF775_2/RocxNMP6D5QaZoNMGsmcxAp3675vF1RjF9TpY9HBaesz/Image.png)

</div>

- 500ê°œ
- Lossê°’ ë³€í™”

<div align="center">

| **Step** | **Training Loss** | **Validation Loss** |
| -------- | ----------------- | ------------------- |
| 50       | 4.645700          | 4.492012            |
| 100      | 4.556200          | 4.426577            |
| 150      | 4.432800          | 4.408548            |
| 200      | 4.433300          | 4.403012            |
| 250      | 4.598700          | 4.401348            |
| 300      | 4.277900          | 4.400284            |
| 350      | 4.472500          | 4.400099            |

</div>

<div align="center">

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/62502B2B-EFEE-482A-8CA8-0111949AAA62_2/iN7hweYbSsLQOgLPkWSpXr68TqfQTDLql0vYrqLsCPgz/Image.png)

</div>

- 2000ê°œ
- Lossê°’ ë³€í™”(ì²­ë¡ìƒ‰)

<div align="center">

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/7973B2CB-A9F9-42FA-89BB-7A831AEAB3E9_2/rLf8pGZCyXhH5GvbLpzXGXlEuFA0ayaJj5AA5CPy9FIz/Image.png)

</div>

<div align="center">

| **Step** | **Training Loss** | **Validation Loss** |
| -------- | ----------------- | ------------------- |
| 50       | 4.620100          | 4.637240            |
| 100      | 4.611300          | 4.567899            |
| 150      | 4.586400          | 4.554155            |
| 200      | 4.444700          | 4.549914            |
| 250      | 4.744000          | 4.546744            |
| 300      | 4.646400          | 4.544673            |
| 350      | 4.588600          | 4.542412            |
| 400      | 4.587200          | 4.540801            |
| 450      | 4.697900          | 4.539388            |
| 500      | 4.494200          | 4.537243            |
| 550      | 4.405500          | 4.535868            |
| 600      | 4.634600          | 4.534741            |

</div>

- ë³€í™”ëŸ‰ì´ ì ì–´ ì¡°ê¸°ì¤‘ë‹¨í•¨.
  > Lossê°’ì´ ì•ˆì¤„ì–´ë“¦.