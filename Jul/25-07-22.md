# 오늘 내가 배운 것들(Today I Learned)

- 피드백 아키텍처 수정(V3) & 챗봇 파인튜닝 진행을 하였다.

## 1. JSON 내에서의 줄바꿈 표기

- JSON 문자열에서 줄바꿈은 항상 \n으로 저장됩니다.
- 예: "text": "안녕하세요.\n반갑습니다."
- 실제로 파싱해서 Python 객체(딕셔너리)로 만들면,
- \n이 실제 줄바꿈 문자로 변환됩니다.

---

## 2. SSE/스트리밍에서의 줄바꿈 처리

- SSE로 전송할 때는, 파싱된 문자열(즉, 실제 \n이 들어간 상태)을 그대로 보내면
- 클라이언트(브라우저, 프론트엔드)에서 줄바꿈이 적용됩니다.
- 만약 JSON 문자열을 그대로 보내면(즉, \n이 포함된 상태),
- 프론트엔드에서 파싱 후 출력할 때
- \n이 실제 줄바꿈이 아니라, 문자 \n으로 보일 수 있습니다.

---

## 3. vLLM/챗봇 파이프라인에서의 권장 방식

- output을 JSON 문자열로 저장할 때는 \n이 맞습니다.

- (JSON 표준에 따라 이스케이프)

- 실제 챗봇 응답으로 쓸 때는,
  - JSON을 파싱해서 Python 객체로 만든 뒤
  - 사용자에게 보여줄 때는 실제 줄바꿈 문자(\n)가 들어가야
  - 프론트엔드에서 줄바꿈이 적용됩니다.

> 그렇단 말이지?

---

## 결론 (추천)

- 데이터셋 저장 시:
  - JSON 문자열 내에서는 \n이 맞음 (자동 처리됨)
- 챗봇 응답 시:
  - JSON 파싱 후,
  - Python 문자열에서 \n이 실제 줄바꿈으로 들어가야 함
- SSE/vLLM 스트리밍:
  - 파싱된 문자열(실제 \n 포함)을 스트림으로 보내면
  - 프론트엔드에서 줄바꿈이 잘 보임

---

## 각 모듈의 의미

### 1. q_proj, k_proj, v_proj, o_proj

- 이 네 개는 Self-Attention 연산에서 핵심이 되는 선형 변환 레이어다.
- 입력 임베딩 → Query/Key/Value로 변환 → 어텐션 → Output projection
  - q_proj: Query projection (쿼리 변환)
  - k_proj: Key projection (키 변환)
  - v_proj: Value projection (값 변환)
  - o_proj: Output projection (어텐션 결과 변환)

### 2. gate_proj, up_proj, down_proj

- 이 세 개는 Feed-Forward Network(FFN) 부분에서 사용되는 선형 레이어이다.
  - up_proj: FFN에서 차원을 확장하는 레이어 (보통 hidden_dim → ffn_dim)
  - gate_proj: Gated Linear Unit(GLU) 구조에서 게이트 역할을 하는 레이어
  - down_proj: FFN에서 차원을 다시 줄이는 레이어 (ffn_dim → hidden_dim)

### 실행

- 실행을 근데 내가 처음엔 LoRA구성을 아래와 같이 거의 풀파인튜닝으로 설정했었다..

```python
# ✅ 2. LoRA 구성
lora_config = LoraConfig(
r=16,
lora_alpha=32,
target_modules=[
    "q_proj", "v_proj", "k_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
],
lora_dropout=0.05,
bias="none",
task_type="CAUSAL_LM"
)
```

- 그래서 파인튜닝 과정이 처음 50step부터 아래와 같이 파격적으로 낮게 나왔다.

<div align="center">

| **Step** | **Training Loss** | **Validation Loss** |
| -------- | ----------------- | ------------------- |
| 50       | 0.125700          | 0.133153            |
| 100      | 0.111000          | 0.095606            |
| 150      | 0.065400          | 0.088307            |
| 200      | 0.088700          | 0.082485            |
| 250      | 0.073300          | 0.077784            |
| 300      | 0.073400          | 0.072931            |
| 350      | 0.068400          | 0.069739            |
| 400      | 0.053200          | 0.067034            |
| 450      | 0.050300          | 0.065709            |
| 500      | 0.049000          | 0.064869            |

</div>

<p align="center">

<img src= "https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/C991F5D7-DBA2-4766-9321-D7E862C78755_2/d1YonEmq7rn2iyvwosiCGyeMwyYXo8YCppFyXlxD5VIz/Image.png" alt="그래프" />

</p>

- Validation Loss도 낮은걸 보면 모델이 암기하는게 아니라, 실제로 잘 예측하고 있다는 뜻일 수 있음ㅇㅇ
- 하지만 너무 빨리 수렴하면 오버피팅 위험이 있음
  - 실제로 inference(추론)에서 "헛소리"가 늘어나거나
  - 새로운 입력에 대해 일반화가 안 될 수 있음
- 왜 이 현상이 걱정되냐면, 내 챗봇 기능이 단순히 추천이 아니라, 사용자와 대화(free-text)를 통해 친환경 챌린지를 추천해줘야 하기 떄문이다….!

## 그래서 꿀팁ㅇㅇ

1. target_modules를 적절히 줄여서 실험
    - 처음엔 "q_proj", "v_proj"만,
    - 그 다음 "k_proj", "o_proj" 추가,
    - 점진적으로 늘려가며 Loss와 실제 성능을 비교
2. 학습률을 더 낮춰서 실험
    - 5e-5 → 2e-5, 1e-5 등으로 줄여보세요.
3. Validation set에서 실제 샘플을 뽑아 직접 확인
    - Loss가 낮아도, 실제 답변이 이상하면 오버피팅/암기일 수 있습니다.
4. r, alpha 등 LoRA 하이퍼파라미터도 조정
    - r=8, alpha=16 등으로 줄여서 실험해보세요.

---

+ config.json 수정
    - `"chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}<s>[INST] {{ message['content'] }} [/INST]{% else %} {{ message['content'] }} </s>{% endif %}{% endfor %}`”. 추가함.

```json
{
"architectures": [
"MistralForCausalLM"
],
"attention_dropout": 0.0,
"bos_token_id": 1,
"eos_token_id": 2,
"head_dim": null,
"hidden_act": "silu",
"hidden_size": 4096,
"initializer_range": 0.02,
"intermediate_size": 14336,
"max_position_embeddings": 32768,
"model_type": "mistral",
"num_attention_heads": 32,
"num_hidden_layers": 32,
"num_key_value_heads": 8,
"rms_norm_eps": 1e-05,
"rope_theta": 1000000.0,
"sliding_window": null,
"tie_word_embeddings": false,
"torch_dtype": "bfloat16",
"transformers_version": "4.53.2",
"use_cache": true,
"vocab_size": 32768,
"chat_template": "{% for message in messages %}{% if message['role'] == 'user' %}<s>[INST] {{ message['content'] }} [/INST]{% else %} {{ message['content'] }} </s>{% endif %}{% endfor %}"
}
```