# 오늘 내가 배운 것들(Today I Learned)

- 챗봇이랑 피드백 vLLM 적용을 시켰다.
- 일단 답변 속도가 엄청 빠르다!

---

## 실행 순서

1. `requirements.txt` 파일 실행
2. `pip install vllm` 명령어 추가 실행
    > vLLM이 xformers를 빌드할 때 torch가 이미 설치되어 있어야 함

- vLLM 서버 실행 명령어

```plaintext
python3 -m vllm.entrypoints.openai.api_server   --model /home/ubuntu/mistral/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db   --host 0.0.0.0   --port 8800
```

- 또한 Text/LLM/router/monitoring_router.py 로 모니터링 코드 추가하였다.

```python
from prometheus_client import (
    generate_latest, Counter, Gauge, Histogram, REGISTRY
)
from prometheus_client import process_collector, platform_collector

# collector 등록은 이미 되어 있으면 생략
if not any(type(c).__name__ == "ProcessCollector" for c in REGISTRY._collector_to_names):
    process_collector.ProcessCollector()
if not any(type(c).__name__ == "PlatformCollector" for c in REGISTRY._collector_to_names):
    platform_collector.PlatformCollector()
   
# 모델 추론 시간 (Histogram): 추론 시간 분포 파악
model_inference_duration_seconds = Histogram(
    'ai_model_inference_duration_seconds', 'Duration of AI model inference in seconds',
    buckets=(0.001, 0.01, 0.1, 1.0, 5.0, 10.0, float('inf'))
)

# 현재 서비스 중인 모델 버전 (Gauge)
model_version = Gauge('ai_current_model_version', 'Current AI model version')
model_version.set(2.1) # 실제 사용 중인 AI 모델 버전으로 설정
```

- `main.py` 파일도 추가 수정하였다.

```python
from prometheus_client import start_http_server

if __name__ == "__main__":
    # 9104 포트에서 exporter 실행
    start_http_server(9104)
    uvicorn.run("main:app", host="0.0.0.0", port=8000)
```

- 따라서 기존 `uvicorn main:app ~` 으로 FastAPI 서버 실행했었는데, 코드 수정으로 인해 `python main.py` 실행 시  metrics 연동(port: 9104) 및 FastAPI 서버 동시 실행

## 서버 키는 순서

- `sudo systemctl start redis-server` : Redis 서버 실행
- `rq woker feedback`: 큐 워커 실행
- `uvicorn main:app --host 0.0.0.0 —port 8000` : FastAPI서버 실행
- `python -m vllm.entrypoints.openai.api_server --model /home/ubuntu/mistral/models--mistralai--Mistral-7B-Instruct-v0.3/snapshots/e0bc86c23ce5aae1db576c8cca6f06f1f73af2db --port 8800 --host 0.0.0.0` : vLLM 서버 실행