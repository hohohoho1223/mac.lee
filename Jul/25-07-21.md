# 오늘 내가 배운 것들(Today I Learned)

- 하룻밤 사이에 데이터 10000개로 v2를 돌려놓고 잤다.
- 아래는 Trainer 설정이다.
  > v2(10000건)은 스텝수를 8->16으로 조정

```json
# 6. Trainer 설정
args = TrainingArguments(
    output_dir="./lora_adapter_v2",
    num_train_epochs=3, # 전체 데이터셋 반복학습 횟수
    per_device_train_batch_size=1, # GPU 환경 여건상 1로 고정
    gradient_accumulation_steps=16, # v2(10000건)은 스텝수를 8->16으로 조정 
    fp16=True,
    save_steps=50,
    logging_steps=5,
    report_to="wandb",
    eval_strategy="steps",   # ✅ 반드시 추가 (validation 해야함)
    eval_steps=50,                  # ✅ evaluation 주기
    load_best_model_at_end=True,    # ✅ EarlyStopping 필수
    metric_for_best_model="eval_loss",  # ✅ EarlyStopping 필수
    learning_rate=2e-4
)
```

- 아래는 학습 결과이다.

<div align="center">

| **Step** | **Training Loss** | **Validation Loss** |
| -------- | ----------------- | ------------------- |
| 50       | 4.685500          | 4.649430            |
| 100      | 4.794500          | 4.640215            |
| 150      | 4.696400          | 4.633816            |
| 200      | 4.746500          | 4.630203            |
| 250      | 4.603300          | 4.623814            |
| 300      | 4.620100          | 4.621994            |
| 350      | 4.672700          | 4.620154            |
| 400      | 4.655700          | 4.619279            |
| 450      | 4.696000          | 4.617799            |
| 500      | 4.589800          | 4.617834            |
| 550      | 4.567600          | 4.616465            |
| 600      | 4.735300          | 4.615729            |
| 650      | 4.575000          | 4.614254            |
| 700      | 4.598400          | 4.613454            |
| 750      | 4.639500          | 4.612561            |
| 800      | 4.633700          | 4.611720            |
| 850      | 4.473900          | 4.611136            |
| 900      | 4.664800          | 4.610097            |
| 950      | 4.628600          | 4.609463            |
| 1000     | 4.570600          | 4.609000            |
| 1050     | 4.594400          | 4.608626            |
| 1100     | 4.592100          | 4.608234            |
| 1150     | 4.635400          | 4.607999            |
| 1200     | 4.578500          | 4.607731            |
| 1250     | 4.603100          | 4.607697            |
| 1300     | 4.571700          | 4.607403            |
| 1350     | 4.640600          | 4.607166            |
| 1400     | 4.578900          | 4.607059            |
| 1450     | 4.531500          | 4.606997            |

</div>

<p align="center">

<img src="https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/F5ECEBA3-B7C5-4EEB-9608-192EBD8EF5C4_2/zY3NkzYh0435cRolx6r2Mpb3v9eWAtIsOyxRyVXjyTUz/Image.png" alt="로스값 그래프"/>

</p>

- 음..여전히 Loss값이 4범위에서 움직인다
  > 허깅페이스 주소: [https://huggingface.co/maclee123/leafresh_merged_v2/tree/main](https://huggingface.co/maclee123/leafresh_merged_v2/tree/main)
- 출력결과:
  - “challenges”에 들어갈 내용이 "recommend"에 같이 들어감

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/BCAE41A7-EBD7-4AAF-8CF0-8D3BADBCAF99_2/qx4D1IImp2anC9pjZ9ImAaMthlFPohxphW4ascp4CMwz/Image.png)

- 학습데이터 중에  일부 output만 줄바꿈 텍스트, 일부만 JSON 문자열이면

→ 모델이 혼란스러워져서 recommend에만 JSON을 몰아넣거나, 구조가 깨질 수 있다고함

## 왜 이런 현상이 발생하는가?

1. 파인튜닝 데이터에 혼합 포맷이 섞여 있어서
    - 일부는 JSON 이스케이프 문자열, 일부는 줄바꿈 텍스트 등
    - 모델이 "꼭 JSON 구조로 내보내야 한다"는 규칙을 제대로 학습하지 못함
2. 모델이 "recommend"에 전체 JSON을 몰아넣는 패턴을 학습
    - 데이터셋에 recommend에 JSON을 문자열로 넣은 예시가 있거나,
    - 프롬프트가 충분히 강하게 "recommend/challenges를 최상위 필드로"를 강조하지 않아서