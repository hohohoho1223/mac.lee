# 오늘 내가 배운 것들(Today I Learned)
## 딥다이브 진행

---

### 시계열 데이터 분석이 데이터를 시간 기반으로 이해하는 데 중요한 이유를 설명하시오.🤔

1. 시간의 흐름 별로 데이터를 분류하여 패턴을 알아낼수 있다.(계절성 및 트랜드)

2. 이상치(outlier)를 탐지하여 급격한 변화를 알아낼수 있다.

**a. Standard Deviation (정규분포) :** 데이터의 표준편차를 이용하여 탐지

> - 1표준편차 : 평균을 기준으로 1시그마 범위 이상이면 이상치로 판단
> - 2표준편차 : 평균을 기준으로 양쪽 2시그마 범위이상이면 이상치로 판단
> - 3표준편차 : 평균을 기준으로 양쪽 3시그마 범위 이상이면 이상치로 판단
    **식스시그마(양쪽 3시그마씩)를 이용하여 품질관리에도 쓰임!(수업시간에 배웠어요)
> - 이때 Z-score 공식을 사용한다.

![z = \frac{X - \mu}{\sigma}](https://latex.codecogs.com/png.latex?z%20=%20%5Cfrac%7BX-%5Cmu%7D%7B%5Csigma%7D)

> - Z-score > 3(sigma) | Z-score < -3 이면 이상치로 처리

**b. IQR (Interquartile Range) with Box plots**

- IQR : Q3-Q1(제 3사분위 값 - 제 1사분위 값) 으로 구함

> 왜 IQR을 쓰는가? → 중앙값 주변 50% 범위를 나타내는데, 이를 통해 데이터의 집중도를 확인할 수 있기 때문이다!

- 1.5*IQR 값을 이용하여 구간을 정하는데, 1.5보다 큰수를 곱하기도 하며 최극단의 이상치를 판별할 수 있다.

---

3. 이동평균기법 (SMA,EMA,WMA 등) 사용하여 추세흐름을 파악할수 있다.

```py
#단순이동평균(SMA, Simple Moving Average) 
df["SMA"] = df["value"].rolling(window='윈도우크기').mean()
#지수이동평균(EMA, Exponential Moving Average)
df["EMA"] = df["value"].ewm(span = '윈도우크기', adjust=False).mean()
    # adjust=False : 가중치 조정 없이 단순 현재 데이터와 과거데이터의 평균 계산
#가중이동평균(WMA, Weighted Moving Average)
weights = np.arange(1,윈도우크기+1) #가중치 배열 설정
df["WMA"] = df.["value"].apply(lambda x: np.dot(x,weights)/weights.sum(),raw=True)
    # weights 값을 설정하여 행렬간 내적 및 평균을 통해 가중이동평균을 구한다.
```

---

4. 다른 시계열 간의 비교로 상관관계를 분석하여 인과관계를 알 수 있다.

- [피어슨 상관 계수](https://ko.wikipedia.org/wiki/%ED%94%BC%EC%96%B4%EC%8A%A8_%EC%83%81%EA%B4%80_%EA%B3%84%EC%88%98)(Pearson Correlation Coefficient)

> - +1 ~ -1 사이의 값을 가지며, +1은 양의 선형상관관계 -1 은 음의 선형상관관계를 의미함
> - 피어슨 상관계수는 선형관계만 측정함(비선형 관계는 X)
> - 피어슨 상관계수는 데이터가 등분산성을 가진다고 가정 하므로, 변수 데이터가 정규 분포를 가지지 않거나 outlier가 발생할 경우 분석 결과가 틀어짐

- [켄달 상관 계수](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient)(Kendall Correlation Coefficient)

> - -1에서 +1 사이의 값을 가진다.
> - +1: 완벽한 양의 순위 관계 (A값이 커지면 B값도 항상 커진다)
> - 1: 완벽한 음의 순위 관계
> - 0: 순위 간의 관계가 없음

- 순위관계 : 데이터셋 내에서 각 값이 상대적으로 어느 위치에 있는지를 나타내는 것

```py
import pandas as pd
from scipy.stats import kendalltau

# 샘플 데이터 생성
data = {'키': [168,170,175,176,178,180,181,182],
        '몸무게': [68,70,60,65,72,78,80,75]}

df = pd.DataFrame(data)

# Kendall 순위 상관계수 계산
correlation, p_value = kendalltau(df['키'], df['몸무게'])

# 결과 출력
print(f"Kendall 순위 상관계수: {correlation:.4f}")
print(f"P-value: {p_value:.4f}")
```

---

5. ‘ARIMA’와 같은 시계열 분석 기법을 통해 예측 모델링 설계가 가능하다.

> - ARIMA = AR(자기회귀) + I (차분) + MA(이동평균)
> - 자기회귀(AR) : 과거의 값이 현재 값에 영향을 미친다는 가정을 기반
> - 차분(Differencing) : ‘현재 데이터 값-이전 데이터 값’이므로 변화량을 파악함

![Y't = Y_t - Y_{t-1}](https://latex.codecogs.com/png.latex?Y't%20=%20Y_t%20-%20Y%7Bt-1%7D)

> - 이동평균(MR) : 과거의 예측오차가 현재 값에 영향을 미친다는 가정을 기반

---

- PDF vs CDF vs PPF

- PDF (확률 밀도 함수, Probability Density Function ): 특정 지점에서의 밀도를 나타내며, 이값이 높을수록 해당지점에서의 확률이 상대적으로 높음

> - 높은 PDF값 : 특정 값이 정규분포에서 자주 발생할 가능성이 높다.
> - PDF 값 자체는 “확률”이 아닌, “확률밀도”다! 특정 범위의 확률을 계산하려면 PDF값을 적분해야함
> - PDF값이 0.1이면 그것이 10%확률을 의미하진 않음.

- CDF (누적 분포 함수, Cumulative Distribution Function) : 특정 값까지의 누적확률을 나타냄

> - CDF는 항상 증가하는 함수임.(당연 계속된 누적확률을 나타내므로)
> - 0과 1사이에 있으며 (x)가 증가할수록 CDF값 증가
> - CDF 는 PDF를 적분한것임! → 특정 값(x)까지의 정적분 & 밀도의 누적합

- PPF (분위분 함수, Percent-Point Function)

> - 주어진 확률에 대해 해당 확률을 가지는 값(분위수)을 제공하는 함수
> - PPF는 CDF의 역함수이다.
> - CDF가 특정 확률에 도달하는 값을 찾는 반면, PPF는 특정값에 도달하는 확률이다.

---

> 출처 : [https://velog.io/@stand_hyo/Data-데이터-전처리-이상치Outlier와-결측치Missing-Value-처리하기](https://velog.io/@stand_hyo/Data-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%84%EC%B2%98%EB%A6%AC-%EC%9D%B4%EC%83%81%EC%B9%98Outlier%EC%99%80-%EA%B2%B0%EC%B8%A1%EC%B9%98Missing-Value-%EC%B2%98%EB%A6%AC%ED%95%98%EA%B8%B0)