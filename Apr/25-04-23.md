# 오늘 내가 배운 것들(Today I Learned)

- 오늘은 파이널프로젝트 에서 "랭체인 구조화"를 진행하였다.
- 또한 아키텍처 수정도 진행하였다.(그냥 안끝나는중)

---

# LLM모델을 자체 서빙 코드
- 아래 코드는 일부 내용을 발췌했다

```py
# chatbot_app_vertex.py
%%writefile chatbot_app_vertex.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from llm_model_vertex import get_llm_response
from langchain_rag_chain import qa_chain
import json

app = FastAPI()

class CategoryRequest(BaseModel):
    userId: int
    category: str
    location: str
    work_type: str

class FreeTextRequest(BaseModel):
    userId: int
    location: str
    work_type: str
    user_message: str

@app.post("/chatbot/select-category")
def select_category(req: CategoryRequest):
    prompt = f"""
    {req.location} 환경에 있는 {req.work_type} 사용자가 {req.category}를 실천할 때,
    절대적으로 환경에 도움이 되는 챌린지를 아래 JSON 형식으로 3가지 추천해줘:
    반드시 순수 JSON만 출력해줘.
    {{
        "status": 200,
        "message": "성공!",
        "data": {{
            "recommand": "설명 텍스트",
            "challenges": [
                {{"title": "챌린지 이름", "description": "설명"}}
            ]
        }}
    }}
    """
    return get_llm_response(prompt)

@app.post("/chatbot/freetext")
def free_text(req: FreeTextRequest):
    if not req.user_message.strip():
        raise HTTPException(status_code=400, detail="user_message는 필수입니다.")

    if not isinstance(req.user_message, str) or len(req.user_message.strip()) < 5:
        raise HTTPException(status_code=422, detail="user_message는 문자열이어야 하며, 최소한의 의미를 가져야 합니다.")

    prompt = f"""
    사용자 메시지: "{req.user_message}"
    지역: {req.location}, 직업: {req.work_type}
    환경에 도움이 되는 챌린지를 아래 JSON 형식으로만 3가지 추천해주세요:
    {{
        "status": 200,
        "message": "성공!",
        "data": {{
            "recommand": "설명 텍스트",
            "challenges": [
                {{"title": "챌린지 이름", "description": "설명"}}
            ]
        }}
    }}
    """
    return get_llm_response(prompt)

@app.post("/chatbot/freetext-rag")
def freetext_rag(req: FreeTextRequest):
    if not req.user_message.strip():
        raise HTTPException(status_code=400, detail="user_message는 필수입니다.")
    if not isinstance(req.user_message, str) or len(req.user_message.strip()) < 5:
        raise HTTPException(status_code=422, detail="user_message는 문자열이어야 하며, 최소한의 의미를 가져야 합니다.")

    # LangChain 체인을 통한 멀티스텝 LLM 호출
    result = qa_chain.run(req.user_message)

    # 응답에서 JSON 추출이 안 되면 예외 처리
    try:
        return json.loads(result)
    except Exception:
        return {
            "status": 500,
            "message": "❌ LangChain 응답 파싱 실패",
            "data": None
        }
```

```py
%%writefile langchain_rag_chain.py
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.vectorstores import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain.llms.base import LLM
from vertexai.generative_models import GenerativeModel
from google.oauth2 import service_account

# Gemini LLM 래퍼 클래스
def init_gemini_llm(credentials_path, project, location, model_name):
    class GeminiLLM(LLM):
        def __init__(self):
            credentials = service_account.Credentials.from_service_account_file(credentials_path)
            vertexai.init(project=project, location=location, credentials=credentials)
            self.model = GenerativeModel(model_name)

        def _call(self, prompt, stop=None):
            response = self.model.generate_content(prompt)
            return getattr(response, "text", "")

        @property
        def _llm_type(self):
            return "gemini-vertex"

    return GeminiLLM()

# 환경 설정
CREDENTIALS_PATH = "/content/kakao-project-457106-b926aa186fc4.json"
PROJECT = "kakao-project-457106"
LOCATION = "us-central1"
MODEL_NAME = "gemini-1.5-flash"

llm = init_gemini_llm(CREDENTIALS_PATH, PROJECT, LOCATION, MODEL_NAME)

# 벡터 DB 설정 (Chroma 기반)
embedding_fn = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embedding_fn)
retriever = vectorstore.as_retriever()

# 프롬프트 템플릿
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
다음 문서를 참고하여 사용자 요청에 맞는 친환경 챌린지를 JSON 형식으로 3개 추천해주세요.

문서:
{context}

요청:
{question}

아래 포맷을 그대로 지켜서 JSON만 출력하세요:
{
  "status": 200,
  "message": "성공!",
  "data": {
    "recommand": "...",
    "challenges": [
      {"title": "...", "description": "..."},
      {"title": "...", "description": "..."},
      {"title": "...", "description": "..."}
    ]
  }
}
"""
)

# LangChain 체인 정의
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt}
)

# 외부에서 import할 수 있게 export
__all__ = ["qa_chain"]
```

- 여기서 의문점이 들었다

## 이미 chatbot_app_vertex.py에서 프롬프트가 설계되어있는데, LanChain 쓸 때 왜 langchain_rag_chain.py에 또 PromptTemplate이 필요하지?
> 나는 하나의 프롬프트 랭체인을 설계하는걸로 알고 있었다.

| **구분**                                    | **목적**                                      | **프롬프트 위치**                |
| ----------------------------------------- | ------------------------------------------- | -------------------------- |
| **기존 구조 (llm_model_vertex.py)**           | FastAPI에서 바로 Gemini 호출하는 **단일 스텝 처리**       | get_llm_response() 안에 하드코딩 |
| **LangChain 구조 (langchain_rag_chain.py)** | 검색 → 문서 조립 → 프롬프트로 **LLM에 전달하는 멀티스텝 체인 구성** | PromptTemplate으로 템플릿화      |

- 왜 LangChain에서 `PromptTemplete`을 따로 쓰는가?
    - LangChain에서의 프롬프트도 **하나의 모듈처럼 재사용이 가능한 구성요소로 취급함
    - 그래서 "문서 + 질문 -> 프롬프트로 조립" 이라는 멀티스텝 중간 단계를 자동으로 처리하게 만들기 위한 것
- 결론: 프롬프트가 중복되는게 아니라 목적이 다르기 때문에 분리한 것이다!
- 내가 이해한 바로는 우선 `def free_text()` 에서 LLM호출로 답변을 받고, 해당 답변과 참고 문서(RAG기법)를 체인처럼 엮은다음 최종으로 LLM(PromptTemplate)에다가 호출하여 답변을 받는것 이라고 이해했다.

- 전체흐름
    ```mermaid
    graph LR
    A[사용자 자유 입력] --> B[LangChain: question 등록]
    B --> C[Retriever → 관련 문서 검색 (context)]
    C --> D[PromptTemplate에 context + question 삽입]
    D --> E[조립된 프롬프트 → LLM 호출 (get_llm_response 내부)]
    E --> F[LLM 응답(JSON 챌린지)]
    ```
    - 나는 여기서 벡터 DB는 단순 .txt 넣는 저장소라고 생각이 들어서 왜 벡터DB가 차이가 나는거지? 라는 의구심이 들었다. 
    > GPT담변: 벡터 DB는 단순 저장소가 아니라 “문장을 수치화(임베딩)해서 유사도를 기준으로 검색하는 특수한 데이터베이스”입니다.
    - 그렇다고 한다
    - 그럼 일반 DB와 벡처 DB의 차이는 뭘까?
    | **항목** | **일반 RDB (예: PostgreSQL)**  | **벡터 DB (예: Qdrant, Pinecone)** |
| ------ | --------------------------- | ------------------------------- |
| 저장 대상  | 문자열, 숫자 등 명시적 스키마 데이터       | 텍스트 → 임베딩 벡터 (고차원 float 리스트)    |
| 조회 방식  | WHERE 절로 정확히 일치하는 데이터 검색    | 유사한 의미를 가진 문장을 찾아냄 (코사인 유사도 등)  |
| 활용 목적  | 트랜잭션, 필터링 중심 (예: 유저, 주문 정보) | 자연어 기반 유사 질의 대응 (예: 챗봇, 검색)     |
| 데이터 구조 | 테이블(행/열)                    | 벡터 + 메타데이터 구조                   |
| 인덱싱 방식 | B-Tree, Hash                | HNSW, IVF, PQ 등 고속 벡터 인덱싱       |
- 벡터화로 임베딜을 해서 코사인 유사도를 통해 의미를 가진 자료를 찾는것!
    - .txt 파일은 벡터 DB에 들어가기 위한 "소스" 일뿐
    - 실제 벡터DB는 텍스트 -> 벡터 변환 + 고속 유사도 검색까지 포함된 기능을 제공
    - 암튼 .txt파일의 자연어를 벡터화해서 벡터 DB에 넣는거임 .txt파일이 DB가 아니라

## 그럼 벡터DB가 다양한 이유는 뭔데?

1. **벡터 검색 성능 향상 기법이 다름**

- 각 DB마다 **“벡터를 어떻게 인덱싱할 것인가”**에 대한 전략이 다르다
- 벡터는 수천-수만 차원의 **실수 배열 이기떄문에 일반 SQL처럼 WHERE절로 못찾음**

| **DB**       | **검색 알고리즘 예시**                | **특징**                  |
| ------------ | ----------------------------- | ----------------------- |
| **FAISS**    | IVF, HNSW, PQ                 | Facebook이 개발, 로컬에서 빠름   |
| **Qdrant**   | HNSW + SIMD 최적화               | CPU 기반 빠른 검색, 필터링 기능 우수 |
| **Pinecone** | 고유 벡터 엔진 (유사 HNSW)            | 클라우드 전용, 관리형, 멀티테넌시     |
| **Weaviate** | HNSW + Graph + Object Storage | 메타데이터/객체검색 강함           |
| **Milvus**   | IVF_FLAT, HNSW, GPU 지원        | 대용량, 병렬성 좋음             |
| **Chroma**   | sqlite 기반 내부구조                | 간편하지만 고성능은 아님 (실험용)     |

2. **메타데이터 필터링 지원 여부**

> 단순 유사도만 보는게 아니라 location = “도시” AND category = “제로웨이스트” 같이 필터도 걸고 싶을 떄 

| **벡터 DB** | **메타데이터 필터링** | **설명**              |
| --------- | ------------- | ------------------- |
| Qdrant    | 지원            | 정교한 필터 + AND/OR/NOT |
| FAISS     | 미지원           | 오로지 벡터 유사도만         |
| Pinecone  | 지원            | 관리형에서 필터 가능         |
| Chroma    | 제한적           | JSON 기반 but 성능은 낮음  |

3. **배포/확장성 차이**

> 로컬 -> 클라우드 -> API 연동 방식까지 다양

| **항목**    | **FAISS**    | **Qdrant** | **Pinecone** | **Chroma** |
| --------- | ------------ | ---------- | ------------ | ---------- |
| 로컬 실행     | ✅            | ✅ (Docker) | ❌            | ✅          |
| 클라우드 API  | ❌            | ✅          | ✅ (관리형 SaaS) | ❌          |
| 수십만~백만 벡터 | ⚠️ (RAM 많아야) | ✅          | ✅            | ❌          |
| 실시간 서비스   | ❌            | ✅          | ✅            | ❌          |

## 결론 :

- **벡터 DB는 “속도”, “필터링”, “확장성”, “배포 방식” 이 4가지를 기준으로 기술이 달라진다.**
- 단순히 벡터 저장이 아니라, 벡터 검색을 **빠르고 정확하고 안전하게 할 수 있도록** 각자 방식이 다름

---

- 그럼 해당 프로젝트(Leafresh)는 JSON 형태로 전달하므로 Chroma랑 어울리지 않을까?
    - Chroma가 JSON형태로 저장할 수 있는건 맞지만
    - API 연동용 실서비스에 쓰기엔 리스크가 존재한다.
    ```json
    vectorstore.add_texts(
    texts=["텀블러 사용 챌린지"],
    metadatas=[{"category": "제로웨이스트", "location": "도시"}]
    )
    ```
    -  몰론 내부적으론 “벡터 + JSON 메타데이터" 구조가 맞음
    - 하지만 이걸 “API 기반으로 안정적으로 서빙한다?”는 다른 문제임

- 비교
| **항목**                        | **Qdrant**           | **FAISS**    | **Pinecone**   | **Weaviate** | **Chroma** |
| ----------------------------- | -------------------- | ------------ | -------------- | ------------ | ---------- |
| **검색 속도** <br/>(1M 벡터, Top-5) | **~15ms**            | ~20ms        | ~30ms          | ~22ms        | ~25ms      |
| **확장성** <br/>(수십만 벡터 이상)      | ✅ 매우 좋음              | ❌ RAM 한계     | ✅ SaaS 기반 무제한  | ✅ 분산 가능      | ❌ 실험 수준    |
| **디스크 저장 (Persistence)**      | ✅ O (disk 기반)        | ❌ X (RAM 기반) | ✅ O            | ✅ O          | ❌ 비정형 JSON |
| **메타데이터 필터링**                 | ✅ 매우 강력              | ❌ 없음         | ✅ O            | ✅ O          | ❌ 없음       |
| **멀티 테넌시 (collection 분리)**    | ✅ O (collection 지원)  | ❌ 불가         | ✅ O            | ✅ O          | ✅ 제한적      |
| **LangChain 통합 난이도**          | ✅ 쉬움                 | ✅ 쉬움         | ✅ 쉬움           | ⚠ 약간 복잡      | ✅ 쉬움       |
| **운영 난이도**                    | ⚠ GCE 직접 배포 필요       | ✅ 로컬만 사용     | ✅ SaaS, 설치 불필요 | ⚠ 설정 많음      | ✅ 테스트용 적합  |
| **한국어 벡터 호환성**                | ✅ 적절 (임베딩 모델에 따라 다름) | ✅            | ✅              | ✅            | ✅          |
| **Open Source 여부**            | ✅ 완전 공개              | ✅ 완전 공개      | ❌ 유료           | ✅ 공개         | ✅ 공개       |
| **비용**                        | ✅ 무료 (자체 호스팅)        | ✅ 무료         | ❌ 유료           | ✅ 무료 (자체)    | ✅ 무료       |

- 근데 ChromaDB가 "실시간 서비스가 어렵다"는게 무슨뜻이지?
    >  Chroma는 로컬파일 기반이기 떄문에, 거비스 배포와 실시간 요청 처리를 감당하기 어려움
    1. 🔒 동시성(concurrency) 제한
	    - Chroma는 기본적으로 Python 프로세스 안에서 동작하는 in-process vector store입니다.
	    - 여러 FastAPI 요청이 동시에 chroma_db에 접근할 경우 → 파일 잠금(lock), I/O 충돌 발생 가능
	    - 특히 코랩/서버에서 동시에 add_texts() 또는 similarity_search() 호출하면 오류 발생 확률이 큼

    2. 실시간 인덱싱 반영 속도 느림
	    - add_documents()로 벡터를 넣더라도, 검색에 반영되기까지 delay가 발생하거나
	    - 자체적인 인덱스 재구성(rebuild) 로 인해 느려질 수 있음
	    - Pinecone, Qdrant처럼 삽입 즉시 검색에 반영되는 구조가 아님

    3. 독립 실행 서버(REST API 서버)로 동작하지 않음
	    - Chroma는 from_documents()로 직접 접근해야만 함
	    - localhost:6333 같은 REST API를 열 수 없음 → 서버 분리형 구조 불가
	    - 따라서 벡터 DB와 FastAPI를 따로 띄워 **분산 서비스**하기 어려움

> 비유로 이해하면 
> 1. 카페에서 손님 10명이 동시에 커피주문 하는데, 내가 혼자서 다 만들어야하는 상황
> 2. 손님이 메뉴판 수정이 필요하다 하지만 바쁜 나는 일일히 고치기에는 시간이 소요
> 3. 카페 서버를 외부에서 호출하고 싶지만 내가 전화도 안받고 묵묵응답

- 서버 분산을 왜 필요로 하는걸까?
| 목적 | 이유 / 효과 |
|------------------------|------------------------------------------------------------------------------|
|   동시 요청 처리         | 여러 요청을 나눠 처리하면 속도와 안정성 향상<br>예: FastAPI는 API 전담, Qdrant는 검색만 담당 |
|   역할 분리             | 각 서버가 명확한 책임을 가짐<br>→ 유지보수 용이, LLM만 재배포 가능                              |
|   확장성 (Scalability)  | 트래픽 증가 시 해당 서버만 수평 확장 가능<br>예: 벡터 수 증가 시 Qdrant만 고사양으로 이관         |
|   독립 배포 (Isolation) | 문제 발생 시 개별 서버만 리부팅/수정 가능<br>→ 전체 서비스 영향 최소화                        |
|   보안 정책 분리         | 서버별 IP 제한, 인증, CORS 다르게 설정 가능<br>→ 내부망 Qdrant + 외부 오픈 API 구성 가능        |
|   실시간 갱신           | 벡터 DB에 문서 추가해도 API 서버 재시작 불필요<br>→ 실시간 문서 업데이트 반영 가능               |
- 유저수에 따른 서버 과부화(트레픽증가)처리 및 보안 정책 그리고 실시간 갱신이 큰 이유 같다.
> CORS(Cross-Origin Resource Sharing) : 서로다른 출처(origin)의 자바스크립트 요청을 허용할 것인지 결정하는 보안 정책
- 포트가 다르면 왜 다른 출처일까?
    > 출처(origin)는 3가지로 구성된다:
    > 출처 = 프로토콜 + 호스트 + 포트
    | 구성 요소    | 예시                | 설명                                |
|-------------|---------------------|-------------------------------------|
| **프로토콜** | `http`, `https`     | 통신 방식 (보안 여부 포함)              |
| **호스트**   | `localhost`, `example.com` | 서버 주소 (도메인 또는 IP)              |
| **포트**     | `3000`, `8000`      | 서비스가 열려 있는 포트 번호             |







| **문제점**          | **설명**                                                                     |
| ---------------- | -------------------------------------------------------------------------- |
| **파일 기반 저장소**    | persist_directory='./chroma_db'로 저장됨. 파일에 직접 접근해야 하므로 **API 서버를 분리하기 어렵다** |
| **인덱싱 느림/없음**    | 데이터를 추가해도 내부 인덱스 자동 갱신이 느리고 불안정할 수 있음. 실시간으로 “추가→검색”이 빠르게 이어지기 힘듦          |
| **동시성 부족**       | FastAPI에서 요청이 많아지면 여러 쓰레드가 동시에 chroma_db 파일을 건드릴 수 있어 충돌 위험                |
| **재시작 시 초기화 위험** | 코랩/로컬 환경은 재시작 시 데이터 휘발 가능성 있음                                              |
| **서버 분리 불가**     | 벡터 서버를 API 서버와 **완전히 분리된 인스턴스로 띄울 수 없음**                                   |

- 구조요약
```mermaid
graph TD
  A[사용자 입력] --> B[qa_chain.run()]
  B --> C[Qdrant 벡터 검색 (context)]
  C --> D[PromptTemplate 조립]
  D --> E[Mistral 모델 호출]
  E --> F[JSON 응답 출력]
```






---
# 프로젝트 아키텍처 수정

- API URI가 대대적으로 개편됐다.
    - LLM ChatBot
        - /ai/chatbot/select-category → /ai/chatbot/recommendation/message
        - /ai/chatbot/freetext → /ai/chatbot/recommendation/form
    - LLM Censor
        - /ai/validate-challenge →  /ai/challenges/group/validation
    - LLM Verify
        - /ai/verify-image → /ai/verifications/image
        - /ai/verify-result → /callback/ai/verifications/result