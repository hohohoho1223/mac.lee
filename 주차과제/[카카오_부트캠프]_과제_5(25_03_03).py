# -*- coding: utf-8 -*-
"""[카카오 부트캠프] 과제 5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pDviM_YthlP2SxndDy_2tS27GkxymLsJ

1. 비선형 활성화 함수 3가지(시그모이드, 하이퍼볼릭 탄젠트, 렐루)를 정의하고 -10부터 10까지의 값 100개를 생성한 뒤 입력값으로 사용하여 비선형 활성화 함수를 적용한 값 출력해보세요.
"""

#1
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return (np.exp(x*2) - 1) / (np.exp(x*2) + 1)

def relu(x):
    return np.maximum(0, x)

x_value = np.random.rand(100) # rand로 구현
x = x_value * 20 -10
# x_value = np.linspace(-10, 10, 100)
#  > numpy.linspace는 NumPy 라이브러리에서 제공하는 함수로, 주어진 구간 내에서 균일한 간격으로 값을 생성하는 데 사용됨

y_sigmoid = sigmoid(x)
y_tanh = tanh(x)
y_relu = relu(x)

plt.figure(figsize=(12, 8))

plt.subplot(3, 1, 1)
plt.plot(x, y_sigmoid, label='Sigmoid', color='blue')
plt.title('Sigmoid Function')
plt.xlabel('Input Value')
plt.ylabel('Output Value')
plt.grid()
plt.legend()
print(y_sigmoid)

plt.subplot(3, 1, 2)
plt.plot(x, y_tanh, label='Hyperbolic Tangent', color='green')
plt.title('Hyperbolic Tangent Function')
plt.xlabel('Input Value')
plt.ylabel('Output Value')
plt.grid()
plt.legend()
print(y_tanh)

plt.subplot(3, 1, 3)
plt.plot(x, y_relu, label='ReLU', color='red')
plt.title('ReLU Function')
plt.xlabel('Input Value')
plt.ylabel('Output Value')
plt.grid()
plt.legend()
print

plt.tight_layout() #여러 개의 서브플롯을 포함하는 그림을 그릴 때, 서브플롯 간의 간격을 자동으로 조정하여 레이아웃을 최적화
plt.show()

"""2. 다층 퍼셉트론을 활용하여 선형적으로 분리되지 않는 데이터를 분류하는 모델을 구현해보세요.
    - XOR 문제와는 다른 비선형 데이터셋(예: make_moons 또는 make_circles 활용)을 생성하여 분류
    -  입력층, 은닉층(활성화 함수 포함), 출력층을 구성하여 MLP 설계
"""

#2
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import make_moons
from sklearn.datasets import make_circles
import matplotlib.pyplot as plt


X, y = make_moons(n_samples=100, shuffle=True, noise=0.1, random_state=42) # 일부러 '가우시안 노이즈'를 추가(더 현실적)
# X , y = make_circles(n_samples=100, shuffle=True, noise=None, random_state=None)

# X는 생성된 데이터 포인트들의 특성(2D 좌표)으로 구성된 배열이고,
# y는 각 데이터 포인트에 대한 클래스 레이블(0 또는 1)로 구성된 배열임.

X_tensor = torch.tensor(X, dtype=torch.float32) #왜 입력값은 float32 인가? -> 부동소수점 연산처리로 수치연산에 적합하며, 활성화 함수는 입력값에 대한 연속적인 변환은 수행하므로 부동소수점 연산을 기반으로 작동하기 때문에.
# 어떻게 이진분류로 판단? -> moons와 circles에서 생성된 레이블(y)은 일반적으로 0과 1로[0,1]구성되어있어 2개의 클래스이다.
y_tensor = torch.tensor(y, dtype=torch.long)  # 레이블을 Long 타입으로 설정

dataset = TensorDataset(X_tensor, y_tensor) # 입력데이터(X_tensor)와 레이블(y_tensor)을 하나의 데이터 셋으로 묶어주는 역할
train_loader = DataLoader(dataset, batch_size=32, shuffle=True)

class FCNN(nn.Module):
    def __init__(self, input_dim):
        super(FCNN , self).__init__() #nn.module의 기능을 사용하기 위해 호출
        self.fc1 = nn.Linear(input_dim,64)
        self.fc2 = nn.Linear(64,32)
        self.fc3 = nn.Linear(32,2) # CrossEntropyLoss를 사용하므로 출력 노드 수를 2로 설정

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x) #로짓 출력 : 로짓은 확률로 변환되기 전의 원시 점수(raw score)를 나타냄.
        return x

model = FCNN(2) #입력 차원은 2
criterion = nn.CrossEntropyLoss()
# criterion = nn.CrossEntropyLoss() : 클래스 레이블이 정수형(Long)이어야 하며 이진 문제에서 로짓(모델의 출력)이 1개일때

optimizer = optim.Adam(model.parameters(), lr=0.01)

num_epochs = 1000
for epoch in range(num_epochs):
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    if (epoch + 1) % 100 == 0:
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

"""3. 간단한 CNN(Convolutional Neural Network)을 구성하여 가상 데이터(또는 MNIST 등 간단한 데이터셋)를 분류해보세요."""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

# 하이퍼파라미터 설정
batch_size = 64  # 배치 크기
learning_rate = 0.001  # 학습률
num_epochs = 5  # 에폭 수 (전체 데이터셋을 몇 번 반복할 것인지)

# 데이터 전처리
transform = transforms.Compose([
    transforms.ToTensor(),  # 이미지를 텐서로 변환
    transforms.Normalize((0.5,), (0.5,))  # 평균 0.5, 표준편차 0.5로 정규화
])

# MNIST 데이터셋 다운로드 및 로드
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)  # 학습 데이터셋
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)  # 테스트 데이터셋

# 데이터로더 생성
train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)  # 학습 데이터로더
test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)  # 테스트 데이터로더

# CNN 모델 정의
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()  # 부모 클래스 초기화
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)  # 첫 번째 합성곱 레이어
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)  # 두 번째 합성곱 레이어
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # 최대 풀링 레이어
        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # 첫 번째 완전 연결 레이어
        self.fc2 = nn.Linear(128, 10)  # 두 번째 완전 연결 레이어 (10개의 클래스)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))  # 첫 번째 합성곱 후 ReLU 활성화 및 풀링
        x = self.pool(torch.relu(self.conv2(x)))  # 두 번째 합성곱 후 ReLU 활성화 및 풀링
        x = x.view(-1, 64 * 7 * 7)  # 텐서를 1차원으로 변환 (플래튼)
        x = torch.relu(self.fc1(x))  # 첫 번째 완전 연결 레이어 후 ReLU 활성화
        x = self.fc2(x)  # 두 번째 완전 연결 레이어 (출력)
        return x  # 출력 반환

# 모델, 손실 함수, 옵티마이저 정의
model = SimpleCNN()  # CNN 모델 인스턴스 생성
criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류용 손실 함수
optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam 옵티마이저

# 학습 루프
for epoch in range(num_epochs):  # 각 에폭에 대해
    model.train()  # 모델을 학습 모드로 설정
    for images, labels in train_loader:  # 배치 단위로 학습 데이터 로드
        optimizer.zero_grad()  # 기울기 초기화
        outputs = model(images)  # 모델을 통해 예측값 생성
        loss = criterion(outputs, labels)  # 손실 계산
        loss.backward()  # 역전파
        optimizer.step()  # 파라미터 업데이트

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')  # 에폭별 손실 출력

# 모델 평가
model.eval()  # 모델을 평가 모드로 설정
correct = 0  # 정확한 예측 수 초기화
total = 0  # 총 예측 수 초기화

with torch.no_grad():  # 기울기 계산 없이 평가
    for images, labels in test_loader:  # 테스트 데이터 로드
        outputs = model(images)  # 모델을 통해 예측값 생성
        _, predicted = torch.max(outputs.data, 1)  # 가장 높은 확률을 가진 클래스 예측
        total += labels.size(0)  # 총 예측 수 증가
        correct += (predicted == labels).sum().item()  # 정확한 예측 수 증가

# 정확도 출력
print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')  # 테스트 정확도 출력

#실험
import torch
from PIL import Image
import torchvision.transforms as transforms

# 모델을 평가 모드로 설정
model.eval()

# 손글씨 이미지 로드 및 전처리
image_path = '/content/손글씨체 다운.jpeg'  # 손글씨 이미지 경로
image = Image.open(image_path).convert('L')  # 이미지를 그레이스케일로 변환
transform = transforms.Compose([
    transforms.Resize((28, 28)),  # 이미지 크기를 28x28로 조정
    transforms.ToTensor(),  # 텐서로 변환
    transforms.Normalize((0.5,), (0.5,))  # 정규화
])
image = transform(image).unsqueeze(0)  # 배치 차원 추가

# 예측 수행
with torch.no_grad():
    output = model(image)  # 모델에 입력
    _, predicted = torch.max(output.data, 1)  # 예측 클래스
    print(f'Predicted Digit: {predicted.item()}')  # 예측 결과 출력