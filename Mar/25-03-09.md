# 오늘 내가 배운 것들(Today I Learned)

- 미니퀘스트 & 6주차 과제 진행

---

- 경사하강법 미니퀘스트 중

    - 2개의 파라미터(절편과 기울기)를 무작위로 초기화합니다.

    ```python
        theta = np.random.randn(2, 1)  # 예: [[b], [w]]
    ```

	- Q. 왜 파라미터가 2개인지 의문이 생겼다

    > 나는 보통 튜풀형태로(행,열) 나타낼때, 열 부분을 `column` 이라 생각하고 각 특성을 나타내며 행 부분은 해당 특성의 데이터 갯수라고 생각했다.


	- 선형회귀를 할때는 보통 하나의 열을 더 추가한다

    - 첫번째 열은 절편을 위한1, 두번쨰 열은 실제 특성의 값

    > 나는 첫번째 열이 인덱스를 나타내는 역할인줄 알았다.

    > 하지만 ‘첫번째 열은 절편을 학습 하기위한 가상의 특성’이다

    > 여기서 1로 채워진 열은 절편b와 곱해져서 행렬연산을 가능하게 만드는 역할을 한다.

	- Q. 그럼 왜 절편 b도 학습을 해야하는걸까?

        - w : 가중치(기울기) → 데이터의 변화를 얼마나 반영하는지 결정

        - b : 절편(intercept) → **모델이 예측하는 값의 기준점을 이동시키는 역할**
        
    > 만약 여기서 절편을 무시해버려 0으로 만든다면, 해당 그래프는 반드시 원점을 지나게 되는데, '이는 큰 오류이다'

    > - 키가 0이면 몸무게도 0kg인가? 아니다.

    > 현실적인 모델이 되려면 최소한 절편이 필요한 것이다.


	- b (절편)을 학습하는 값으로 두면, 모델이 데이터에 맞춰서 자동으로 조정 할 수 있다. 그래서 첫번째 열을 1로 채워서 행렬연산에 표시를 한것이다. 

    - Q. 그럼 학습을 시킨다고 하는데 왜 행렬연산에서 1과 곱하는지? 그럼 값이 안변하는것 같은데

		- 맞는말임. 첫번째 열이 항상 1로 고정되어 있기에 `행렬연산` 을 할때는 해당 열의 값이 변하지않음

		- 하지만 **b자체값 은 변할 수 있음**

		- 그것이 바로 경사하강법으로 인한 b값 업데이트이다

		- 아래는  절편의 학습을 확인한는 코드임

    ```python
    import numpy as np
    
    # 샘플 데이터 (x: 입력, y: 실제값)
    X = np.array([1, 2, 3, 4, 5])
    y = np.array([2, 4, 6, 8, 10])  # y = 2x
    
    # 절편을 위해 첫 번째 열을 1로 추가
    X_b = np.c_[np.ones((X.shape[0], 1)), X]  # (5, 2) 행렬
    
    # 초기 가중치 (b와 w)
    theta = np.random.randn(2, 1)  # [[b], [w]]
    
    # 학습률과 반복 횟수
    learning_rate = 0.01
    n_iterations = 1000
    m = len(X)
    
    # 경사 하강법
    for iteration in range(n_iterations):
        predictions = X_b.dot(theta)  # 예측값
        error = predictions - y.reshape(-1, 1)  # 오차 계산
        gradients = (2/m) * X_b.T.dot(error)  # 손실 함수의 기울기
        theta -= learning_rate * gradients  # 업데이트
    
    # 최적의 b와 w 출력
    print(f"절편 b: {theta[0, 0]}")
    print(f"가중치 w: {theta[1, 0]}")
    ```

	- 정리하자면, 절편 b의 값을 업데이트 하기위해 X행렬 첫번째 열에 1로 구성된 열을 추가하고, 결과값을 바탕으로 경사하강법을 실시하며 최적의 값을 도출한다!
	- MNIST 데이터셋을 이용한 신경망 모델 비교중

    ```py
    input_size = 784 
    hidden_size = 128 
    output_size = 10 
    W1 = np.random.randn(input_size, hidden_size) * 0.01 
    b1 = np.zeros((1, hidden_size)) 
    W2 = np.random.randn(hidden_size, output_size) * 0.01 
    b2 = np.zeros((1, output_size))
    ```

	- 여기서 왜 은닉층을 128개로 하지? 

	- 사용자가 설정하는 ‘하이퍼파라미터' 이기에, 보통 GPU연산 최적화의 맞춰 128,256,512 등 2의 배수로 지정함

---

    ```py
    x_train, x_test = x_train / 255.0, x_test / 255.0
    ```

- 왜 255로 나누는 이유?

    - 픽셀 값을 0~1로 정규화(normalization) 하기 위해서
    - 각 픽셀 값은 0~255 사이의 정수임.

    > 0 : 검은색(완전한 배경)

    > 255 : 흰색(완전한 전경)

    > 그 사이값들 : 회색(픽셀 밝기 정도)

    -  또한 기울기 소실 문제 해결등 학습 속도 증가

    ```py
    _train_onehot = to_categorical(y_train, 10)
    y_test_onehot  = to_categorical(y_test, 10)
    ```

- 원-핫 인코딩을 하는 이유?


    - 다중 클래스 분류에 `softmax` 함수 사용으로, 각 클래스의 확률분포를 계산하기 때문에 정수 레이블이 아니라 확률 분포(원-핫 벡터) 형태여야 함
    - **`Categorical Crossentropy`  손실함수 사용**
    > 다중 클래스 분류에서는 해당 손실함수를 쓰므로 이 함수 또한 원-핫 벡터를 기대함.

---

- 6주차 과제 도중

    - ResNet

    ```python
    ase_model = models.resnet50(weights=None)
    base_model = nn.Sequential(*list(base_model.children())[:-2])
    ```

		- 제일 끝 두 레이어를 제거한다

			- Global Average Pooling(GAP) Layer

    ```python
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    ```


				- AdaptiveAvgPool2d((1,1)) → 7x7 크기의 Feature Map을 (1,1)로 축소.

- 이 과정에서 **공간적 정보(위치 정보)를 잃어버리고, 평균값만 남김**.
- 이후 Flatten()을 거쳐 1D 벡터로 변환됨.

    - 공간적 정보를 일어버린다?

    > `AdaptiveAvgPool2d((1,1))` 을 적용하면 Feature Map의 크기가 7*7 *→ 1 *1* 로 줄어든다

    > 그니까 7*7에 있던 대상의 특징이 풀링으로 인해 평균값만 빼와서 1*1로 넣어버리니 대상 특징이 뭉개지고 존재 여부만 알수 있게됨

- Fully Connected (FC) Layer(출력층)

    ```python
    (fc): Linear(in_features=2048, out_features=1000, bias=True)
    ```


	- Linear(2048, 1000) → ImageNet 학습을 위해 2048차원 벡터를 1000개의 클래스로 변환.

	- 우리가 원하는 num_classes=10과 다르므로 **새로운 출력층을 추가해야 함**.


    - **Softmax(dim=1)에서 dim=1의 의미**

	    - 소프트 맥스를 적용할 차원을 1차원으로 지정

		- 예를 들어, batch_size=32,num_classes=10이라면:

    ```python
    (32, 10)
    ```


	- 여기서 각 샘플마다 10개의 클래스에대란 점수(로짓값)가 있음

	- dim =1 의 의미

		- `softmax(dim=1)` 을 적용하면 각 행(배치 내 개별 샘플)에 대해 소프트맥스를 적용

		- 즉, 각 샘플(각 행)의 클래스 확률 총합이 1이 되도록 변환

    ```py
    probabilities = nn.Softmax(dim=1)(logits)
    ```


	- logits의 크기가 (32,10)이면:

	    - `Softmax(dim=1)` 을 적용하면 각 행 (각 샘플)에 대해 Softmax가 적용됨

	- logits가 뭔가?

    > `Softmax()` 함수가 적용되기전 전 출력층의 출력값을 의미함. 

    > 따라서 이 값들은 '원래의 점수(숫자)’ 임 아직 확률값이 아님

    > `Softmax()` 에 들어가야 확률값으로 반환되는것임

    > softmax 그래프 예시

![image](https://res.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/3696A8AD-80F2-470D-8C6E-0A88062928D7_2/y4eykJogDduWh4pibLGdKxbauCjmpi3NTUuRUFKbVPEz/Softmax%20Function%20Converting%20Logits%20to%20Probabilities.png)

- 가로가 Logits(실제 점수 값) 세로가 확률(0,1)

- 각 샘플이란 뭘 의미할까?

    -  ‘각 batch_size안에 포함된 개별 데이터’를 지칭

- **왜 손실 함수로 CrossEntropyLoss를 사용하는가?**

    - 손실함수는 경사하강법이 최소화하려는 대상임

    - 다중 클래스 분류에서   `CrossEntropyLoss` 를 사용하는 이유는 ‘예측 확률 분포(Softmax 결과)와 실제 정답 준초 사이의 차이를 최소화' 하는 역할을 하기 위함임


- `x = torch.flatten(x, 1)` 을 이옹해 1차원 배열로 변환하는 이유?

	- `Fully Connected(FC) Layer` 입력 형식에 맞추기 위함

	- 왜냐면 FC 층은 선형 계층이므로, 1차원 벡터 형태로만 입력이 가능하다

| **Layer Type**                        | **입력 데이터 형태**                         |
| ------------------------------------- | ------------------------------------- |
| **CNN (Conv2D, Pooling)**             | (batch_size, channels, height, width) |
| **Fully Connected Layer (nn.Linear)** | (batch_size, features)                |

- 즉, CNN의 출력인 2D Feature Map을 FC Layer에 입력하려면 반드시 1D 벡터로 변환

    > 근데 1차원 으로 줄였다고 해서 연산량이 감소할 것이라고 오해할수있다(몰론 내가 그랬음)

    > 하지만 `.flatten()` 자체는 단순한 `reshape` 연산이므로 **계산량이 변하지 않음**!!