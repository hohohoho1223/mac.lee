# 오늘 내가 배운 것들(Today I Learned)

- 오늘은 사전훈련 및 전이학습에 대해 배웠다.

---

- `include_top = False` 
    - input - (low) hidden …. (high → top) - output
    - 그럼 탑을 왜 뺌? 뒷부분만 내가 커스텀 한다는 뜻?

    ```python
    from tensorflow.keras.applications import VGG16

    # include_top=False -> Fully Connected Layer 제외 (Feature Extractor만 사용)
    base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

    # 모델 구조 확인
    base_model.summary()
    ```

	- `model.compile` 컴파일로 하나로 다 취합한 표현(keras 모델)
		- Keras의 model.compile()은 **손실 함수(loss function), 옵티마이저(optimizer), 그리고 평가 지표(metrics)를 한 번에 설정**하는 함수
- ResNet
    - 예측값과 실측값의 차이로 생기는 '부분’
    - 잔차(residual)은 모델의 예측 오차를 나타내는 지표이다.
    - 근데 모델 층이 너무 깊으면 이전 층에 대한 정보가 소실될수 있음 → 잔차 연결을 통해 이전 층의 정보를 연경함.
- VGG16(Visual Geometry Group)
- 전이학습(Transfer Learning)
	- '사전 훈련된 모델’을 가져다가 새로 튜닝을 해서 새로운 문제에 적용 시키는 것

	    1. Fine-Tunning(미세 조정)

			- **PTM (Pretrained Model) +FT (Fine-Tuning)** 조합이 이상적임

			- 새로운 데이터셋에 맞춰 '미세 조정'을 하여 성능을 최적화한다

			- 하위 레이어는 동경(기본적인 추출 능력을 잘 학습)

			- 상위 레이어를 재학습한다.

    ```python
    # 하위 레이어를 동결하고, 상위 레이어만 동결 해제
    for layer in base_model.layers:
        layer.trainable = False
    for layer in model.layers[-4:]: # 이 리스트의 마지막 네 개의 요소를 선택
        layer.trainable = True
    ```


	- 뿐만 아니라 하이브리드 방법또한 존재함.(상위 하위 각각 모델 튜닝)

    ```python
    import numpy as np
    from tensorflow.keras.utils import to_categorical
    from tensorflow.keras.models import Model
    from tensorflow.keras.layers import Flatten, Dense, Input
    from tensorflow.keras.applications import VGG16
    from tensorflow.keras.optimizers import Adam
    
    # 가상 데이터셋 생성
    num_classes = 10
    input_shape = (224, 224, 3)
    
    x_train = np.random.random((1000, 224, 224, 3))
    y_train = np.random.randint(num_classes, size=(1000,))
    x_test = np.random.random((200, 224, 224, 3))
    y_test = np.random.randint(num_classes, size=(200,))
    
    # 원-핫 인코딩
    y_train = to_categorical(y_train, num_classes)
    y_test = to_categorical(y_test, num_classes)
    
    # 사전 학습된 모델 불러오기
    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)
    
    # 모델의 구조 변경 (새로운 분류 레이어 추가)
    x = Flatten()(base_model.output)
    x = Dense(256, activation='relu')(x)
    x = Dense(num_classes, activation='softmax')(x)
    
    model = Model(inputs=base_model.input, outputs=x)
    
    # 하위 레이어를 동결하고, 상위 레이어만 동결 해제
    for layer in base_model.layers:
        layer.trainable = False
    for layer in model.layers[-4:]: # 모델의 레이어 중 마지막 네 개의 레이어를 선택
        layer.trainable = True
    
    # 모델 컴파일
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    
    # 상위 레이어 학습
    model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
    
    # 일부 하위 레이어 동결 해제
    for layer in model.layers[-15:]: # 모델의 레이어 중 첫 번째 15개의 레이어를 선택
        layer.trainable = True
    
    # 모델 재컴파일 및 전체 모델 학습
    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])
    model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)
    
    # 모델 평가
    loss, accuracy = model.evaluate(x_test, y_test)
    print(f'Test Loss: {loss}')
    print(f'Test Accuracy: {accuracy}')
    ```

	2. Feature-Extraction(특징 추출)

		- 중간 계층에서 대표적인 패턴이나 형태를 효과적으로 포착 할수 있는 구간 → 여길 freeze 시켜서 역전파 발생하지 않게 해서 가중치를 보존한다.

		- 출력층 부분을 classification한다.

    ```python
    import torch
    import torchvision.models as models
    import torch.nn as nn
    
    # 1️⃣ 사전 훈련된 ResNet 불러오기 (Pretrained=True)
    model = models.resnet50(pretrained=True)
    
    # 2️⃣ 중간 계층까지 Freeze (고정)
    for param in model.parameters():
        param.requires_grad = False  # 가중치 업데이트 X
    
    # 3️⃣ 최종 계층(Classifier) 변경 (새로운 데이터셋에 맞게 수정)
    num_features = model.fc.in_features  # 기존 FC 입력 차원 가져오기
    model.fc = nn.Linear(num_features, 10)  # 예: 10개의 클래스를 분류하는 경우
    
    # 4️⃣ 새로운 데이터셋으로 학습
    optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)  # 최종 계층만 학습
    ```

	3. Zero-Shot Learning(제로샷 학습)

		- 훈련 데이터에 없는 새로운 클래스나 작업을 사전 지식이나 관련정보 없이도 수행하는 기법ㄷㄷ(줄 +말 = 얼룩말)


- validation이 왜 떨어지는가?
    - 모델이 일반화를 하지못해서이다.
        1. 과적합
        2. 과소적합
        3. 데이터 분포차이

            - 학습 데이터와 검증 데이터의 분포가 다를경우


        1. 하이퍼 파라미터 세팅 문제

            - 학습률, 배치크기, 정칙화(regularization) 설정
