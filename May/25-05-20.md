# 오늘 내가 배운 것들(Today I Learned)

- 챗봇 고도화 작업을 하였다.
    - 사용자 별로 sessionId를 발급해 챗봇에서의 대화를 저장하여 해당 대화 기반으로 LLM답변을 만드는 기능이다.
- 모믜면접(운영체제) 공부를 하였다.

---

+ LLM_chatbot_free_text_model.py

```python
# LLM_chatbot_free_text_model.py
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_qdrant import Qdrant
from langchain_community.embeddings import SentenceTransformerEmbeddings
from qdrant_client import QdrantClient
from langchain_google_vertexai import VertexAI
from dotenv import load_dotenv
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated, Sequence, Optional, Dict, List
import os
import json

load_dotenv()

QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME")

qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
embedding_model = SentenceTransformerEmbeddings(model_name="BAAI/bge-small-en-v1.5")

vectorstore = Qdrant(
    client=qdrant_client,
    collection_name=COLLECTION_NAME,
    embeddings=embedding_model
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 3}) # 사용자 질문으로 부터 가장 유사한 3개 문서 검색

# RAG 방식 챌린지 추천을 위한 Output Parser 정의
rag_response_schemas = [
    ResponseSchema(name="recommend", description="추천 텍스트를 한 문장으로 출력해줘."),
    ResponseSchema(name="challenges", description="추천 챌린지 리스트, 각 항목은 title, description 포함, description은 한 문장으로 요약해주세요.")
]

# LangChain의 StructuredOutputParser를 사용하여 JSON 포맷을 정의
rag_parser = StructuredOutputParser.from_response_schemas(rag_response_schemas)

# JSON 포맷을 이스케이프 처리
escaped_format = rag_parser.get_format_instructions().replace("{", "{{").replace("}", "}}")

# RAG 방식 챌린지 추천을 위한 PromptTemplate 정의
custom_prompt = PromptTemplate(
    input_variables=["context", "query", "messages"],
    template=f"""
다음 문서와 이전 대화 기록을 참고하여 사용자에게 적절한 친환경 챌린지를 3개 추천해주세요.
반드시 문서에서 제공된 정보를 기반으로 답변해주세요.

이전 대화 기록:
{{messages}}

문서:
{{context}}

현재 요청:
{{query}}

응답은 반드시 다음 JSON 형식을 따라주세요:
{escaped_format}
"""
)

# LLM 초기화 (VertexAI)
llm = VertexAI(model_name="gemini-2.0-flash", temperature=0.3)

# LLMChain 체인 생성 (retriever는 app_router에서 별도 사용)
qa_chain = LLMChain(
    llm=llm,
    prompt=custom_prompt
)

# 대화 상태를 관리하기 위한 타입 정의
class ChatState(TypedDict):
    messages: Annotated[Sequence[str], "대화 기록"]
    current_query: str
    context: str
    response: str
    should_continue: bool  # 대화 계속 여부
    error: Optional[str]   # 오류 메시지
    docs: Optional[list]   # 검색된 문서
    sessionId: str   # 세션 ID

# 대화 그래프 노드 정의
def validate_query(state: ChatState) -> ChatState:
    """사용자 질문 유효성 검사"""
    if len(state["current_query"].strip()) < 5:
        state["error"] = "질문은 최소 5자 이상이어야 합니다."
        state["should_continue"] = False
    else:
        state["should_continue"] = True
    return state

def retrieve_context(state: ChatState) -> ChatState:
    """관련 컨텍스트 검색"""
    if not state["should_continue"]:
        return state
    try:
        # RAG 검색 수행
        docs = retriever.get_relevant_documents(state["current_query"])
        state["docs"] = docs
        state["context"] = "\n".join([doc.page_content for doc in docs])
        
        # 검색된 문서가 없는 경우
        if not docs:
            state["error"] = "관련된 챌린지 정보를 찾을 수 없습니다."
            state["should_continue"] = False
    except Exception as e:
        state["error"] = f"컨텍스트 검색 중 오류 발생: {str(e)}"
        state["should_continue"] = False
    return state

def generate_response(state: ChatState) -> ChatState:
    """응답 생성"""
    if not state["should_continue"]:
        return state
    try:
        messages = "\n".join(state["messages"])
        print(f"Generating response for query: {state['current_query']}")  # 디버깅용 로그
        
        response = qa_chain.invoke({
            "context": state["context"],
            "query": state["current_query"],
            "messages": messages
        })
        
        print(f"Raw LLM response: {response['text']}")  # 디버깅용 로그
        
        # JSON 파싱 시도
        try:
            response_text = response["text"]
            if "```json" in response_text:
                response_text = response_text.split("```json")[1]
            if "```" in response_text:
                response_text = response_text.split("```")[0]
            response_text = response_text.strip()
            
            parsed_response = json.loads(response_text)
            # 필수 필드 검증
            if "recommend" not in parsed_response or "challenges" not in parsed_response:
                raise ValueError("응답에 필수 필드가 없습니다.")
            if not isinstance(parsed_response["challenges"], list):
                raise ValueError("challenges는 리스트 형태여야 합니다.")
            
            state["response"] = json.dumps(parsed_response, ensure_ascii=False)
            print(f"Parsed response: {state['response']}")  # 디버깅용 로그
            
        except json.JSONDecodeError as e:
            print(f"JSON 파싱 오류: {str(e)}")  # 디버깅용 로그
            state["error"] = "응답 형식이 올바르지 않습니다."
            state["should_continue"] = False
            return state
        except ValueError as e:
            print(f"응답 검증 오류: {str(e)}")  # 디버깅용 로그
            state["error"] = str(e)
            state["should_continue"] = False
            return state
        
        # 대화 기록 업데이트
        state["messages"] = list(state["messages"]) + [
            f"User: {state['current_query']}",
            f"Assistant: {state['response']}"
        ]
    except Exception as e:
        print(f"응답 생성 중 예외 발생: {str(e)}")  # 디버깅용 로그
        state["error"] = f"응답 생성 중 오류 발생: {str(e)}"
        state["should_continue"] = False
    return state

def handle_error(state: ChatState) -> ChatState:
    """오류 처리"""
    if state["error"]:
        state["response"] = state["error"]
        # 오류 메시지도 대화 기록에 추가
        state["messages"] = list(state["messages"]) + [
            f"User: {state['current_query']}",
            f"Assistant: {state['error']}"
        ]
    return state

# 대화 그래프 구성
def create_chat_graph():
    workflow = StateGraph(ChatState)
    
    # 노드 추가
    workflow.add_node("validate", validate_query)
    workflow.add_node("retrieve", retrieve_context)
    workflow.add_node("generate", generate_response)
    workflow.add_node("handle_error", handle_error)
    
    # 엣지 연결
    workflow.add_edge("validate", "retrieve")
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", "handle_error")
    workflow.add_edge("handle_error", END)
    
    # 조건부 라우팅
    workflow.add_conditional_edges(
        "validate",
        lambda x: "retrieve" if x["should_continue"] else "handle_error"
    )
    
    # 시작 노드 설정
    workflow.set_entry_point("validate")
    
    return workflow.compile()

# 대화 그래프 인스턴스 생성
chat_graph = create_chat_graph()

# 대화 상태 저장소
conversation_states: Dict[str, ChatState] = {}

def process_chat(sessionId: str, query: str) -> str:
    """대화 처리 함수"""
    # 이전 대화 상태 가져오기 또는 새로 생성
    if sessionId not in conversation_states:
        conversation_states[sessionId] = {
            "messages": [],
            "current_query": "",
            "context": "",
            "response": "",
            "should_continue": True,
            "error": None,
            "docs": None,
            "sessionId": sessionId
        }
    
    # 현재 상태 업데이트
    state = conversation_states[sessionId]
    state["current_query"] = query
    
    # 대화 그래프 실행
    result = chat_graph.invoke(state)
    
    # 상태 저장
    conversation_states[sessionId] = result
    
    return result["response"]

def clear_conversation(sessionId: str):
    """대화 기록 삭제"""
    if sessionId in conversation_states:
        del conversation_states[sessionId]

def get_conversation_history(sessionId: str) -> List[str]:
    """대화 기록 조회
    
    Args:
        sessionId: 사용자 세션 ID
    
    Returns:
        List[str]: 대화 기록 리스트
    """
    if sessionId in conversation_states:
        return conversation_states[sessionId]["messages"]
    return []
````

    + LLM_chatbot_base_info_model.py

```python
# LLM_chatbot_base_info_model.py
from google.cloud import aiplatform
from vertexai.preview.generative_models import GenerativeModel
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.prompts import PromptTemplate
from google.oauth2 import service_account
from fastapi import HTTPException
from fastapi.responses import JSONResponse
from dotenv import load_dotenv
import os
import json

load_dotenv()

#  환경 변수에서 값 가져오기
SERVICE_ACCOUNT_FILE = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT")
LOCATION = os.getenv("VERTEX_AI_LOCATION")
MODEL_NAME = os.getenv("VERTEX_MODEL_NAME")

# Vertex AI 초기화
credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)
aiplatform.init(project=PROJECT_ID, location=LOCATION, credentials=credentials)

# GenerativeModel 초기화(SDK 방식 사용)
model = GenerativeModel(model_name=MODEL_NAME)

# base-info_response_schemas 정의
base_response_schemas = [
    ResponseSchema(name="recommend", description=f"추천 텍스트를 한 문장으로 출력해줘.(예: '이런 챌린지를 추천합니다.')"),
    ResponseSchema(name="challenges", description="추천 챌린지 리스트, 각 항목은 title, description 포함, description은 한 문장으로 요약해주세요.")
                   ]

# base-info_output_parser 정의 
base_parser = StructuredOutputParser.from_response_schemas(base_response_schemas)

# base-info_prompt 정의
escaped_format = base_parser.get_format_instructions().replace("{", "{{").replace("}", "}}")
base_prompt = PromptTemplate(
    input_variables=["location", "workType", "category"],
    template=f"""
{{location}} 환경에 있는 {{workType}} 사용자가 {{category}}를 실천할 때,
절대적으로 환경에 도움이 되는 챌린지를 아래 JSON 형식으로 3가지 추천해주세요.

JSON 포맷:
{escaped_format}

응답은 반드시 위 JSON 형식 그대로 출력하세요.

"""
)

# base-info_Output Parser 정의
def get_llm_response(prompt):
    try:
        model = GenerativeModel(model_name=MODEL_NAME)
        response = model.generate_content(prompt)

        raw_text = response.text if hasattr(response, 'text') else response
    
        if isinstance(raw_text, dict): # dict이면 그대로 사용
            parsed = raw_text
        else:
            text = raw_text.strip()
            parsed = base_parser.parse(text)
            if isinstance(parsed.get("challenges"), str):
                parsed["challenges"] = json.loads(parsed["challenges"])

        return JSONResponse(
            status_code=200,
            content={
                "status": 200,
                "message": "성공!",
                "data": parsed
            }
        )

    except HTTPException as http_err:
        raise http_err
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"챌린지 추천 중 내부 오류 발생: {str(e)}")

```

    + chatbot_router.py

```python
# chatbot_router.py
from model.chatbot.LLM_chatbot_base_info_model import base_prompt, get_llm_response
from model.chatbot.LLM_chatbot_free_text_model import qa_chain, retriever, process_chat, clear_conversation
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from fastapi import HTTPException
from pydantic import BaseModel
from typing import Optional
import json
import re

router = APIRouter()

# 키워드 및 비속어 필터링 리스트
ENV_KEYWORDS = [
    "환경", "지구", "에코", "제로웨이스트", "탄소", "분리수거", "플라스틱", "텀블러", "기후", "친환경",
    "일회용", "미세먼지", "재활용", "자원", "대중교통", "도보", "비건", "탄소중립", "그린", "에너지",
    "쓰레기","아무","추천","챌린지","도움","도와줘","자세히","상세히"
    ]

BAD_WORDS = [
    "시발", "씨발", "fuck", "shit", "개새끼", "병신", "ㅅㅂ", "ㅄ", "ㅂㅅ","fuckyou", "asshole", "tlqkf","ㅈ"
    ]

class CategoryRequest(BaseModel):
    sessionId: Optional[str] = None
    location: Optional[str] = None
    workType: Optional[str] = None
    category: Optional[str] = None
class FreeTextRequest(BaseModel):
    sessionId: Optional[str] = None
    location: Optional[str] = None
    workType: Optional[str] = None
    message: Optional[str] = None

# 비-RAG 방식 챌린지 추천
@router.post("/ai/chatbot/recommendation/base-info")
def select_category(req: CategoryRequest):
    missing_fields = []
    # 필수 필드 검사
    if not req.location:
        missing_fields.append("location")
    if not req.workType:
        missing_fields.append("workType")
    if not req.category:
        missing_fields.append("category")
    if missing_fields:
        return JSONResponse(
            status_code=400,
            content={
                "status": 400,
                "message": f"{missing_fields}은 필수입니다.",
                "data": None
            }
        )

    # LLM 호출을 위한 prompt 구성
    prompt = base_prompt.format(
        location=req.location,
        workType=req.workType,
        category=req.category
    )

    try:
        response = get_llm_response(prompt)
        # sessionId가 있는 경우 대화 기록에 추가
        if req.sessionId:
            process_chat(req.sessionId, f"카테고리: {req.category}, 위치: {req.location}, 직업: {req.workType}")
        return response
    except HTTPException as http_err:
        raise http_err # 내부 HTTPException을 먼저 처리
    except Exception as e:
        return JSONResponse(
            status_code=502,
            content={
                "status": 502,
                "message": "AI 서버로부터 추천 결과를 받아오는 데 실패했습니다.",
                "data": None
            }
        )

# LangChain 기반 RAG 추천
@router.post("/ai/chatbot/recommendation/free-text")
def freetext_rag(req: FreeTextRequest):
    missing_fields = []
    if not req.location:
        missing_fields.append("location")
    if not req.workType:
        missing_fields.append("workType")
    if not req.message or not req.message.strip():
        missing_fields.append("message")

    if missing_fields:
        return JSONResponse(
            status_code=400,
            content={
                "status": 400,
                "message": f"{missing_fields}은 필수입니다.",
                "data": None
            }
        )
    if len(req.message.strip()) < 5:
        return JSONResponse(
            status_code=422,
            content={
                "status": 422,
                "message": "message는 문자열이어야 하며, 최소 5자 이상의 문자열이어야 합니다.",
                "data": None
            }
        )
        
    message_lower = req.message.lower()
    if not any(k in req.message for k in ENV_KEYWORDS) or any(b in message_lower for b in BAD_WORDS):
        return JSONResponse(
            status_code=200,
            content={
                "status": 200,
                "message": "사용자에게 자유 메세지를 기반으로 챌린지를 추천합니다.",
                "data": {
                    "recommend": "저는 친환경 챌린지를 추천해드리는 Leafresh 챗봇이에요! 환경 관련 질문을 해주시면 더 잘 도와드릴 수 있어요.",
                    "challenges": None
                }
            }
        )
    
    try:
        # 대화 기록을 포함한 응답 생성
        response_text = process_chat(req.sessionId, req.message)
        
        try:
            # JSON 파싱 시도
            parsed = json.loads(response_text)
            
            if isinstance(parsed.get("challenges"), str):
                parsed["challenges"] = json.loads(parsed["challenges"])
            
            return JSONResponse(
                status_code=200,
                content={
                    "status": 200,
                    "message": "사용자 자유 메시지를 기반으로 챌린지를 추천합니다.",
                    "data": parsed
                }
            )
            
        except json.JSONDecodeError:
            return JSONResponse(
                status_code=500,
                content={
                    "status": 500,
                    "message": "챌린지 추천 중 내부 오류가 발생했습니다. 잠시 후 다시 시도해주세요.",
                    "data": None
                }
            )
            
    except Exception as e:
        return JSONResponse(
            status_code=502,
            content={
                "status": 502,
                "message": "AI 서버로부터 추천 결과를 받아오는 데 실패했습니다.",
                "data": None
            }
        )
```

## 코드 설명

- `sessionId` 추가 하였고, `conversation_states` 를 전역 변수로 설정하였다.
- 전역변수가 되므로 base-info와 free-text 두 모드가 동일한 conversation_states 딕셔너리를 공유하게 된다
    - 이 말은 즉슨 같은 `sessionId` 를 사용하면 어느 API든지 대화 내용이 하나로 누적된다는 뜻이다
- 설계 방식에서 수정해야 할 부분을 느꼈다.
    - 지금 챗봇 순서가 base-info 입력 후 → 카테고리 재선택 or 자유입력(free-text)인데, 현재 코드에선 free-text의 프롬프팅은 “현재 요청:{{query}}” 로만 받고 있다.
    - API 바디 설계를 수정해야 할 것 같고(실제로 free-text 바디 안에 `location` & `workType` 이 필요한가에 대해서)

---

##  모의 면접 공부

### 1. 페이징

-  공부하다가, 세그멘테이션(segmentation) 방식에 대해 궁금해졌다.
- 또한, 외부 단편화 문제가 무엇인지에 대해 알아보았다.
    - 세그멘테이션 방식: 프로세스를 의미 있는 논리적 단위로 나누는 방식이다.
    - 연속적인 주소 공간을 사용하는 세그멘테이션 방식에서 발생할 수 있는 외부 단편화 기법을 해결 할 수있다.
        - 왜? -> 페이지가 논리적 메모리 단위로 자르는 크기(약 4KB)와 동일하게 메모리(RAM)를 프레임으로 나눠서 각 프레임에 페이지를 비연속적으로 할당 할 수 있기 때문
        - “논리 주소 공간” 이란?:

            > CPU가 인식하는 메모리 주소 공간이다. -> 프러세스가 사용하는 가상의 메모리 공간

        - 그럼 왜 논리 주소가 필요한가?:

            > 현제 운영체제는 여러 프로세스가 동시에 실행되기 때문에 모든 프로세스가 자기만의 독립적인 주소 공간을 갖도록 해야함.

    - 아 그럼 여러 프로세스가 작동하는 주소 공간은 "설계 계획”인 뿐이고, 실제는 RAM에 물리적으로 메모리를 할당 받아야 실행되는구나!!
    - 예시:   
        - 프로세스 A는 0x00000000 ~ 0x7FFFFFFF
        - 프로세스 B도 0x00000000 ~ 0x7FFFFFFF

        → 주소는 같아 보이지만 **실제로는 서로 다른 공간이다!**

### 2. CPU 스케줄링

- 프로세스란?:
    - 실행 중인 프로그램(다른 프로세스와 자원 공유 안함)

- 스레드란?:
    - 프로세스 내부에서 실제 작업을 수행하는 실행 흐름 단위
    - 하나의 프로세스에는 **여러개의 스레드를 가질 수 있다!**

- 큐란?:
    > 선입선출 방식의 자료구조 -> 먼저 온놈 처리

    ### 선점형

    1. 라운드 로빈 (Round Robin)
        - 모든 프로세스가 **하나의 준비 큐에 줄 서 있음**
        - 정해진 시간(타임퀀텀) 동안 CPU를 나눠 가짐
        - 순서대로 돌면서 처리 → 다시 맨 뒤로 감 → “회전목마” 느낌 🎠
    2. 다단계 큐
        - 우선순위 별로 큐를 여러개 준비
        - 높은 우선순위 큐가 항상 먼저 실행
    3. 다단계 피드백 큐(+에에징 기법)
        - CPU를 오래쓰면 -> 낮은 우선순위 큐로 이동
        - 그럼 짧게 쓰면? -> 높은 우선순위 큐로 이동
        - 공정성 + 응답속더 모두 고려함 굳! 

    ### 비선점형

    1. FCFS(First Come First Service) 방식 -> 먼저 온놈 서비스 처리
    2. SFJ(Shortest Job First) 방식 -> CPU 적게 쓴놈 먼저 처리

### 3. 뮤텍스(Mutex: Mutual Exclusion_상호간 제외ㅋ)란?

- 여러 개의 스레드가 공유 자원에 접근하는 것을 방지하는 방식이다. 한번에 하나의 스레드가 접근 할 수 있으며, 이때 Lock을 가지고 접근하며, 해당 스레드가 접근을 종료하면 다음 스레드가 lock의 소유를 가지며 접근하는 방식이다.  

- 왜 하나의 스레드만 접근함?

    **1.** **데이터의 일관성과 안정성 유지**
    - 여러 스레드가 동시에 같은 자원(예: 변수, 파일, 리스트 등)을 수정하면 **데이터 충돌이나 오염**이 발생할 수 있음

    **2.** **Race Condition(경쟁 조건) 방지**
    - 여러 스레드가 “누가 먼저 자원에 접근할지” 경합할 때 예측 불가능하기에 

### 4. 교착 상태란?

- 2개 이상의 프로세스 또는 쓰레드가 서로의 작업을 "무한히" 기다리는 DeadLock상태에 빠지는 것.

    - 서로의 자원을 점유하고 반납하지 않으며, 상대방의 자원을 요청함으로써 생기는 현상
    - 이러한 교착 상태가 생기려면 4가지 조건이 충족 되어야 함
        1. 상호 배제:
            - 한 번에 하나의 프로세스 만이 특정 자원을 사용할 수 있다
        1. 점유 대기:
            - 프로세스가 할당된 자원을 점유한 상태에서 다른 자원을 요청 but 얻지 못하는 상황
        1. 비선점:
            - 프로세스가 어떤 자원을 끝낼 때 까지 그 자원을 쓰지 못하는 상황
        1. 순환 대기:
            - 각 프로세스는 순환적으로 다음 프로세스가 필요로 하는 자원을 요청하는 상황

5. 프로세스 와 스레드 차이
- 프로세스는 독립적으로 실행 되므로 메모리 영역 또한 공유 하지 않는다. 반면에 스레드는 한 프로세스 내에서 ID, 레지스터 값, 스택 같은 정보들을 가지며 자원을 공유한다. 그래서 서로 다른 코드를 동시에 실행시킬 수 있다. 
- 예시:

```python
# 하나의 파이썬 프로그램(= 프로세스)
def download_image():
# 이미지 다운로드 코드
...

def resize_image():
# 리사이징 코드
...

def save_image():
# 저장 코드
...
# 이 세 가지 작업을 스레드로 실행할 수 있다
```