# ì˜¤ëŠ˜ ë‚´ê°€ ë°°ìš´ ê²ƒë“¤(Today I Learned)

- ì±—ë´‡ ê³ ë„í™” ì‘ì—…ì„ í•˜ì˜€ë‹¤.
    - ì‚¬ìš©ì ë³„ë¡œ sessionIdë¥¼ ë°œê¸‰í•´ ì±—ë´‡ì—ì„œì˜ ëŒ€í™”ë¥¼ ì €ì¥í•˜ì—¬ í•´ë‹¹ ëŒ€í™” ê¸°ë°˜ìœ¼ë¡œ LLMë‹µë³€ì„ ë§Œë“œëŠ” ê¸°ëŠ¥ì´ë‹¤.
- ëª¨ë¯œë©´ì ‘(ìš´ì˜ì²´ì œ) ê³µë¶€ë¥¼ í•˜ì˜€ë‹¤.

---

+ LLM_chatbot_free_text_model.py

```python
# LLM_chatbot_free_text_model.py
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_qdrant import Qdrant
from langchain_community.embeddings import SentenceTransformerEmbeddings
from qdrant_client import QdrantClient
from langchain_google_vertexai import VertexAI
from dotenv import load_dotenv
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated, Sequence, Optional, Dict, List
import os
import json

load_dotenv()

QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME")

qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
embedding_model = SentenceTransformerEmbeddings(model_name="BAAI/bge-small-en-v1.5")

vectorstore = Qdrant(
    client=qdrant_client,
    collection_name=COLLECTION_NAME,
    embeddings=embedding_model
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 3}) # ì‚¬ìš©ì ì§ˆë¬¸ìœ¼ë¡œ ë¶€í„° ê°€ì¥ ìœ ì‚¬í•œ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰

# RAG ë°©ì‹ ì±Œë¦°ì§€ ì¶”ì²œì„ ìœ„í•œ Output Parser ì •ì˜
rag_response_schemas = [
    ResponseSchema(name="recommend", description="ì¶”ì²œ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜."),
    ResponseSchema(name="challenges", description="ì¶”ì²œ ì±Œë¦°ì§€ ë¦¬ìŠ¤íŠ¸, ê° í•­ëª©ì€ title, description í¬í•¨, descriptionì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.")
]

# LangChainì˜ StructuredOutputParserë¥¼ ì‚¬ìš©í•˜ì—¬ JSON í¬ë§·ì„ ì •ì˜
rag_parser = StructuredOutputParser.from_response_schemas(rag_response_schemas)

# JSON í¬ë§·ì„ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬
escaped_format = rag_parser.get_format_instructions().replace("{", "{{").replace("}", "}}")

# RAG ë°©ì‹ ì±Œë¦°ì§€ ì¶”ì²œì„ ìœ„í•œ PromptTemplate ì •ì˜
custom_prompt = PromptTemplate(
    input_variables=["context", "query", "messages"],
    template=f"""
ë‹¤ìŒ ë¬¸ì„œì™€ ì´ì „ ëŒ€í™” ê¸°ë¡ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ì ì ˆí•œ ì¹œí™˜ê²½ ì±Œë¦°ì§€ë¥¼ 3ê°œ ì¶”ì²œí•´ì£¼ì„¸ìš”.
ë°˜ë“œì‹œ ë¬¸ì„œì—ì„œ ì œê³µëœ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì´ì „ ëŒ€í™” ê¸°ë¡:
{{messages}}

ë¬¸ì„œ:
{{context}}

í˜„ì¬ ìš”ì²­:
{{query}}

ì‘ë‹µì€ ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ì„ ë”°ë¼ì£¼ì„¸ìš”:
{escaped_format}
"""
)

# LLM ì´ˆê¸°í™” (VertexAI)
llm = VertexAI(model_name="gemini-2.0-flash", temperature=0.3)

# LLMChain ì²´ì¸ ìƒì„± (retrieverëŠ” app_routerì—ì„œ ë³„ë„ ì‚¬ìš©)
qa_chain = LLMChain(
    llm=llm,
    prompt=custom_prompt
)

# ëŒ€í™” ìƒíƒœë¥¼ ê´€ë¦¬í•˜ê¸° ìœ„í•œ íƒ€ì… ì •ì˜
class ChatState(TypedDict):
    messages: Annotated[Sequence[str], "ëŒ€í™” ê¸°ë¡"]
    current_query: str
    context: str
    response: str
    should_continue: bool  # ëŒ€í™” ê³„ì† ì—¬ë¶€
    error: Optional[str]   # ì˜¤ë¥˜ ë©”ì‹œì§€
    docs: Optional[list]   # ê²€ìƒ‰ëœ ë¬¸ì„œ
    sessionId: str   # ì„¸ì…˜ ID

# ëŒ€í™” ê·¸ë˜í”„ ë…¸ë“œ ì •ì˜
def validate_query(state: ChatState) -> ChatState:
    """ì‚¬ìš©ì ì§ˆë¬¸ ìœ íš¨ì„± ê²€ì‚¬"""
    if len(state["current_query"].strip()) < 5:
        state["error"] = "ì§ˆë¬¸ì€ ìµœì†Œ 5ì ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤."
        state["should_continue"] = False
    else:
        state["should_continue"] = True
    return state

def retrieve_context(state: ChatState) -> ChatState:
    """ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰"""
    if not state["should_continue"]:
        return state
    try:
        # RAG ê²€ìƒ‰ ìˆ˜í–‰
        docs = retriever.get_relevant_documents(state["current_query"])
        state["docs"] = docs
        state["context"] = "\n".join([doc.page_content for doc in docs])
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
        if not docs:
            state["error"] = "ê´€ë ¨ëœ ì±Œë¦°ì§€ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            state["should_continue"] = False
    except Exception as e:
        state["error"] = f"ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        state["should_continue"] = False
    return state

def generate_response(state: ChatState) -> ChatState:
    """ì‘ë‹µ ìƒì„±"""
    if not state["should_continue"]:
        return state
    try:
        messages = "\n".join(state["messages"])
        print(f"Generating response for query: {state['current_query']}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
        
        response = qa_chain.invoke({
            "context": state["context"],
            "query": state["current_query"],
            "messages": messages
        })
        
        print(f"Raw LLM response: {response['text']}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
        
        # JSON íŒŒì‹± ì‹œë„
        try:
            response_text = response["text"]
            if "```json" in response_text:
                response_text = response_text.split("```json")[1]
            if "```" in response_text:
                response_text = response_text.split("```")[0]
            response_text = response_text.strip()
            
            parsed_response = json.loads(response_text)
            # í•„ìˆ˜ í•„ë“œ ê²€ì¦
            if "recommend" not in parsed_response or "challenges" not in parsed_response:
                raise ValueError("ì‘ë‹µì— í•„ìˆ˜ í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            if not isinstance(parsed_response["challenges"], list):
                raise ValueError("challengesëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.")
            
            state["response"] = json.dumps(parsed_response, ensure_ascii=False)
            print(f"Parsed response: {state['response']}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
            
        except json.JSONDecodeError as e:
            print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {str(e)}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
            state["error"] = "ì‘ë‹µ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤."
            state["should_continue"] = False
            return state
        except ValueError as e:
            print(f"ì‘ë‹µ ê²€ì¦ ì˜¤ë¥˜: {str(e)}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
            state["error"] = str(e)
            state["should_continue"] = False
            return state
        
        # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
        state["messages"] = list(state["messages"]) + [
            f"User: {state['current_query']}",
            f"Assistant: {state['response']}"
        ]
    except Exception as e:
        print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {str(e)}")  # ë””ë²„ê¹…ìš© ë¡œê·¸
        state["error"] = f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        state["should_continue"] = False
    return state

def handle_error(state: ChatState) -> ChatState:
    """ì˜¤ë¥˜ ì²˜ë¦¬"""
    if state["error"]:
        state["response"] = state["error"]
        # ì˜¤ë¥˜ ë©”ì‹œì§€ë„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        state["messages"] = list(state["messages"]) + [
            f"User: {state['current_query']}",
            f"Assistant: {state['error']}"
        ]
    return state

# ëŒ€í™” ê·¸ë˜í”„ êµ¬ì„±
def create_chat_graph():
    workflow = StateGraph(ChatState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("validate", validate_query)
    workflow.add_node("retrieve", retrieve_context)
    workflow.add_node("generate", generate_response)
    workflow.add_node("handle_error", handle_error)
    
    # ì—£ì§€ ì—°ê²°
    workflow.add_edge("validate", "retrieve")
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", "handle_error")
    workflow.add_edge("handle_error", END)
    
    # ì¡°ê±´ë¶€ ë¼ìš°íŒ…
    workflow.add_conditional_edges(
        "validate",
        lambda x: "retrieve" if x["should_continue"] else "handle_error"
    )
    
    # ì‹œì‘ ë…¸ë“œ ì„¤ì •
    workflow.set_entry_point("validate")
    
    return workflow.compile()

# ëŒ€í™” ê·¸ë˜í”„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
chat_graph = create_chat_graph()

# ëŒ€í™” ìƒíƒœ ì €ì¥ì†Œ
conversation_states: Dict[str, ChatState] = {}

def process_chat(sessionId: str, query: str) -> str:
    """ëŒ€í™” ì²˜ë¦¬ í•¨ìˆ˜"""
    # ì´ì „ ëŒ€í™” ìƒíƒœ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒˆë¡œ ìƒì„±
    if sessionId not in conversation_states:
        conversation_states[sessionId] = {
            "messages": [],
            "current_query": "",
            "context": "",
            "response": "",
            "should_continue": True,
            "error": None,
            "docs": None,
            "sessionId": sessionId
        }
    
    # í˜„ì¬ ìƒíƒœ ì—…ë°ì´íŠ¸
    state = conversation_states[sessionId]
    state["current_query"] = query
    
    # ëŒ€í™” ê·¸ë˜í”„ ì‹¤í–‰
    result = chat_graph.invoke(state)
    
    # ìƒíƒœ ì €ì¥
    conversation_states[sessionId] = result
    
    return result["response"]

def clear_conversation(sessionId: str):
    """ëŒ€í™” ê¸°ë¡ ì‚­ì œ"""
    if sessionId in conversation_states:
        del conversation_states[sessionId]

def get_conversation_history(sessionId: str) -> List[str]:
    """ëŒ€í™” ê¸°ë¡ ì¡°íšŒ
    
    Args:
        sessionId: ì‚¬ìš©ì ì„¸ì…˜ ID
    
    Returns:
        List[str]: ëŒ€í™” ê¸°ë¡ ë¦¬ìŠ¤íŠ¸
    """
    if sessionId in conversation_states:
        return conversation_states[sessionId]["messages"]
    return []
````

    + LLM_chatbot_base_info_model.py

```python
# LLM_chatbot_base_info_model.py
from google.cloud import aiplatform
from vertexai.preview.generative_models import GenerativeModel
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.prompts import PromptTemplate
from google.oauth2 import service_account
from fastapi import HTTPException
from fastapi.responses import JSONResponse
from dotenv import load_dotenv
import os
import json

load_dotenv()

#  í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°’ ê°€ì ¸ì˜¤ê¸°
SERVICE_ACCOUNT_FILE = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT")
LOCATION = os.getenv("VERTEX_AI_LOCATION")
MODEL_NAME = os.getenv("VERTEX_MODEL_NAME")

# Vertex AI ì´ˆê¸°í™”
credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)
aiplatform.init(project=PROJECT_ID, location=LOCATION, credentials=credentials)

# GenerativeModel ì´ˆê¸°í™”(SDK ë°©ì‹ ì‚¬ìš©)
model = GenerativeModel(model_name=MODEL_NAME)

# base-info_response_schemas ì •ì˜
base_response_schemas = [
    ResponseSchema(name="recommend", description=f"ì¶”ì²œ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ì¶œë ¥í•´ì¤˜.(ì˜ˆ: 'ì´ëŸ° ì±Œë¦°ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.')"),
    ResponseSchema(name="challenges", description="ì¶”ì²œ ì±Œë¦°ì§€ ë¦¬ìŠ¤íŠ¸, ê° í•­ëª©ì€ title, description í¬í•¨, descriptionì€ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.")
                   ]

# base-info_output_parser ì •ì˜ 
base_parser = StructuredOutputParser.from_response_schemas(base_response_schemas)

# base-info_prompt ì •ì˜
escaped_format = base_parser.get_format_instructions().replace("{", "{{").replace("}", "}}")
base_prompt = PromptTemplate(
    input_variables=["location", "workType", "category"],
    template=f"""
{{location}} í™˜ê²½ì— ìˆëŠ” {{workType}} ì‚¬ìš©ìê°€ {{category}}ë¥¼ ì‹¤ì²œí•  ë•Œ,
ì ˆëŒ€ì ìœ¼ë¡œ í™˜ê²½ì— ë„ì›€ì´ ë˜ëŠ” ì±Œë¦°ì§€ë¥¼ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œ 3ê°€ì§€ ì¶”ì²œí•´ì£¼ì„¸ìš”.

JSON í¬ë§·:
{escaped_format}

ì‘ë‹µì€ ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ ê·¸ëŒ€ë¡œ ì¶œë ¥í•˜ì„¸ìš”.

"""
)

# base-info_Output Parser ì •ì˜
def get_llm_response(prompt):
    try:
        model = GenerativeModel(model_name=MODEL_NAME)
        response = model.generate_content(prompt)

        raw_text = response.text if hasattr(response, 'text') else response
    
        if isinstance(raw_text, dict): # dictì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            parsed = raw_text
        else:
            text = raw_text.strip()
            parsed = base_parser.parse(text)
            if isinstance(parsed.get("challenges"), str):
                parsed["challenges"] = json.loads(parsed["challenges"])

        return JSONResponse(
            status_code=200,
            content={
                "status": 200,
                "message": "ì„±ê³µ!",
                "data": parsed
            }
        )

    except HTTPException as http_err:
        raise http_err
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì±Œë¦°ì§€ ì¶”ì²œ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

```

    + chatbot_router.py

```python
# chatbot_router.py
from model.chatbot.LLM_chatbot_base_info_model import base_prompt, get_llm_response
from model.chatbot.LLM_chatbot_free_text_model import qa_chain, retriever, process_chat, clear_conversation
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from fastapi import HTTPException
from pydantic import BaseModel
from typing import Optional
import json
import re

router = APIRouter()

# í‚¤ì›Œë“œ ë° ë¹„ì†ì–´ í•„í„°ë§ ë¦¬ìŠ¤íŠ¸
ENV_KEYWORDS = [
    "í™˜ê²½", "ì§€êµ¬", "ì—ì½”", "ì œë¡œì›¨ì´ìŠ¤íŠ¸", "íƒ„ì†Œ", "ë¶„ë¦¬ìˆ˜ê±°", "í”Œë¼ìŠ¤í‹±", "í…€ë¸”ëŸ¬", "ê¸°í›„", "ì¹œí™˜ê²½",
    "ì¼íšŒìš©", "ë¯¸ì„¸ë¨¼ì§€", "ì¬í™œìš©", "ìì›", "ëŒ€ì¤‘êµí†µ", "ë„ë³´", "ë¹„ê±´", "íƒ„ì†Œì¤‘ë¦½", "ê·¸ë¦°", "ì—ë„ˆì§€",
    "ì“°ë ˆê¸°","ì•„ë¬´","ì¶”ì²œ","ì±Œë¦°ì§€","ë„ì›€","ë„ì™€ì¤˜","ìì„¸íˆ","ìƒì„¸íˆ"
    ]

BAD_WORDS = [
    "ì‹œë°œ", "ì”¨ë°œ", "fuck", "shit", "ê°œìƒˆë¼", "ë³‘ì‹ ", "ã……ã…‚", "ã…„", "ã…‚ã……","fuckyou", "asshole", "tlqkf","ã…ˆ"
    ]

class CategoryRequest(BaseModel):
    sessionId: Optional[str] = None
    location: Optional[str] = None
    workType: Optional[str] = None
    category: Optional[str] = None
class FreeTextRequest(BaseModel):
    sessionId: Optional[str] = None
    location: Optional[str] = None
    workType: Optional[str] = None
    message: Optional[str] = None

# ë¹„-RAG ë°©ì‹ ì±Œë¦°ì§€ ì¶”ì²œ
@router.post("/ai/chatbot/recommendation/base-info")
def select_category(req: CategoryRequest):
    missing_fields = []
    # í•„ìˆ˜ í•„ë“œ ê²€ì‚¬
    if not req.location:
        missing_fields.append("location")
    if not req.workType:
        missing_fields.append("workType")
    if not req.category:
        missing_fields.append("category")
    if missing_fields:
        return JSONResponse(
            status_code=400,
            content={
                "status": 400,
                "message": f"{missing_fields}ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.",
                "data": None
            }
        )

    # LLM í˜¸ì¶œì„ ìœ„í•œ prompt êµ¬ì„±
    prompt = base_prompt.format(
        location=req.location,
        workType=req.workType,
        category=req.category
    )

    try:
        response = get_llm_response(prompt)
        # sessionIdê°€ ìˆëŠ” ê²½ìš° ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        if req.sessionId:
            process_chat(req.sessionId, f"ì¹´í…Œê³ ë¦¬: {req.category}, ìœ„ì¹˜: {req.location}, ì§ì—…: {req.workType}")
        return response
    except HTTPException as http_err:
        raise http_err # ë‚´ë¶€ HTTPExceptionì„ ë¨¼ì € ì²˜ë¦¬
    except Exception as e:
        return JSONResponse(
            status_code=502,
            content={
                "status": 502,
                "message": "AI ì„œë²„ë¡œë¶€í„° ì¶”ì²œ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "data": None
            }
        )

# LangChain ê¸°ë°˜ RAG ì¶”ì²œ
@router.post("/ai/chatbot/recommendation/free-text")
def freetext_rag(req: FreeTextRequest):
    missing_fields = []
    if not req.location:
        missing_fields.append("location")
    if not req.workType:
        missing_fields.append("workType")
    if not req.message or not req.message.strip():
        missing_fields.append("message")

    if missing_fields:
        return JSONResponse(
            status_code=400,
            content={
                "status": 400,
                "message": f"{missing_fields}ì€ í•„ìˆ˜ì…ë‹ˆë‹¤.",
                "data": None
            }
        )
    if len(req.message.strip()) < 5:
        return JSONResponse(
            status_code=422,
            content={
                "status": 422,
                "message": "messageëŠ” ë¬¸ìì—´ì´ì–´ì•¼ í•˜ë©°, ìµœì†Œ 5ì ì´ìƒì˜ ë¬¸ìì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.",
                "data": None
            }
        )
        
    message_lower = req.message.lower()
    if not any(k in req.message for k in ENV_KEYWORDS) or any(b in message_lower for b in BAD_WORDS):
        return JSONResponse(
            status_code=200,
            content={
                "status": 200,
                "message": "ì‚¬ìš©ìì—ê²Œ ììœ  ë©”ì„¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì±Œë¦°ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.",
                "data": {
                    "recommend": "ì €ëŠ” ì¹œí™˜ê²½ ì±Œë¦°ì§€ë¥¼ ì¶”ì²œí•´ë“œë¦¬ëŠ” Leafresh ì±—ë´‡ì´ì—ìš”! í™˜ê²½ ê´€ë ¨ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë” ì˜ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”.",
                    "challenges": None
                }
            }
        )
    
    try:
        # ëŒ€í™” ê¸°ë¡ì„ í¬í•¨í•œ ì‘ë‹µ ìƒì„±
        response_text = process_chat(req.sessionId, req.message)
        
        try:
            # JSON íŒŒì‹± ì‹œë„
            parsed = json.loads(response_text)
            
            if isinstance(parsed.get("challenges"), str):
                parsed["challenges"] = json.loads(parsed["challenges"])
            
            return JSONResponse(
                status_code=200,
                content={
                    "status": 200,
                    "message": "ì‚¬ìš©ì ììœ  ë©”ì‹œì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì±Œë¦°ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.",
                    "data": parsed
                }
            )
            
        except json.JSONDecodeError:
            return JSONResponse(
                status_code=500,
                content={
                    "status": 500,
                    "message": "ì±Œë¦°ì§€ ì¶”ì²œ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
                    "data": None
                }
            )
            
    except Exception as e:
        return JSONResponse(
            status_code=502,
            content={
                "status": 502,
                "message": "AI ì„œë²„ë¡œë¶€í„° ì¶”ì²œ ê²°ê³¼ë¥¼ ë°›ì•„ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                "data": None
            }
        )
```

## ì½”ë“œ ì„¤ëª…

- `sessionId` ì¶”ê°€ í•˜ì˜€ê³ , `conversation_states` ë¥¼ ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì˜€ë‹¤.
- ì „ì—­ë³€ìˆ˜ê°€ ë˜ë¯€ë¡œ base-infoì™€ free-text ë‘ ëª¨ë“œê°€ ë™ì¼í•œ conversation_states ë”•ì…”ë„ˆë¦¬ë¥¼ ê³µìœ í•˜ê²Œ ëœë‹¤
    - ì´ ë§ì€ ì¦‰ìŠ¨ ê°™ì€ `sessionId` ë¥¼ ì‚¬ìš©í•˜ë©´ ì–´ëŠ APIë“ ì§€ ëŒ€í™” ë‚´ìš©ì´ í•˜ë‚˜ë¡œ ëˆ„ì ëœë‹¤ëŠ” ëœ»ì´ë‹¤
- ì„¤ê³„ ë°©ì‹ì—ì„œ ìˆ˜ì •í•´ì•¼ í•  ë¶€ë¶„ì„ ëŠê¼ˆë‹¤.
    - ì§€ê¸ˆ ì±—ë´‡ ìˆœì„œê°€ base-info ì…ë ¥ í›„ â†’ ì¹´í…Œê³ ë¦¬ ì¬ì„ íƒ or ììœ ì…ë ¥(free-text)ì¸ë°, í˜„ì¬ ì½”ë“œì—ì„  free-textì˜ í”„ë¡¬í”„íŒ…ì€ â€œí˜„ì¬ ìš”ì²­:{{query}}â€ ë¡œë§Œ ë°›ê³  ìˆë‹¤.
    - API ë°”ë”” ì„¤ê³„ë¥¼ ìˆ˜ì •í•´ì•¼ í•  ê²ƒ ê°™ê³ (ì‹¤ì œë¡œ free-text ë°”ë”” ì•ˆì— `location` & `workType` ì´ í•„ìš”í•œê°€ì— ëŒ€í•´ì„œ)

---

##  ëª¨ì˜ ë©´ì ‘ ê³µë¶€

### 1. í˜ì´ì§•

-  ê³µë¶€í•˜ë‹¤ê°€, ì„¸ê·¸ë©˜í…Œì´ì…˜(segmentation) ë°©ì‹ì— ëŒ€í•´ ê¶ê¸ˆí•´ì¡Œë‹¤.
- ë˜í•œ, ì™¸ë¶€ ë‹¨í¸í™” ë¬¸ì œê°€ ë¬´ì—‡ì¸ì§€ì— ëŒ€í•´ ì•Œì•„ë³´ì•˜ë‹¤.
    - ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ì‹: í”„ë¡œì„¸ìŠ¤ë¥¼ ì˜ë¯¸ ìˆëŠ” ë…¼ë¦¬ì  ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ë°©ì‹ì´ë‹¤.
    - ì—°ì†ì ì¸ ì£¼ì†Œ ê³µê°„ì„ ì‚¬ìš©í•˜ëŠ” ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°©ì‹ì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ì™¸ë¶€ ë‹¨í¸í™” ê¸°ë²•ì„ í•´ê²° í•  ìˆ˜ìˆë‹¤.
        - ì™œ? -> í˜ì´ì§€ê°€ ë…¼ë¦¬ì  ë©”ëª¨ë¦¬ ë‹¨ìœ„ë¡œ ìë¥´ëŠ” í¬ê¸°(ì•½ 4KB)ì™€ ë™ì¼í•˜ê²Œ ë©”ëª¨ë¦¬(RAM)ë¥¼ í”„ë ˆì„ìœ¼ë¡œ ë‚˜ëˆ ì„œ ê° í”„ë ˆì„ì— í˜ì´ì§€ë¥¼ ë¹„ì—°ì†ì ìœ¼ë¡œ í• ë‹¹ í•  ìˆ˜ ìˆê¸° ë•Œë¬¸
        - â€œë…¼ë¦¬ ì£¼ì†Œ ê³µê°„â€ ì´ë€?:

            > CPUê°€ ì¸ì‹í•˜ëŠ” ë©”ëª¨ë¦¬ ì£¼ì†Œ ê³µê°„ì´ë‹¤. -> í”„ëŸ¬ì„¸ìŠ¤ê°€ ì‚¬ìš©í•˜ëŠ” ê°€ìƒì˜ ë©”ëª¨ë¦¬ ê³µê°„

        - ê·¸ëŸ¼ ì™œ ë…¼ë¦¬ ì£¼ì†Œê°€ í•„ìš”í•œê°€?:

            > í˜„ì œ ìš´ì˜ì²´ì œëŠ” ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ê°€ ë™ì‹œì— ì‹¤í–‰ë˜ê¸° ë•Œë¬¸ì— ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ìê¸°ë§Œì˜ ë…ë¦½ì ì¸ ì£¼ì†Œ ê³µê°„ì„ ê°–ë„ë¡ í•´ì•¼í•¨.

    - ì•„ ê·¸ëŸ¼ ì—¬ëŸ¬ í”„ë¡œì„¸ìŠ¤ê°€ ì‘ë™í•˜ëŠ” ì£¼ì†Œ ê³µê°„ì€ "ì„¤ê³„ ê³„íšâ€ì¸ ë¿ì´ê³ , ì‹¤ì œëŠ” RAMì— ë¬¼ë¦¬ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ í• ë‹¹ ë°›ì•„ì•¼ ì‹¤í–‰ë˜ëŠ”êµ¬ë‚˜!!
    - ì˜ˆì‹œ:   
        - í”„ë¡œì„¸ìŠ¤ AëŠ” 0x00000000 ~ 0x7FFFFFFF
        - í”„ë¡œì„¸ìŠ¤ Bë„ 0x00000000 ~ 0x7FFFFFFF

        â†’ ì£¼ì†ŒëŠ” ê°™ì•„ ë³´ì´ì§€ë§Œ **ì‹¤ì œë¡œëŠ” ì„œë¡œ ë‹¤ë¥¸ ê³µê°„ì´ë‹¤!**

### 2. CPU ìŠ¤ì¼€ì¤„ë§

- í”„ë¡œì„¸ìŠ¤ë€?:
    - ì‹¤í–‰ ì¤‘ì¸ í”„ë¡œê·¸ë¨(ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì™€ ìì› ê³µìœ  ì•ˆí•¨)

- ìŠ¤ë ˆë“œë€?:
    - í”„ë¡œì„¸ìŠ¤ ë‚´ë¶€ì—ì„œ ì‹¤ì œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ” ì‹¤í–‰ íë¦„ ë‹¨ìœ„
    - í•˜ë‚˜ì˜ í”„ë¡œì„¸ìŠ¤ì—ëŠ” **ì—¬ëŸ¬ê°œì˜ ìŠ¤ë ˆë“œë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‹¤!**

- íë€?:
    > ì„ ì…ì„ ì¶œ ë°©ì‹ì˜ ìë£Œêµ¬ì¡° -> ë¨¼ì € ì˜¨ë†ˆ ì²˜ë¦¬

    ### ì„ ì í˜•

    1. ë¼ìš´ë“œ ë¡œë¹ˆ (Round Robin)
        - ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ **í•˜ë‚˜ì˜ ì¤€ë¹„ íì— ì¤„ ì„œ ìˆìŒ**
        - ì •í•´ì§„ ì‹œê°„(íƒ€ì„í€€í…€) ë™ì•ˆ CPUë¥¼ ë‚˜ëˆ  ê°€ì§
        - ìˆœì„œëŒ€ë¡œ ëŒë©´ì„œ ì²˜ë¦¬ â†’ ë‹¤ì‹œ ë§¨ ë’¤ë¡œ ê° â†’ â€œíšŒì „ëª©ë§ˆâ€ ëŠë‚Œ ğŸ 
    2. ë‹¤ë‹¨ê³„ í
        - ìš°ì„ ìˆœìœ„ ë³„ë¡œ íë¥¼ ì—¬ëŸ¬ê°œ ì¤€ë¹„
        - ë†’ì€ ìš°ì„ ìˆœìœ„ íê°€ í•­ìƒ ë¨¼ì € ì‹¤í–‰
    3. ë‹¤ë‹¨ê³„ í”¼ë“œë°± í(+ì—ì—ì§• ê¸°ë²•)
        - CPUë¥¼ ì˜¤ë˜ì“°ë©´ -> ë‚®ì€ ìš°ì„ ìˆœìœ„ íë¡œ ì´ë™
        - ê·¸ëŸ¼ ì§§ê²Œ ì“°ë©´? -> ë†’ì€ ìš°ì„ ìˆœìœ„ íë¡œ ì´ë™
        - ê³µì •ì„± + ì‘ë‹µì†ë” ëª¨ë‘ ê³ ë ¤í•¨ êµ³! 

    ### ë¹„ì„ ì í˜•

    1. FCFS(First Come First Service) ë°©ì‹ -> ë¨¼ì € ì˜¨ë†ˆ ì„œë¹„ìŠ¤ ì²˜ë¦¬
    2. SFJ(Shortest Job First) ë°©ì‹ -> CPU ì ê²Œ ì“´ë†ˆ ë¨¼ì € ì²˜ë¦¬

### 3. ë®¤í…ìŠ¤(Mutex: Mutual Exclusion_ìƒí˜¸ê°„ ì œì™¸ã…‹)ë€?

- ì—¬ëŸ¬ ê°œì˜ ìŠ¤ë ˆë“œê°€ ê³µìœ  ìì›ì— ì ‘ê·¼í•˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ë°©ì‹ì´ë‹¤. í•œë²ˆì— í•˜ë‚˜ì˜ ìŠ¤ë ˆë“œê°€ ì ‘ê·¼ í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë•Œ Lockì„ ê°€ì§€ê³  ì ‘ê·¼í•˜ë©°, í•´ë‹¹ ìŠ¤ë ˆë“œê°€ ì ‘ê·¼ì„ ì¢…ë£Œí•˜ë©´ ë‹¤ìŒ ìŠ¤ë ˆë“œê°€ lockì˜ ì†Œìœ ë¥¼ ê°€ì§€ë©° ì ‘ê·¼í•˜ëŠ” ë°©ì‹ì´ë‹¤.  

- ì™œ í•˜ë‚˜ì˜ ìŠ¤ë ˆë“œë§Œ ì ‘ê·¼í•¨?

    **1.** **ë°ì´í„°ì˜ ì¼ê´€ì„±ê³¼ ì•ˆì •ì„± ìœ ì§€**
    - ì—¬ëŸ¬ ìŠ¤ë ˆë“œê°€ ë™ì‹œì— ê°™ì€ ìì›(ì˜ˆ: ë³€ìˆ˜, íŒŒì¼, ë¦¬ìŠ¤íŠ¸ ë“±)ì„ ìˆ˜ì •í•˜ë©´ **ë°ì´í„° ì¶©ëŒì´ë‚˜ ì˜¤ì—¼**ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ

    **2.** **Race Condition(ê²½ìŸ ì¡°ê±´) ë°©ì§€**
    - ì—¬ëŸ¬ ìŠ¤ë ˆë“œê°€ â€œëˆ„ê°€ ë¨¼ì € ìì›ì— ì ‘ê·¼í• ì§€â€ ê²½í•©í•  ë•Œ ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•˜ê¸°ì— 

### 4. êµì°© ìƒíƒœë€?

- 2ê°œ ì´ìƒì˜ í”„ë¡œì„¸ìŠ¤ ë˜ëŠ” ì“°ë ˆë“œê°€ ì„œë¡œì˜ ì‘ì—…ì„ "ë¬´í•œíˆ" ê¸°ë‹¤ë¦¬ëŠ” DeadLockìƒíƒœì— ë¹ ì§€ëŠ” ê²ƒ.

    - ì„œë¡œì˜ ìì›ì„ ì ìœ í•˜ê³  ë°˜ë‚©í•˜ì§€ ì•Šìœ¼ë©°, ìƒëŒ€ë°©ì˜ ìì›ì„ ìš”ì²­í•¨ìœ¼ë¡œì¨ ìƒê¸°ëŠ” í˜„ìƒ
    - ì´ëŸ¬í•œ êµì°© ìƒíƒœê°€ ìƒê¸°ë ¤ë©´ 4ê°€ì§€ ì¡°ê±´ì´ ì¶©ì¡± ë˜ì–´ì•¼ í•¨
        1. ìƒí˜¸ ë°°ì œ:
            - í•œ ë²ˆì— í•˜ë‚˜ì˜ í”„ë¡œì„¸ìŠ¤ ë§Œì´ íŠ¹ì • ìì›ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤
        1. ì ìœ  ëŒ€ê¸°:
            - í”„ë¡œì„¸ìŠ¤ê°€ í• ë‹¹ëœ ìì›ì„ ì ìœ í•œ ìƒíƒœì—ì„œ ë‹¤ë¥¸ ìì›ì„ ìš”ì²­ but ì–»ì§€ ëª»í•˜ëŠ” ìƒí™©
        1. ë¹„ì„ ì :
            - í”„ë¡œì„¸ìŠ¤ê°€ ì–´ë–¤ ìì›ì„ ëë‚¼ ë•Œ ê¹Œì§€ ê·¸ ìì›ì„ ì“°ì§€ ëª»í•˜ëŠ” ìƒí™©
        1. ìˆœí™˜ ëŒ€ê¸°:
            - ê° í”„ë¡œì„¸ìŠ¤ëŠ” ìˆœí™˜ì ìœ¼ë¡œ ë‹¤ìŒ í”„ë¡œì„¸ìŠ¤ê°€ í•„ìš”ë¡œ í•˜ëŠ” ìì›ì„ ìš”ì²­í•˜ëŠ” ìƒí™©

5. í”„ë¡œì„¸ìŠ¤ ì™€ ìŠ¤ë ˆë“œ ì°¨ì´
- í”„ë¡œì„¸ìŠ¤ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ë˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ì˜ì—­ ë˜í•œ ê³µìœ  í•˜ì§€ ì•ŠëŠ”ë‹¤. ë°˜ë©´ì— ìŠ¤ë ˆë“œëŠ” í•œ í”„ë¡œì„¸ìŠ¤ ë‚´ì—ì„œ ID, ë ˆì§€ìŠ¤í„° ê°’, ìŠ¤íƒ ê°™ì€ ì •ë³´ë“¤ì„ ê°€ì§€ë©° ìì›ì„ ê³µìœ í•œë‹¤. ê·¸ë˜ì„œ ì„œë¡œ ë‹¤ë¥¸ ì½”ë“œë¥¼ ë™ì‹œì— ì‹¤í–‰ì‹œí‚¬ ìˆ˜ ìˆë‹¤. 
- ì˜ˆì‹œ:

```python
# í•˜ë‚˜ì˜ íŒŒì´ì¬ í”„ë¡œê·¸ë¨(= í”„ë¡œì„¸ìŠ¤)
def download_image():
# ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ ì½”ë“œ
...

def resize_image():
# ë¦¬ì‚¬ì´ì§• ì½”ë“œ
...

def save_image():
# ì €ì¥ ì½”ë“œ
...
# ì´ ì„¸ ê°€ì§€ ì‘ì—…ì„ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰í•  ìˆ˜ ìˆë‹¤
```