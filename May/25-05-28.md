# 오늘 내가 배운 것들(Today I Learned)

- feedback(API) 모델 설계 및 SSE방식 적용
  - 사실상 챗봇도 이떄 sessionId & 대화 내역 추가
- 설계 방식에서 수정해야 할 부분을 느꼈다.
  - 지금 챗봇 순서가 base-info 입력 후 → 카테고리 재선택 or 자유입력(free-text)인데, 현재 코드에선 free-text의 프롬프팅은 “현재 요청:{{query}}” 로만 받고 있다.
- 또한 사용자 별로 sessionId를 발급해 챗봇에서의 대화를 저장하여 해당 대화 기반으로 LLM답변을 만드는 기능이다.

---

- 아래 코드는 다시 경신되었음.
- LLM_chatbot_free_text_model.py

````python
# LLM_chatbot_free_text_model.py
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_qdrant import Qdrant
from langchain_community.embeddings import SentenceTransformerEmbeddings
from qdrant_client import QdrantClient
from langchain_google_vertexai import VertexAI
from dotenv import load_dotenv
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated, Sequence, Optional, Dict, List
import os
import json

load_dotenv()

QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME")

qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
embedding_model = SentenceTransformerEmbeddings(model_name="BAAI/bge-small-en-v1.5")

vectorstore = Qdrant(
    client=qdrant_client,
    collection_name=COLLECTION_NAME,
    embeddings=embedding_model
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) # 사용자 질문으로 부터 가장 유사한 3개 문서 검색

# RAG 방식 챌린지 추천을 위한 Output Parser 정의
rag_response_schemas = [
    ResponseSchema(name="recommend", description="추천 텍스트를 한 문장으로 출력해줘.(예: '이런 챌린지를 추천합니다.')"),
    ResponseSchema(name="challenges", description="추천 챌린지 리스트, 각 항목은 title, description 포함, description은 한 문장으로 요약해주세요.")
]

# LangChain의 StructuredOutputParser를 사용하여 JSON 포맷을 정의
rag_parser = StructuredOutputParser.from_response_schemas(rag_response_schemas)

# JSON 포맷을 이스케이프 처리
escaped_format = rag_parser.get_format_instructions().replace("{", "{{").replace("}", "}}")

# RAG 방식 챌린지 추천을 위한 PromptTemplate 정의
custom_prompt = PromptTemplate(
    input_variables=["context", "query", "messages"],
    template=f"""
다음 문서와 이전 대화 기록을 참고하여 사용자에게 적절한 친환경 챌린지를 3개 추천해주세요.

이전 대화 기록:
{{messages}}

문서:
{{context}}

현재 요청:
{{query}}

응답은 반드시 다음 JSON 형식을 따라주세요:
{escaped_format}
"""
)

# LLM 초기화 (VertexAI)
llm = VertexAI(model_name="gemini-2.0-flash", temperature=0.7)

# LLMChain 체인 생성 (retriever는 app_router에서 별도 사용)
qa_chain = LLMChain(
    llm=llm,
    prompt=custom_prompt
)

# 대화 상태를 관리하기 위한 타입 정의
class ChatState(TypedDict):
    messages: Annotated[Sequence[str], "대화 기록"]
    current_query: str
    context: str
    response: str
    should_continue: bool  # 대화 계속 여부
    error: Optional[str]   # 오류 메시지
    docs: Optional[list]   # 검색된 문서
    sessionId: str   # 세션 ID

# 대화 그래프 노드 정의
def validate_query(state: ChatState) -> ChatState:
    """사용자 질문 유효성 검사"""
    if len(state["current_query"].strip()) < 5:
        state["error"] = "질문은 최소 5자 이상이어야 합니다."
        state["should_continue"] = False
    else:
        state["should_continue"] = True
    return state

def retrieve_context(state: ChatState) -> ChatState:
    """관련 컨텍스트 검색"""
    if not state["should_continue"]:
        return state
    try:
        # RAG 검색 수행
        docs = retriever.get_relevant_documents(state["current_query"])
        state["docs"] = docs
        state["context"] = "\n".join([doc.page_content for doc in docs])
        
        # 검색된 문서가 없는 경우
        if not docs:
            state["error"] = "관련된 챌린지 정보를 찾을 수 없습니다."
            state["should_continue"] = False
    except Exception as e:
        state["error"] = f"컨텍스트 검색 중 오류 발생: {str(e)}"
        state["should_continue"] = False
    return state

def generate_response(state: ChatState) -> ChatState:
    """응답 생성"""
    if not state["should_continue"]:
        return state
    try:
        messages = "\n".join(state["messages"])
        print(f"Generating response for query: {state['current_query']}")  # 디버깅용 로그
        
        response = qa_chain.invoke({
            "context": state["context"],
            "query": state["current_query"],
            "messages": messages
        })
        
        print(f"Raw LLM response: {response['text']}")  # 디버깅용 로그
        
        # JSON 파싱 시도
        try:
            response_text = response["text"]
            if "```json" in response_text:
                response_text = response_text.split("```json")[1]
            if "```" in response_text:
                response_text = response_text.split("```")[0]
            response_text = response_text.strip()
            
            parsed_response = json.loads(response_text)
            # 필수 필드 검증
            if "recommend" not in parsed_response or "challenges" not in parsed_response:
                raise ValueError("응답에 필수 필드가 없습니다.")
            if not isinstance(parsed_response["challenges"], list):
                raise ValueError("challenges는 리스트 형태여야 합니다.")
            
            state["response"] = json.dumps(parsed_response, ensure_ascii=False)
            print(f"Parsed response: {state['response']}")  # 디버깅용 로그
            
        except json.JSONDecodeError as e:
            print(f"JSON 파싱 오류: {str(e)}")  # 디버깅용 로그
            state["error"] = "응답 형식이 올바르지 않습니다."
            state["should_continue"] = False
            return state
        except ValueError as e:
            print(f"응답 검증 오류: {str(e)}")  # 디버깅용 로그
            state["error"] = str(e)
            state["should_continue"] = False
            return state
        
        # 대화 기록 업데이트
        state["messages"] = list(state["messages"]) + [
            f"User: {state['current_query']}",
            f"Assistant: {state['response']}"
        ]
    except Exception as e:
        print(f"응답 생성 중 예외 발생: {str(e)}")  # 디버깅용 로그
        state["error"] = f"응답 생성 중 오류 발생: {str(e)}"
        state["should_continue"] = False
    return state

def handle_error(state: ChatState) -> ChatState:
    """오류 처리"""
    if state["error"]:
        state["response"] = state["error"]
        # 오류 메시지도 대화 기록에 추가
        state["messages"] = list(state["messages"]) + [
            f"User: {state['current_query']}",
            f"Assistant: {state['error']}"
        ]
    return state

# 대화 그래프 구성
def create_chat_graph():
    workflow = StateGraph(ChatState)
    
    # 노드 추가
    workflow.add_node("validate", validate_query)
    workflow.add_node("retrieve", retrieve_context)
    workflow.add_node("generate", generate_response)
    workflow.add_node("handle_error", handle_error)
    
    # 엣지 연결
    workflow.add_edge("validate", "retrieve")
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", "handle_error")
    workflow.add_edge("handle_error", END)
    
    # 조건부 라우팅
    workflow.add_conditional_edges(
        "validate",
        lambda x: "retrieve" if x["should_continue"] else "handle_error"
    )
    
    # 시작 노드 설정
    workflow.set_entry_point("validate")
    
    return workflow.compile()

# 대화 그래프 인스턴스 생성
chat_graph = create_chat_graph()

# 대화 상태 저장소
conversation_states: Dict[str, ChatState] = {}

def process_chat(sessionId: str, query: str) -> str:
    """대화 처리 함수"""
    # 이전 대화 상태 가져오기 또는 새로 생성
    if sessionId not in conversation_states:
        conversation_states[sessionId] = {
            "messages": [],
            "current_query": "",
            "context": "",
            "response": "",
            "should_continue": True,
            "error": None,
            "docs": None,
            "sessionId": sessionId
        }
    
    # 현재 상태 업데이트
    state = conversation_states[sessionId]
    state["current_query"] = query
    
    # 대화 그래프 실행
    result = chat_graph.invoke(state)
    
    # 상태 저장
    conversation_states[sessionId] = result
    
    return result["response"]

def clear_conversation(sessionId: str):
    """대화 기록 삭제"""
    if sessionId in conversation_states:
        del conversation_states[sessionId]

def get_conversation_history(sessionId: str) -> List[str]:
    """대화 기록 조회
    Args:
        sessionId: 사용자 세션 ID
    
    Returns:
        List[str]: 대화 기록 리스트
    """
    if sessionId in conversation_states:
        return conversation_states[sessionId]["messages"]
    return []

````

    + LLM_chatbot_base_info_model.py

```python
# LLM_chatbot_base_info_model.py
from google.cloud import aiplatform
from vertexai.preview.generative_models import GenerativeModel
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
from langchain.prompts import PromptTemplate
from google.oauth2 import service_account
from fastapi import HTTPException
from fastapi.responses import JSONResponse
from dotenv import load_dotenv
import os
import json

load_dotenv()

#  환경 변수에서 값 가져오기
SERVICE_ACCOUNT_FILE = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
PROJECT_ID = os.getenv("GOOGLE_CLOUD_PROJECT")
LOCATION = os.getenv("VERTEX_AI_LOCATION")
MODEL_NAME = os.getenv("VERTEX_MODEL_NAME")

# Vertex AI 초기화
credentials = service_account.Credentials.from_service_account_file(SERVICE_ACCOUNT_FILE)
aiplatform.init(project=PROJECT_ID, location=LOCATION, credentials=credentials)

# GenerativeModel 초기화(SDK 방식 사용)
model = GenerativeModel(model_name=MODEL_NAME)

# base-info_response_schemas 정의
base_response_schemas = [
    ResponseSchema(name="recommend", description=f"추천 텍스트를 한 문장으로 출력해줘.(예: '이런 챌린지를 추천합니다.')"),
    ResponseSchema(name="challenges", description="추천 챌린지 리스트, 각 항목은 title, description 포함, description은 한 문장으로 요약해주세요.")
                   ]

# base-info_output_parser 정의 
base_parser = StructuredOutputParser.from_response_schemas(base_response_schemas)

# base-info_prompt 정의
escaped_format = base_parser.get_format_instructions().replace("{", "{{").replace("}", "}}")
base_prompt = PromptTemplate(
    input_variables=["location", "workType", "category"],
    template=f"""
{{location}} 환경에 있는 {{workType}} 사용자가 {{category}}를 실천할 때,
절대적으로 환경에 도움이 되는 챌린지를 아래 JSON 형식으로 3가지 추천해주세요.

JSON 포맷:
{escaped_format}

응답은 반드시 위 JSON 형식 그대로 출력하세요.

"""
)

# base-info_Output Parser 정의
def get_llm_response(prompt):
    try:
        model = GenerativeModel(model_name=MODEL_NAME)
        response = model.generate_content(prompt)

        raw_text = response.text if hasattr(response, 'text') else response
    
        if isinstance(raw_text, dict): # dict이면 그대로 사용
            parsed = raw_text
        else:
            text = raw_text.strip()
            parsed = base_parser.parse(text)
            if isinstance(parsed.get("challenges"), str):
                parsed["challenges"] = json.loads(parsed["challenges"])

        return JSONResponse(
            status_code=200,
            content={
                "status": 200,
                "message": "성공!",
                "data": parsed
            }
        )

    except HTTPException as http_err:
        raise http_err
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"챌린지 추천 중 내부 오류 발생: {str(e)}")

```

    + chatbot_router.py

```python
# chatbot_router.py
from ..model.chatbot.LLM_chatbot_base_info_model import base_prompt, get_llm_response
from ..model.chatbot.LLM_chatbot_free_text_model import qa_chain, retriever, process_chat, clear_conversation
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from fastapi import HTTPException
from pydantic import BaseModel
from typing import Optional
import json
import re

router = APIRouter()

# 키워드 및 비속어 필터링 리스트
ENV_KEYWORDS = [
    "환경", "지구", "에코", "제로웨이스트", "탄소", "분리수거", "플라스틱", "텀블러", "기후", "친환경",
    "일회용", "미세먼지", "재활용", "자원", "대중교통", "도보", "비건", "탄소중립", "그린", "에너지",
    "쓰레기","아무","추천","챌린지","도움","도와줘","자세히","상세히"
    ]

BAD_WORDS = [
    "시발", "씨발", "fuck", "shit", "개새끼", "병신", "ㅅㅂ", "ㅄ", "ㅂㅅ","fuckyou", "asshole", "tlqkf","ㅈ"
    ]

class CategoryRequest(BaseModel):
    sessionId: Optional[str] = None
    location: Optional[str] = None
    workType: Optional[str] = None
    category: Optional[str] = None
class FreeTextRequest(BaseModel):
    sessionId: Optional[str] = None
    message: Optional[str] = None

# 비-RAG 방식 챌린지 추천
@router.post("/ai/chatbot/recommendation/base-info")
def select_category(req: CategoryRequest):
    missing_fields = []
    # 필수 필드 검사
    if not req.location:
        missing_fields.append("location")
    if not req.workType:
        missing_fields.append("workType")
    if not req.category:
        missing_fields.append("category")
    if missing_fields:
        return JSONResponse(
            status_code=400,
            content={
                "status": 400,
                "message": f"{missing_fields}은 필수입니다.",
                "data": None
            }
        )

    # LLM 호출을 위한 prompt 구성
    prompt = base_prompt.format(
        location=req.location,
        workType=req.workType,
        category=req.category
    )

    try:
        response = get_llm_response(prompt)
        # sessionId가 있는 경우 대화 기록에 추가
        if req.sessionId:
            process_chat(req.sessionId, f"카테고리: {req.category}, 위치: {req.location}, 직업: {req.workType}")
        return response
    except HTTPException as http_err:
        raise http_err # 내부 HTTPException을 먼저 처리
    except Exception as e:
        return JSONResponse(
            status_code=502,
            content={
                "status": 502,
                "message": "AI 서버로부터 추천 결과를 받아오는 데 실패했습니다.",
                "data": None
            }
        )

# LangChain 기반 RAG 추천
@router.post("/ai/chatbot/recommendation/free-text")
def freetext_rag(req: FreeTextRequest):
    missing_fields = []
    if not req.message or not req.message.strip():
        missing_fields.append("message")

    if missing_fields:
        return JSONResponse(
            status_code=400,
            content={
                "status": 400,
                "message": f"{missing_fields}은 필수입니다.",
                "data": None
            }
        )
    if len(req.message.strip()) < 5:
        return JSONResponse(
            status_code=422,
            content={
                "status": 422,
                "message": "message는 문자열이어야 하며, 최소 5자 이상의 문자열이어야 합니다.",
                "data": None
            }
        )
        
    message_lower = req.message.lower()
    if not any(k in req.message for k in ENV_KEYWORDS) or any(b in message_lower for b in BAD_WORDS):
        return JSONResponse(
            status_code=200,
            content={
                "status": 200,
                "message": "사용자에게 자유 메세지를 기반으로 챌린지를 추천합니다.",
                "data": {
                    "recommend": "저는 친환경 챌린지를 추천해드리는 Leafresh 챗봇이에요! 환경 관련 질문을 해주시면 더 잘 도와드릴 수 있어요.",
                    "challenges": None
                }
            }
        )
    
    try:
        # 대화 기록을 포함한 응답 생성
        response_text = process_chat(req.sessionId, req.message)
        
        try:
            # JSON 파싱 시도
            parsed = json.loads(response_text)
            
            if isinstance(parsed.get("challenges"), str):
                parsed["challenges"] = json.loads(parsed["challenges"])
            
            return JSONResponse(
                status_code=200,
                content={
                    "status": 200,
                    "message": "사용자 자유 메시지를 기반으로 챌린지를 추천합니다.",
                    "data": parsed
                }
            )
            
        except json.JSONDecodeError:
            return JSONResponse(
                status_code=500,
                content={
                    "status": 500,
                    "message": "챌린지 추천 중 내부 오류가 발생했습니다. 잠시 후 다시 시도해주세요.",
                    "data": None
                }
            )
            
    except Exception as e:
        return JSONResponse(
            status_code=502,
            content={
                "status": 502,
                "message": f"AI 서버로부터 추천 결과를 받아오는 데 실패했습니다.", # AI 서버 오류
                "data": None
                }
        )
```

- `location` & `workType`  삭제
- `sessionId` 추가 
- 전역 변수 `conversation_states` 설정
  - 전역변수가 되므로 base-info와 free-text 두 모드가 동일한 conversation_states 딕셔너리를 공유하게 된다
  - 이 말은 즉슨 같은 `sessionId` 를 사용하면 어느 API든지 대화 내용이 하나로 누적된다는 뜻

---

# 챗봇 및 피드백 모델 변경사항 분석 보고서

[data.25-05-28(수)]

## 1. 피드백 모델 (`LLM_feedback_model.py`)

### 1.1 주요 변경사항

- **스트리밍 응답 구현**

  ```python
  # Vertex AI를 통한 피드백 생성 (스트리밍 방식 사용)
  response = self.model.generate_content(
      prompt,
      generation_config={
          "temperature": 0.7,
          "top_p": 1,
          "top_k": 32,
          "max_output_tokens": self.max_tokens
      },
      stream=True
  )
  ```

- **응답 형식 표준화**

  ```python
  {
      "status": 200,
      "message": "피드백 생성 중",
      "data": {
          "feedback": "생성된 피드백 텍스트"
      }
  }
  ```

- 현재 피드백 시스템은 비동기 방식과 SSE(Server-Sent Events)를 함께 사용하고있다.

1. 비동기 처리
    - asyncio를 리용한 비동기 스트리밍 구현
    - 메인 스레드 블로킹을 방지하는 구조
2. 스트리밍 응답:
    - Vertex AI 모델의 stream=True 옵션을 사용하여 실시간 스트리밍
    - 청크 단위로 피드백을 전송

- 이러한 구조를 통해 사용자는 피드백이 생성되는 즉시 실시간으로 응답을 받을 수 있으며 서버의 메인 스레드가 블로킹 되지 않아 다른 요청들도 동시에 처리 가능하다.

#### 그럼 왜 비동기로 했을까?

1. 비동기 + SSE 방식의 의미:
    - 비동기: 서버가 요청을 받았을 때, 응답이 완료될 때까지 기다리지 않고 다른 작업을 할 수 있음
    - SSE: 서버에서 클라이언트로 실시간으로 데이터를 스트리밍하는 방식
    - 이 두 가지를 조합하면, 서버는 피드백을 생성하면서 동시에 다른 사용자의 요청도 처리할 수 있음

2. 동기 + SSE 방식:
    - 동기 방식도 SSE와 함께 사용 가능합니다
    - 차이점은:
        - 동기: 한 요청이 완료될 때까지 다른 요청을 처리하지 않음
        - 비동기: 여러 요청을 동시에 처리 가능

- 여러 사용자의 요청을 고려하면 비동기 + SSE 방식으로 구현해야 할 것 같다.

### 1.2 개선된 기능

- **실시간 피드백 생성**
  - 청크 단위로 피드백 전송
  - 비동기 처리로 성능 최적화
  - 한글 인코딩 지원

- **에러 처리 강화**
  - 모델 오류와 일반 서버 오류 구분
  - 상세한 에러 메시지 제공
  - 스택 트레이스 로깅

## 2. 챗봇 모델 (`LLM_chatbot_free_text_model.py`)

### 2.1 주요 변경사항

- **대화 상태 관리 시스템 도입**

  ```python
  class ChatState(TypedDict):
      messages: Annotated[Sequence[str], "대화 기록"]
      current_query: str
      context: str
      response: str
      should_continue: bool
      error: Optional[str]
      docs: Optional[list]
      sessionId: str
  ```

- **대화 그래프 구현**

  ```python
  def create_chat_graph():
      workflow = StateGraph(ChatState)
      workflow.add_node("validate", validate_query)
      workflow.add_node("retrieve", retrieve_context)
      workflow.add_node("generate", generate_response)
      workflow.add_node("handle_error", handle_error)
  ```

### 2.2 개선된 기능

- **세션 기반 대화 관리**

  - `sessionId`를 통한 대화 컨텍스트 유지
  - 대화 기록 저장 및 관리
  - 세션별 독립적인 상태 관리

- **RAG 시스템 개선**
  - 검색 결과 수 증가 (k=5)
  - 컨텍스트 기반 응답 생성
  - JSON 파싱 및 검증 강화

- **응답 품질 향상**
  - 필수 필드 검증
  - 응답 형식 표준화
  - 에러 처리 개선

## 3. 공통 개선사항

### 3.1 API 응답 형식

```python
{
    "status": int,      # HTTP 상태 코드
    "message": str,     # 응답 메시지
    "data": Any        # 응답 데이터
}
```

### 3.2 에러 처리

- **표준화된 에러 응답**
  - 400: 잘못된 요청
  - 500: 서버 오류
  - 상세한 에러 메시지

### 3.3 성능 최적화

- **비동기 처리**
  - `asyncio`를 활용한 비동기 스트리밍
  - 메인 스레드 블로킹 방지

---

- feedback model 코드

```py
from typing import List, Dict, Any, AsyncIterator
from datetime import datetime, timedelta
from vertexai import init
from vertexai.preview.generative_models import GenerativeModel
from dotenv import load_dotenv
import os
import asyncio
import traceback

class FeedbackModel:
    def __init__(self):
        load_dotenv()
        os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
        init(project=os.getenv("GOOGLE_CLOUD_PROJECT"), location=os.getenv("VERTEX_AI_LOCATION"))
        self.model = GenerativeModel(os.getenv("VERTEX_MODEL_NAME"))
        # 한글 기준으로 2-3문장에 적절한 토큰 수 설정 (약 100-150자)
        self.max_tokens = 100
        # 프롬프트 템플릿을 환경 변수에서 가져옴
        self.prompt_template = os.getenv("FEEDBACK_PROMPT_TEMPLATE", """
        다음은 사용자의 챌린지 참여 기록입니다. 이를 바탕으로 긍정적이고 격려하는 피드백을 생성해주세요.

        개인 챌린지:
        {personal_challenges}

        단체 챌린지:
        {group_challenges}

        위 기록을 바탕으로, 사용자의 노력을 인정하고 격려하는 피드백을 생성해주세요.
        실패한 챌린지에 대해서는 위로와 함께 다음 기회를 기대한다는 메시지를 포함해주세요.
        이모지를 적절히 사용하여 친근하고 밝은 톤으로 작성해주세요.
        """)

    def _is_within_last_week(self, date_str: str) -> bool:
        """주어진 날짜가 최근 일주일 이내인지 확인"""
        try:
            date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
            one_week_ago = datetime.now() - timedelta(days=7)
            return date >= one_week_ago
        except (ValueError, TypeError):
            return False

    def _format_personal_challenges(self, challenges: List[Dict[str, Any]]) -> str:
        if not challenges:
            return "참여한 개인 챌린지가 없습니다."
        
        formatted = []
        for challenge in challenges:
            status = "성공" if challenge["isSuccess"] else "실패"
            formatted.append(f"- {challenge['title']} ({status})")
        return "\n".join(formatted)

    def _format_group_challenges(self, challenges: List[Dict[str, Any]]) -> str:
        if not challenges:
            return "참여한 단체 챌린지가 없습니다."
        
        formatted = []
        for challenge in challenges:
            # 실천 결과가 있는 챌린지만 필터링
            submissions = challenge.get("submissions", [])
            if not submissions:
                continue

            # 최근 일주일 이내의 제출만 필터링
            recent_submissions = [
                s for s in submissions
                if self._is_within_last_week(s["submittedAt"])
            ]
            
            if not recent_submissions:
                continue

            success_count = sum(1 for s in recent_submissions if s["isSuccess"])
            total_count = len(recent_submissions)
            
            formatted.append(
                f"- {challenge['title']}\n"
                f"  기간: {challenge['startDate']} ~ {challenge['endDate']}\n"
                f"  최근 일주일 성공률: {success_count}/{total_count}"
            )
        return "\n".join(formatted) if formatted else "최근 일주일 동안 참여한 단체 챌린지가 없습니다."

    async def generate_feedback(self, data: Dict[str, Any]) -> AsyncIterator[Dict[str, Any]]:
        try:
            # 입력 데이터 검증
            if not data.get("memberId"):
                yield {
                    "status": 400,
                    "message": "memberId는 필수 항목입니다.",
                    "data": None
                }
                return
            
            if not data.get("personalChallenges") and not data.get("groupChallenges"):
                yield {
                    "status": 400,
                    "message": "최소 1개의 챌린지 데이터가 필요합니다.",
                    "data": None
                }
                return

            # 챌린지 데이터 포맷팅
            personal_challenges = self._format_personal_challenges(data.get("personalChallenges", []))
            group_challenges = self._format_group_challenges(data.get("groupChallenges", []))

            # 프롬프트 생성
            prompt = self.prompt_template.format(
                personal_challenges=personal_challenges,
                group_challenges=group_challenges
            )

            try:
                # Vertex AI를 통한 피드백 생성 (스트리밍 방식 사용)
                response = self.model.generate_content(
                    prompt,
                    generation_config={
                        "temperature": 0.7,
                        "top_p": 1,
                        "top_k": 32,
                        "max_output_tokens": self.max_tokens
                    },
                    stream=True
                )
                
                full_feedback = ""
                # 동기 제너레이터를 비동기적으로 처리
                for chunk in response:
                    if chunk.candidates and chunk.candidates[0].content.parts:
                        chunk_text = chunk.candidates[0].content.parts[0].text
                        if chunk_text.strip():
                            full_feedback += chunk_text
                            yield {
                                "status": 200,
                                "message": "피드백 생성 중",
                                "data": {
                                    "feedback": chunk_text
                                }
                            }
                    # 다른 작업이 실행될 수 있도록 잠시 양보
                    await asyncio.sleep(0)

                # 최종 피드백 저장 요청을 위한 응답
                yield {
                    "status": 200,
                    "message": "피드백 결과 수신 완료",
                    "data": {
                        "feedback": full_feedback
                    }
                }

            except Exception as model_error:
                error_trace = traceback.format_exc()
                print(f"Model Error: {str(model_error)}\nTrace: {error_trace}")
                yield {
                    "status": 500,
                    "message": f"서버 오류로 피드백 결과 저장 실패. 잠시 후 다시 시도해주세요. AI 모델 오류: {str(model_error)}",
                    "data": None
                }

        except Exception as e:
            error_trace = traceback.format_exc()
            print(f"General Error: {str(e)}\nTrace: {error_trace}")
            yield {
                "status": 500,
                "message": f"서버 오류로 피드백 결과 저장 실패. 잠시 후 다시 시도해주세요. AI 모델 오류: {str(model_error)}",
                "data": None
            } 
```