# 오늘 내가 배운 것들(Today I Learned)

- 오늘은 임베딩 고도화와 QdrantDB활용성에 대해 알아보았음.
- 솔직히 ChromaDB를 써도 되긴 하지만 뭔가 서버 상에서 벡터DB를 써보고 싶어서 QdrantDB를 썼음

---

##  테스트 결과

+ test_RAG.py 코드

```python
# LLM_chatbot_free_text_model.py
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_qdrant import Qdrant
from langchain_community.embeddings import SentenceTransformerEmbeddings
from qdrant_client import QdrantClient
from qdrant_client.http.models import VectorParams, Distance
from langchain_google_vertexai import VertexAI
from dotenv import load_dotenv
from langchain.output_parsers import StructuredOutputParser, ResponseSchema
# from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated, Sequence, Optional, Dict, List
import os
import json
import time
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
from langchain_community.document_loaders import TextLoader
from qdrant_client.http.models import Filter, FieldCondition, MatchValue

load_dotenv()

QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = os.getenv("QDRANT_COLLECTION_NAME")

qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
embedding_model = SentenceTransformerEmbeddings(model_name="BAAI/bge-small-en-v1.5")

vectorstore = Qdrant(
client=qdrant_client,
collection_name=COLLECTION_NAME,
embeddings=embedding_model
)

# 거리 측정 방식별 컬렉션 생성
distance_collections = {
"COSINE": "test_cosine",
"EUCLID": "test_euclid",
"DOT": "test_dot",
"MANHATTAN": "test_manhattan"
}

# 각 거리 측정 방식별 컬렉션 생성 및 데이터 로드
for distance, collection_name in distance_collections.items():
try:
    # 기존 컬렉션이 있다면 삭제
    if qdrant_client.collection_exists(collection_name):
        qdrant_client.delete_collection(collection_name)
    
    # 새 컬렉션 생성
    qdrant_client.create_collection(
        collection_name=collection_name,
        vectors_config=VectorParams(size=384, distance=getattr(Distance, distance))
    )
    
    # 문서 로드 및 임베딩
    documents = TextLoader("challenge_docs.txt").load()
    splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=20)
    chunks = splitter.split_documents(documents)
    
    # 각 컬렉션에 문서 추가
    collection_vectorstore = Qdrant(
        client=qdrant_client,
        collection_name=collection_name,
        embeddings=embedding_model
    )
    collection_vectorstore.add_documents(chunks)
    print(f"{distance} 컬렉션 생성 및 데이터 로드 완료")
    
except Exception as e:
    print(f"{distance} 컬렉션 생성 중 오류 발생: {str(e)}")

# 거리 측정 방식별 RAG
distance_retrievers = {
"코사인 유사도 RAG": Qdrant(
    client=qdrant_client,
    collection_name=distance_collections["COSINE"],
    embeddings=embedding_model
).as_retriever(search_kwargs={"k": 5}),

"유클리드 거리 RAG": Qdrant(
    client=qdrant_client,
    collection_name=distance_collections["EUCLID"],
    embeddings=embedding_model
).as_retriever(search_kwargs={"k": 5}),

"내적 RAG": Qdrant(
    client=qdrant_client,
    collection_name=distance_collections["DOT"],
    embeddings=embedding_model
).as_retriever(search_kwargs={"k": 5}),

"맨해튼 거리 RAG": Qdrant(
    client=qdrant_client,
    collection_name=distance_collections["MANHATTAN"],
    embeddings=embedding_model
).as_retriever(search_kwargs={"k": 5})
}

# 기본 RAG
base_retriever = vectorstore.as_retriever(search_kwargs={"k": 5})

# 유사도 임계값 적용 RAG
threshold_retriever = vectorstore.as_retriever(
search_kwargs={
    "k": 5,
    "score_threshold": 0.7
}
)

# MMR 적용 RAG
mmr_retriever = vectorstore.as_retriever(
search_kwargs={
    "k": 5,
    "mmr": True,
    "mmr_threshold": 0.7
}
)

# 메타데이터 필터링 적용 RAG
metadata_retriever = vectorstore.as_retriever(
search_kwargs={
    "k": 5,
    "filter": {
        "category": "환경보호"  # 예시 필터
    }
}
)

# RAG 방식 챌린지 추천을 위한 Output Parser 정의
rag_response_schemas = [
ResponseSchema(name="recommend", description="추천 텍스트를 한 문장으로 출력해줘."),
ResponseSchema(name="challenges", description="추천 챌린지 리스트, 각 항목은 title, description 포함, description은 한 문장으로 요약해주세요.")
]

# LangChain의 StructuredOutputParser를 사용하여 JSON 포맷을 정의
rag_parser = StructuredOutputParser.from_response_schemas(rag_response_schemas)

# JSON 포맷을 이스케이프 처리
escaped_format = rag_parser.get_format_instructions().replace("{", "{{").replace("}", "}}")

# 테스트용 프롬프트 템플릿
test_prompt = PromptTemplate(
input_variables=["context", "query"],
template=f"""
반드시 문서에서 제공된 정보를 기반으로 사용자에게 적절한 친환경 챌린지를 3개 추천해주세요.

문서:
{{context}}

현재 요청:
{{query}}

응답은 반드시 다음 JSON 형식을 따라주세요:
{escaped_format}
"""
)

# 테스트용 LLMChain
test_chain = LLMChain(
llm=VertexAI(model_name="gemini-2.0-flash", temperature=0.5),
prompt=test_prompt
)

def test_rag_performance(query: str, num_runs: int = 3):
"""RAG 성능 테스트 함수

Args:
    query: 테스트할 쿼리
    num_runs: 각 설정별 실행 횟수
"""
results = {}

for name, retriever in distance_retrievers.items():
    total_time = 0
    total_docs = 0
    successful_runs = 0
    
    print(f"\n=== {name} 테스트 ===")
    
    for i in range(num_runs):
        try:
            # 검색 시간 측정
            start_time = time.time()
            docs = retriever.get_relevant_documents(query)
            search_time = time.time() - start_time
            
            # 문서 수 기록
            total_docs += len(docs)
            
            # LLM 응답 생성 시간 측정
            start_time = time.time()
            context = "\n".join([doc.page_content for doc in docs])
            response = test_chain.invoke({
                "context": context,
                "query": query
            })
            llm_time = time.time() - start_time
            
            total_time += (search_time + llm_time)
            successful_runs += 1
            
            print(f"실행 {i+1}:")
            print(f"- 검색된 문서 수: {len(docs)}")
            print(f"- 검색 시간: {search_time:.2f}초")
            print(f"- LLM 응답 시간: {llm_time:.2f}초")
            print(f"- 총 소요 시간: {search_time + llm_time:.2f}초")
            
        except Exception as e:
            print(f"실행 {i+1} 실패: {str(e)}")
    
    if successful_runs > 0:
        avg_time = total_time / successful_runs
        avg_docs = total_docs / successful_runs
        results[name] = {
            "평균 소요 시간": f"{avg_time:.2f}초",
            "평균 검색 문서 수": f"{avg_docs:.1f}개",
            "성공률": f"{(successful_runs/num_runs)*100:.1f}%"
        }

print("\n=== 테스트 결과 요약 ===")
for name, metrics in results.items():
    print(f"\n{name}:")
    for metric, value in metrics.items():
        print(f"- {metric}: {value}")

# 테스트 실행 예시
if __name__ == "__main__":
test_queries = [
    "환경을 위해 분리수거를 잘하고 싶어요",
    "텀블러 사용하는 방법 알려줘",
    "친환경 운송수단 추천해줘",
    "에너지 절약하는 방법이 궁금해"
]

for query in test_queries:
    print(f"\n=== 쿼리: '{query}' 테스트 시작 ===")
    test_rag_performance(query)
    print(f"=== 쿼리: '{query}' 테스트 종료 ===\n")

# LLM 초기화 (VertexAI)
llm = VertexAI(model_name="gemini-2.0-flash", temperature=0.5)

# LLMChain 체인 생성 (retriever는 app_router에서 별도 사용)
qa_chain = LLMChain(
llm=llm,
prompt=custom_prompt
)
```

- 해당 코드를 새로 만들어서 QdrantDB 검색 속도를 비교 해보았다.
- 해당 결과는 아래와 같다.

+ RAG 시스템 성능 테스트 보고서

## 1. 테스트 개요

### 1.1 테스트 목적

- 다양한 거리 측정 방식(Cosine, Euclidean, Dot, Manhattan)을 사용한 RAG 시스템의 성능 비교
- 각 방식별 검색 속도, 정확도, 안정성 평가

### 1.2 테스트 환경

- 임베딩 모델: BAAI/bge-small-en-v1.5
- 벡터 데이터베이스: Qdrant
- LLM: Gemini 2.0 Flash
- 테스트 데이터: 50개의 친환경 챌린지 문단

### 1.3 테스트 쿼리

1. "환경을 위해 분리수거를 잘하고 싶어요"
2. "텀블러 사용하는 방법 알려줘"
3. "친환경 운송수단 추천해줘"
4. "에너지 절약하는 방법이 궁금해"

## 2. 테스트 결과

### 2.1 전체 평균 성능 비교

| **거리 측정 방식** | **평균 검색 시간** | **평균 LLM 응답 시간** | **총 평균 소요 시간** | **성공률** |
| ------------ | ------------ | ---------------- | -------------- | ------- |
| 코사인 유사도      | 0.25초        | 1.92초            | 2.02초          | 100%    |
| 유클리드 거리      | 0.24초        | 1.77초            | 1.94초          | 100%    |
| 내적           | 0.24초        | 1.71초            | 1.91초          | 100%    |
| 맨해튼 거리       | 0.24초        | 1.74초            | 1.93초          | 100%    |

### 2.2 쿼리별 성능 분석

#### 2.2.1 "환경을 위해 분리수거를 잘하고 싶어요"

- **가장 빠른 방식**: 내적 (1.92초)
- **가장 느린 방식**: 코사인 유사도 (2.33초)
- **검색 문서 수**: 모든 방식 5개

#### 2.2.2 "텀블러 사용하는 방법 알려줘"

- **가장 빠른 방식**: 유클리드 거리 (1.82초)
- **가장 느린 방식**: 코사인 유사도 (2.10초)
- **검색 문서 수**: 모든 방식 5개

#### 2.2.3 "친환경 운송수단 추천해줘"

- **가장 빠른 방식**: 맨해튼 거리 (1.93초)
- **가장 느린 방식**: 코사인 유사도 (2.01초)
- **검색 문서 수**: 모든 방식 5개

#### 2.2.4 "에너지 절약하는 방법이 궁금해"

- **가장 빠른 방식**: 코사인 유사도/내적 (1.65초)
- **가장 느린 방식**: 유클리드 거리 (1.80초)
- **검색 문서 수**: 모든 방식 5개

## 3. 결론 및 권장사항

### 3.1 성능 비교

- **가장 빠른 검색 속도**: 내적 (Dot Product)
- **가장 안정적인 성능**: 맨해튼 거리
- **가장 높은 성공률**: 모든 방식 100%

### 3.2 사용 사례별 권장 방식

1. **일반적인 텍스트 검색**: 내적 (Dot Product)
    - 가장 빠른 평균 응답 시간
    - 안정적인 성능
2. **정확한 거리 기반 검색**: 유클리드 거리
    - 검색 시간이 일관적
    - LLM 응답 시간이 안정적
3. **빠른 검색이 필요한 경우**: 내적
    - 가장 빠른 전체 처리 시간
4. **이상치가 많은 데이터**: 맨해튼 거리
    - 안정적인 성능
    - 일관된 응답 시간

### 3.3 개선 사항

1. **성능 최적화**
    - 배치 처리 크기 조정
    - 캐싱 메커니즘 도입
2. **에러 처리**
    - 타임아웃 설정 최적화
    - 재시도 메커니즘 구현
3. **모니터링**
    - 실시간 성능 모니터링
    - 자동 알림 시스템

---

*참고: 모든 테스트는 3회 반복 실행 후 평균값을 사용하였음*

- 해당 테스트에서 사용된 4가지 거리 측정 방식은 각각 벡터의 크기와 방향을 다르게 고려한다.
- 그럼 고려하는 크기와 방향은 무엇을 의미할까.
- 방향은 단어의 “의미적 유사성”을 판별하고 크기는 빈번하게 등장하는 정도(중요도)를 나타낸다.

- 그렇다면 Qdrant에서 코사인 유사도를 쓰는 이유는 무엇일까 즉 방향만 신경쓰는 이유는 뭘까?
    - 이유는 하나의 정보로 다양한 매칭이 필요하기 때문이다.
    - 아래는 각 기법에 따른 활용 방식이다.
        1. 정확한 매칭이 필요하다 -> 유클리드 거리
        2. 관련된 다양한 정보 -> 코사인
        3. 특정 단어와의 관계를 중요 -> 내적

---

- 그러면 임계값 
    1. 데이터 확장 방향:
        - random_sentences에 더 다양한 환경 보호 활동 추가
        - 각 문장에 메타데이터 추가 (예: 난이도, 카테고리, 소요 시간 등)
        - 실제 사용자 피드백과 결과를 추가
        - 다양한 상황과 맥락의 문장 추가
    2. Qdrant의 추가 기능:
        - 페이로드 필터링 (메타데이터 기반 필터링)
        - 지리적 위치 기반 검색
        - 실시간 업데이트와 동기화
        - 분산 검색 지원