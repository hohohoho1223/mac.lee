# 오늘 내가 배운 것들(Today I Learned)

- 금요일 새벽에 임베딩 모델 성능 비교를 하였다.

---

- Qdrant를 "메모리 모드”로 진행

### **“메모리 모드” Qdrant로 실험해도 신뢰 가능한가?**

| **항목**     | **설명**                                                                            |
| ---------- | --------------------------------------------------------------------------------- |
| **정확도**    | Qdrant에서 사용하는 벡터 유사도(HNSW + cosine/dot 등)는 동일하게 동작함.<br/> 즉, 검색 결과는 **서버 모드와 같음** |
| **영속성 없음** | 세션/런타임이 끊기면 저장된 벡터도 모두 사라짐 (→ **“임시 실험”용**)                                       |
| **결론**     | **모델 성능 비교**나 **검색 품질 평가** 용도로는 메모리 모드면 충분함                                       |

- 실행 코드

```python
import time
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Qdrant
from langchain.schema import Document
from sentence_transformers.util import cos_sim
from sentence_transformers import SentenceTransformer
import pandas as pd

# 모델 리스트
embedding_models = [
    "all-MiniLM-L6-v2",
    "multi-qa-MiniLM-L6-cos-v1",
    "multi-qa-mpnet-base-dot-v1",
    "intfloat/e5-base-v2",
    "BAAI/bge-small-en-v1.5"
]

# 테스트 문서
texts = [
    "플라스틱을 줄이기 위해 텀블러를 사용하자.",
    "걷기나 자전거를 통해 탄소중립을 실천하자.",
    "에너지 절약을 위해 전등을 끄는 습관을 들이자.",
    "제로웨이스트 실천을 위해 다회용기를 사용하자.",
    "비건 식단을 통해 기후변화에 대응하자.",
]

# 사용자 질문
query = "앉아서 일하는 사무직이 환경을 위해 실천할 수 있는 챌린지는?"

# 결과 저장
results = []

# 실험 시작
for model_name in embedding_models:
    print(f"\n[모델] {model_name}")
    
    # 1. 임베딩 객체 생성 + 속도 측정
    start_time = time.time()
    embeddings = HuggingFaceEmbeddings(model_name=model_name)
    raw_model = SentenceTransformer(model_name)
    query_vec = raw_model.encode(query, convert_to_tensor=True)
    encode_time = time.time() - start_time

    # 2. Qdrant 벡터스토어 (메모리)
    qdrant = Qdrant.from_documents(
        documents=[Document(page_content=t) for t in texts],
        embedding=embeddings,
        collection_name="test-collection",
        location=":memory:"
    )

    # 3. RAG 검색
    retriever = qdrant.as_retriever(search_kwargs={"k": 3})
    docs = retriever.invoke(query)

    # 4. 유사도 계산
    sim_score = 0
    for doc in docs:
        doc_vec = raw_model.encode(doc.page_content, convert_to_tensor=True)
        sim_score += float(cos_sim(query_vec, doc_vec)[0][0])
    avg_sim = sim_score / len(docs)

    # 5. 결과 저장
    results.append({
        "모델명": model_name,
        "평균 유사도": round(avg_sim, 4),
        "임베딩 속도 (s)": round(encode_time, 4),
        "상대 성능": ""
    })

# 결과 정리
df = pd.DataFrame(results)
df["상대 성능"] = (
    df["평균 유사도"] / df["임베딩 속도 (s)"]
).apply(lambda x: round(x, 2))
df = df.sort_values(by="상대 성능", ascending=False)

import IPython
IPython.display.display(df)
```

###  **실험 결과 요약**

<img width="850" alt="Image" src="https://github.com/user-attachments/assets/fae13586-7f2a-43e4-9b40-f88e49767f79" />

| **모델명**                    | **평균 유사도** | **속도** | **상대 성능** | **종합 평가**           |
| -------------------------- | ---------- | ------ | --------- | ------------------- |
| multi-qa-mpnet-base-dot-v1 | 0.8174     | 5.45s  | ⭐️ 0.15   | 균형형, 정밀도 우수         |
| BAAI/bge-small-en-v1.5     | 0.8659     | 5.94s  | ⭐️ 0.15   | **정확도 최우수 + 속도 양호** |
| e5-base-v2                 | **0.8858** | 6.49s  | ⭐️ 0.14   | 정확도 최고, 속도 약간 느림    |
| multi-qa-MiniLM-L6-cos-v1  | 0.7737     | 6.06s  | 0.13      | 보통                  |
| all-MiniLM-L6-v2           | 0.6824     | 7.00s  | 0.10      | ❌ 정확도/속도 모두 약함      |

### **종합 추천**

| **목적**                     | **추천 모델**                       |
| -------------------------- | ------------------------------- |
| **정확도 최우선 (챌린지 추천 품질 중요)** | e5-base-v2 or bge-small-en-v1.5 |
| **정확도 + 응답속도 균형 (서비스 대응)** | bge-small-en-v1.5 -> **강력 추천**  |
| **MVP용 속도 우선**             | MiniLM 계열은 오히려 느리고 부정확해서 비추천    |

## 결론
- "bge-small-en-v1.5" 모델 사용