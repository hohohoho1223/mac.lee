# 오늘 내가 배운 것들(Today I Learned)

- 모의 면접 공부를 추가로 더 하였고, 브랜치(feat) 수정 작업
- 챗봇 과부화 테스트 진행 및 서빙 모델 고도화 방향 논의

---

## 모믜 면접 공부

- 커널과 임계영역의 차이

- 커널: OS(운영체제) 자체가 돌아가는 영역 -> 모든 시스템 자원에 접근이 가능하다
- 임계영역: 프로세스 나 여러 스레드가 “공유자원" 애 접근하는 코드 영역 -> 이 코드에 동시에 들어가면 데이너 충돌이 일어난다.
- interrupt이란? : 중단이다. -> 예외상황을 최우선으로 처리하려고 함(timely)
    - 중단 하고 백업을 하고 이러한 활동을 -> “interrupt handler"이라고 함

---

## 과부화 테스트

- hey를 homebrew에서 다운받아서 로컬에서 실행

```bash
hey -n 1000 -c 50 -m POST -H "Content-Type: application/json" -d '{"message":"환경을 위해 분리수거를 잘하고 싶어요"}' http://lo`alhost:9000/ai/chatbot/recommendation/free-text
```

- 실행 결과 100% 정확도 및 응답속도가 너무 잘 나와서 로컬환경에서 돌린 점이 원인이지 않을까 싶다.
- 추후 GPU서버로 배포와 같은 실제 환경에서 실행해야 신뢰성이 높은 수치가 나올 것으로 예상된다.

---

## 서빙 모델 고도화 방향

- 챗봇 API request body 수정 사항이 생겼다.

- 아래는 기존 free-text의 요청 바디이다.

```JSON
{
"sessionId": "string",
"location": "string",
"workType": "string",
"message": "string"
}
```

- 하지만, free-text에서는 사용자의 쿼리 요청(message)만 활용하기에 `location` & `workType` 는 삭제

```JSON
{
"sessionId": "string",
"message": "string"
}
```

### V2 구현 방향

- 현재는 V1배포 기간이다.(스프린트4)

1. `sessionId`는 추후 V2배포때 적용할 예정이다.
2. `recommends` 를 활용하여 사용자간 챗봇 대화를 가능하게 설계

```JSON
    {
  "status": 200,
  "message": "사용자 기본 정보 키워드 선택을 기반으로 챌린지를 추천합니다.",
  "data": {
    "recommend": "string",
    "challenges": [
      {
        "title": "string",
        "description": "string"
      },
      {
        "title": "string",
        "description": "string"
      },
      {
        "title": "string",
        "description": "string"
      }
    ]
  }
}
```

3. Mistral-7B 모델 원본으로 업로드 예정(약 14GB)
    - GCP 환경에서 L4 이므로 저장공간 및 메모리 제약이 크지 않아서 원본 모델 업로드 예정
    - 고도화 방향으로 FP32 → FP16 으로 최적화 + 양자화 모델 구현으로 계획중(메모리 절약 + 성능 개선)
    - 양자화 또한 8bit, 4bit 각 버전에 따른 답변 테스트 진행 예정

4. Colab L4 환경에서 FP32, FP16 성능 테스트 비교 구상중

- 예상 결과: GPU가 FP16 연산을 더 빠르게 처리할 것이며 정밀도는 큰 차이가 없을 것이다.
- 하지만 마찬가지로 추후 GPU서버로 배포와 같은 실제 환경에서 실행해야 신뢰성이 높은 수치가 나올 것으로 예상된다.

- **파인튜닝 조합 예시**
    
    |  | 장점 | 단점 |
    | --- | --- | --- |
    | **LoRA + FP16** | • GPU에서 최적의 성능
    • 메모리 효율적
    • 학습 속도 빠름 | - FP16의 정밀도 손실 |
    | **QLoRA + FP16** | • 최고의 정밀도
    • GPU에서 안정적인 성능 | - 메모리 사용량이 가장 많음 |

### 논의 이슈

- 챗봇 답변에 스트리밍 형식으로 “SSE” 방식을 도입 할 것인가?
    - “챗봇은 사용자가 답변을 요청할 때만 작동하는 방식이라서 실시간 데이터를 전송하는 방식은 맞지 않다다“ 의견이 있음
- 기능을 다 갖춰진 다음 테스트를 진행해야 하는지?(현재는 기능 추가 진행중)