# 오늘 내가 배운 것들(Today I Learned)

-  파싱(Parsing)에 대해서 수정을 하였다.(chatbot_langchain_rag_chain.py)
-  우선 기존 파싱은 LLM모델한테 JSON 형식 답변을 유도하게끔 "설득” 시키는 과정이다.

````python
# chatbot_langchain_rag_chain.py
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_qdrant import Qdrant
from langchain_community.embeddings import SentenceTransformerEmbeddings
from qdrant_client import QdrantClient
from langchain_google_vertexai import VertexAI
from dotenv import load_dotenv
import os

load_dotenv()

QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = "my-new-collection"

# LLM 초기화 (VertexAI)
llm = VertexAI(model_name="gemini-2.0-flash", temperature=0.4)

qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
embedding_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

vectorstore = Qdrant(
    client=qdrant_client,
    collection_name=COLLECTION_NAME,
    embeddings=embedding_model
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) # 사용자 질문으로 부터 가장 유사한 5개 문서 검색

# PromptTemplate
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
다음 문서를 참고하여 사용자 요청에 맞는 친환경 챌린지를 JSON 형식으로 3개 추천해주세요.

문서:
{context}

요청:
{question}

아래 포맷을 반드시 지켜 JSON만 출력해주세요.  
추가 설명 없이 순수 JSON만 반환해야 합니다.
절대로 ```json 또는 ``` 와 같은 마크다운 코드블록을 사용하지 마세요.

{{
  "status": 200,
  "message": "성공!",
  "data": {{
    "recommand": "설명 텍스트",
    "challenges": [
      {{"title": "챌린지 이름", "description": "설명"}},
      {{"title": "챌린지 이름", "description": "설명"}},
      {{"title": "챌린지 이름", "description": "설명"}}
    ]
  }}
}}
"""
)

# RetrievalQA 체인 생성(RAG)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": custom_prompt}
)
````

        - 하지만 이는 지금 MVP 초기 개발단계에선 외부 API를 불러오므로 답변의 퀄리티가 좋아지지만
        - 나중 자체모델(Mistral 7B)을 사용했을 시 불안정한 출력이 우려되었다.(포맨 불안전성)
        - 따라서 `Output Parser` 의 필요성을 느꼈다.

| **구분**                             | **이유**                                                                       |
| ---------------------------------- | ---------------------------------------------------------------------------- |
| **LLM은 항상 문자열을 출력함**               | LLM은 “JSON처럼 보이는 텍스트”를 출력할 뿐, 진짜 JSON 객체를 반환하진 않음                            |
| **자체 모델은 출력 형식이 불안정할 수 있음**        | Gemini나 GPT 계열은 형식 유지력이 높은 반면, Mistral-7B 같은 오픈모델은 출력이 들쭉날쭉할 수 있음            |
| **챗봇은 입력이 매우 다양함**                 | 사용자가 어떤 문장으로 요청할지 예측할 수 없기 때문에, LLM 출력도 다양해지고 이는 파싱에 영향을 준다                  |
| **후처리는 brittle(깨지기 쉬움)**           | 예: json ...  같이 마크다운이 붙으면 json.loads()가 실패할 수 있음                             |
| **Output Parser는 format 기대치를 명시함** | PromptTemplate에 구조를 삽입하고, LLM에게 _정확한 포맷 요구_를 할 수 있음 파싱 실패 시 fallback 처리도 가능함 |

        + 아래 코드는  `Output Parser` 를 적용한 코드이다. 

```python
# chatbot_langchain_rag_chain.py
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_qdrant import Qdrant
from langchain_community.embeddings import SentenceTransformerEmbeddings
from qdrant_client import QdrantClient
from langchain_google_vertexai import VertexAI
from dotenv import load_dotenv
import os
from langchain.output_parsers import StructuredOutputParser, ResponseSchema

load_dotenv()

QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")
COLLECTION_NAME = "my-new-collection"

# LLM 초기화 (VertexAI)
llm = VertexAI(model_name="gemini-2.0-flash", temperature=0.4)

qdrant_client = QdrantClient(url=QDRANT_URL, api_key=QDRANT_API_KEY)
embedding_model = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

vectorstore = Qdrant(
    client=qdrant_client,
    collection_name=COLLECTION_NAME,
    embeddings=embedding_model
)

retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) # 사용자 질문으로 부터 가장 유사한 5개 문서 검색

# Output Parser 정의
response_schemas = [
    ResponseSchema(name="recommand", description="추천 설명 텍스트"),
    ResponseSchema(name="challenges", description="추천 챌린지 리스트, 각 항목은 title, description 포함")
]
parser = StructuredOutputParser.from_response_schemas(response_schemas)

# PromptTemplate 정의
custom_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=f"""
다음 문서를 참고하여 사용자 요청에 맞는 친환경 챌린지를 아래 JSON 형식에 맞춰 3개 추천해주세요.

문서:
{{context}}

요청:
{{question}}

JSON 포맷:
{parser.get_format_instructions()}

응답은 반드시 위 형식 그대로, 마크다운 코드블럭 없이 순수 JSON만 반환하세요.
"""
)

# RetrievalQA 체인 생성
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": custom_prompt}
)
```

- `status` & `message` 부분의 스키마는 요청의 경우에 따라 예외처리를 하여 조건부 응답을 만들어야 하므로  "status" 필드를 아예 LLM이 출력하지 않고, FastAPI에서만 넣게 설계하였다

> “status” & “message”는 LLM이 생성할 내용이 아니라, API서버(FastAPI)가 책임지고 넣어야 하는 필드임

- `status` & `message` 는 HTTP 응답의 '메타 데이터' 역할을 한다.
    - API 호출이 정상 인지 실패인지 출력
    - 응답 구조를 표준화 하는데 사용하기도 함

| **항목**  | **역할**               | **누가 넣어야 하나** |
| ------- | -------------------- | ------------- |
| status  | HTTP 응답 상태 코드 표현     | FastAPI       |
| message | 상태 요약 메시지            | FastAPI       |
| data    | 실제 데이터 (LLM 생성 결과 등) | LLM or DB     |

-  즉 LLM모델은 콘텐츠 생성에만 집중하고, FastAPI가 직접 return 응답 형태로 넣는 게 안정성과 일관성 측면에서 좋다
-  여기서 예외처리 과정에서 의문이 들었다.
    - 근데 이제 최종 JSON구조로 감싼다고 하는데

```python
if not req.user_message or len(req.user_message.strip()) < 5:
        raise HTTPException(status_code=422, detail="user_message는 최소 5자 이상의 문자열이어야 합니다.")
    message_lower = req.user_message.lower()
    if not any(k in req.user_message for k in ENV_KEYWORDS) or any(b in message_lower for b in BAD_WORDS):
        return {
            "status": 403,
            "message": "저는 친환경 챌린지를 추천해드리는 Leafresh 챗봇이에요! 환경 관련 질문을 해주시면 더 잘 도와드릴 수 있어요.",
            "data": None
        }
```

- 이런 경우에는 실제 HTTP 통신이 200으로 뜨지만 "status"는 오류를 표현한건데 그럼 최종 출력에는 내가 "status" 지정한것 + FastAPI가 JSON구조로 감싸서 만든 "status"이렇게 총 2개가 출력되지 않나 우려됨!

    ### 답변 

    > **FastAPI의 return {...} 안의 "status"는 여러분이 만든 API 응답의 JSON 필드이고**,

    > **HTTP 통신의 status code (HTTP 200, 403, 422)와는 전혀 별개임.**

    > 따라서 *“status가 두 번 표현된다”* 같은 충돌은 발생하지 않는다.

- 그러니까 만약 사용자가 챌린치 추천 요청을 바르게 한 상태이면

```python
from chatbot_langchain_rag_chain import parser
      parsed = parser.parse(rag_result["result"])
      return {
          "status": 200,
          "message": "성공!",
          "data": parsed
      }
```

- 이렇게 return을 "내가 status 와 message 키값을 미리 셋팅한 틀 + `parsed` 한 JSON 스키마" 이므로 (몰론 HTTP 통신의 status code는 200)
- 예외 처리를 한 경우도 return값을

```python
return {
            "status": 403,
            "message": "저는 친환경 챌린지를 추천해드리는 Leafresh 챗봇이에요! 환경 관련 질문을 해주시면 더 잘 도와드릴 수 있어요.",
            "data": None
        }
```

- 이렇게 설정하였기에 내가 작성한 “status”는 HTTP 통신의 status code와는 별개로(몰론 통신이 되었으니 status code는 200으로 떴겠지) 403을 반환한다 즉 중복 되는 일은 발생하지 않는다!

+ 이렇게 프롬프트와 파서 분리를 하였다.

```python
# chatbot_app_router.py
from chatbot_langchain_rag_chain import base_prompt, base_parser
```

```python
# chatbot_langchain_rag_chain.py
# base_parser 및 base_prompt 추가
base_response_schemas = [
    ResponseSchema(name="recommand", description="추천 설명 텍스트"),
    ResponseSchema(name="challenges", description="추천 챌린지 리스트, 각 항목은 title, description 포함")
]
base_parser = StructuredOutputParser.from_response_schemas(base_response_schemas)

base_prompt = PromptTemplate(
    input_variables=["location", "work_type", "category"],
    template=f"""
{{location}} 환경에 있는 {{work_type}} 사용자가 {{category}}를 실천할 때,
절대적으로 환경에 도움이 되는 챌린지를 아래 JSON 형식으로 3가지 추천해주세요.

JSON 포맷:
{base_parser.get_format_instructions()}

응답은 반드시 위 JSON 형식 그대로, 마크다운 없이 순수 JSON만 출력하세요.
"""
)
```

-  근데 500 에러가 났다….뭐가 문제일까???