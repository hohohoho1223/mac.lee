# 오늘 내가 배운 것들(Today I Learned)

- 오늘은 아래 두가지 항목에 대해 진행하였다.

-  텍스트 모델의 간이 서버 유무 확인
    - 허깅 페이스(서버 있는 경우)
    - FastAPI(서버 없는 경우)
- 로컬과 GPU서버간 Fastapi 통신 어떻게 할건지

---

-  내 로컬(M1 PRO) 환경에서는 모델 못 돌리니까 GPU 서버에서 돌리고 싶음
- 이때, GCP의 GPU 인스턴스에 Mistral-7B 모델을 올리고, 로컬에서 FastAPI로 통신하며 테스트를 하고자 함
    - 우선 Hugging Face 모델은 서버를 제공하지 않음 -> transformer 라이브러리 사용
    - vLLM은 원래 A100 이상 환경에서 최적화됨...코랩에선 힘들듯

##  방법

---

- GCP에 모델 올려서 API 서버를 만든다
- 로컬에서 요청을 날려 테스트한다

```bash
┌──────────────┐            HTTP            ┌─────────────────────────────┐
│   Mac (로컬) │ ────────────────────────▶ │ GCP GPU 서버 (FastAPI + 모델) │
│  requests, curl 등 │                     │ transformers + Mistral 7B    │
└──────────────┘                           └─────────────────────────────┘
```

| **단계**           | **설명**                            |
| ---------------- | --------------------------------- |
| GCP에 GPU 인스턴스 생성 | T4 / A100 등                       |
| 모델 설치            | transformers, bitsandbytes 등      |
| FastAPI 서버 구성    | Mistral 모델 로드 후 POST /generate 구현 |
| 포트 8000 방화벽 허용   | 외부 접근 가능하도록 설정                    |
| 로컬에서 요청 테스트      | curl, requests, 부하테스트 툴 등         |

- leafresh 프로젝트의 GPU서버 인스턴 

```bash
gcloud compute ssh [leafresh-gce-ai-chatbot](https://console.cloud.google.com/compute/instancesDetail/zones/asia-northeast3-b/instances/leafresh-gce-ai-chatbot?cloudshell=true&inv=1&invt=AbxWng&project=leafresh-gpu1-459708) \
--project=leafresh-gpu1-459708 \
--zone=asia-northeast3-b
```

> GCP에 있는 GPU VM 인스턴스(leafresh-gce-ai-chatbot)에 SSH로 접속한 것 

```bash
Host leafresh-gce-ai
  HostName 35.216.22.118            # GCP 외부 IP
  User iwonho                       # SSH 접속할 사용자 이름
  IdentityFile ~/.ssh/google_compute_engine   # SSH 키 경로
```

## **.zshrc란** 뭘까**?**

- zsh터미널 셀 (z shell)의 개인 설정 파일임

    ###  위치

```bash
~/.zshrc
```

- 터미널을 켤 때마다 자동 실행되는 **초기 설정 스크립트**
- 여기에 PATH, alias, env 설정, pyenv, conda init 등 작성됨
