# 오늘 내가 배운 것들(Today I Learned)

- 현재 여러 요청에 대한 메모리 충돌 오류를 어떻게 해결 할 것 인지 모색중이다.
- 아래는 현재 동작 흐름 요약이다.

```plaintext
1. 클라이언트 요청 → FastAPI 핸들러 진입
2. 핸들러에서 event_generator() 호출
3. event_generator() 내부에서 streamer 생성
4. model.generate(..., streamer=streamer) → 스레드로 실행
5. streamer 가 토큰을 큐에 넣음 → yield로 SSE 전송
```

- 근데 이 구조에서 문제가 되는 이유는:

1. shared_model
    - 모델 인스턴스를 하나로 공유
    - 모든 요청이 하나의 모델에 동시에 접근
2. threading.Thread(target=model.generate)
    - 동기 함수 실행을 백그라운드에서 병렬 실행
    - Pytorch는 GPU 연산을 스레드 간 불안정 함 
3. TextIteratorStreamer()
    - 요청마다 새로 생성됨
    - 내부 큐는 스레드간 분리되지만, 공유된 모델의 출력이 충돌 가능

### 뭔소리?

> streamer 자체는 요청마다 새로 만들어 지기에 토큰 큐는 별개로 생성됨 -> 이건 문제아님
> 하지만 model.generate()는 모든 요청이 하나의 모델을 사용하는 구조 -> 하나의 모델이 여러 스레드에서 동시에 사용-> 이게 문제

| **요청**  | **streamer 인스턴스**  | **model.generate** | **실행 방식**      |
| ------- | ------------------ | ------------------ | -------------- |
| 클라이언트 A | streamer_A (신규 생성) | 스레드 1번 실행          | 백그라운드 스레드에서 실행 |
| 클라이언트 B | streamer_B (신규 생성) | 스레드 2번 실행          | 또 다른 스레드에서 실행  |

- 즉, 각각 양동이(TextIteratorStreamer)를 들고 물 받으려고 하는데 수도꼭지(shared_model)가 하나라서 터지는 상황ㅋㅋ

---

### 아 그래서 어떻게 처리하면 좋을까?

1. **세션** 단위 **모델 인스턴스 분리**

    - 아무리 4비트 양자화(15GB -> 4GB) 했더라도 VRAM이 24GB 환경에는 역부족임

2. **큐 기반 요청 풀 (Request Queue + 응답 알림)** or **세션 기반 순번 제한** ??
    - 근데 이것들 결국 선착순 처리 이잖아?
    - 둘의 차이점은 "사용자 경험을 제어할 수 있는가?" 이다.

| **항목**  | **단순 직렬화**        | **큐 기반 요청 풀**        |
| ------- | ------------------------ | --------------------------- |
| 처리 순서   | 선착순                      | 선착순                         |
| 제어 가능성  | 없음 (사용자 입장에선 대기 상태 불명) | 있음 (서버가 “대기 중” 메시지 전송 가능) |
| 사용자 피드백 | 없음                       | 가능 (진행률, 대기중 안내 등)          |
| 확장성     | 낮음                       | 중간에서 분산 처리, 재시도 로직 넣을 수 있음  |
| 구조 복잡도  | 단순                       | 약간 복잡하지만 유연                 |

- 근데 나는 결국엔 각 sessionId 사용자가 “기다려야 하는 상황"인 것이 사용자 경험(UX) 측면에서 아쉽다고 생각이 들었다. (내가 왜 기다려야 하지?)

3. vLLM 도입 
4. LoRA adapter 분리
5. 추론 전용 서버 여러 개 운영 (수평 확장) -> 이건 뭐 비싸서 패스