# 오늘 내가 배운 것들(Today I Learned)

- vLLM에 대해 찾아보다가 KV-cache에 대해 알아보았다.

---

### KV-cache란?

> **Key-Value Cache의 줄임말**

- 예를들어 LLM이 답변을 "The cat on the table" 이라고 해야한다고 가정해보자.
- 이떄, The를 출력하고 cat을 출력할때, “The”에 대한 쿼리를 Key로 벡터 임베딩을 하여 key/value 캐시에 저장한다.
- 캐시에 저장된 key 벡터를 유사도 검색으로 다음 단어를 예측하므로 계산량을 줄일 수 있다.
- 이떄, 유사도 검색은 "내적"을 사용한다.

### 왜 내적을 사용하는가?

- 내적은 Cosine 정규화 보다 계산이 빠르고 GPU에 최적화 되어있다.
- 대신, 내적에 **√dₖ로 정규화**를 적용해 폭발적인 값 증가를 막는다.

###  내적 VS 코사인 유사도

| **항목** | **내적 (Dot Product)** | **코사인 유사도**      |
| ------ | -------------------- | ---------------- |
| 반영 요소  | 크기 + 방향              | 방향만              |
| 정규화    | 없음                   | 있음 → 길이 무시       |
| 예시     | Attention, 선형 연산     | 문장 유사도 비교, RAG 등 |
| 장점     | 빠름, GPU 최적화          | 방향성만 평가 가능       |
| 단점     | 크기 영향 받음             | 계산량 ↑            |

- 내적 계산의 “입력 값”(Key/Value 벡터)을 다시 계산하지 않도록 캐싱함으로써 전체 연산량을 줄이기 위한 최적화이다.
- 정리하자면 K-V cache는 “벡터로 변환하는 과정” 즉, 각 토큰을 K/V 벡터로 선형 변환하는 연산을 생략해준다!