# 오늘 내가 배운 것들(Today I Learned)

- 오늘도 이어서 SSE 통신 작업을 하였다.(SSE_통신작업_3)

---

- 어제 챗봇 모델 답변 형식에서 봤듯이, 아래와 같이 반환값에 문자열이 아닌 “\" 기호가 같이 출력되는 것을 확인

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/89756103-8872-4CBC-9E71-D220F4AD6FCA_2/2KsBkjj0NeznLTYVe1vxiRfs6TcWVtSdPoWhcw4fXlYz/Image.png)

- 추가 클랜징 로직을 넣어 답변 수정을 하였음

```python
# 추가 클랜징: 따옴표, 괄호, 쉼표 등 제거
                cleaned_text = re.sub(r'["\']', '', cleaned_text)
                cleaned_text = re.sub(r'[\[\]{}]', '', cleaned_text)
                cleaned_text = re.sub(r',\s*$', '', cleaned_text)
                cleaned_text = cleaned_text.strip()
```

- 결과:

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/5C2FA603-7DC3-41FB-9966-96D8CD1751F7_2/VMCd1NaPuLABb3KKVP4MjISyZm14wTISnjI3gJUOQUgz/Image.png)

- 순수 문자열만 잘 출력된다.

---

- `uvicorn main:app --host 0.0.0.0 --port 8000` → `nohup uvicorn main:app --host 0.0.0.0 --port 8000 --reload --env-file .env > uvicorn.log 2>&1 &` 으로 서버를 실행

  - SSH 연결이 끊어지거나 터미널을 닫아도 서버는 계속 동작하게 끔
  - 표준 출력(stdout)과 에러 출력(stderr)을 모두 uvicorn.log 파일에 저장해서 문제 발생 시 디버깅에 용이

---

```plaintext
ERROR:    Exception in ASGI application
  + Exception Group Traceback (most recent call last):
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py", line 412, in run_asgi
  |     result = await app(  # type: ignore[func-returns-value]
  |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 84, in __call__
  |     return await self.app(scope, receive, send)
  |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/fastapi/applications.py", line 1054, in __call__
  |     await super().__call__(scope, receive, send)
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/applications.py", line 123, in __call__
  |     await self.middleware_stack(scope, receive, send)
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
  |     raise exc
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
  |     await self.app(scope, receive, _send)
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/middleware/cors.py", line 83, in __call__
  |     await self.app(scope, receive, send)
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 62, in __call__
  |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py", line 64, in wrapped_app
  |     raise exc
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
  |     await app(scope, receive, sender)
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/routing.py", line 758, in __call__
  |     await self.middleware_stack(scope, receive, send)
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/routing.py", line 778, in app
  |     await route.handle(scope, receive, send)
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/routing.py", line 299, in handle
  |     await self.app(scope, receive, send)
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/routing.py", line 79, in app
  |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py", line 64, in wrapped_app
  |     raise exc
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
  |     await app(scope, receive, sender)
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/starlette/routing.py", line 77, in app
  |     await response(scope, receive, send)
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/sse_starlette/sse.py", line 233, in __call__
  |     async with anyio.create_task_group() as task_group:
  |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py", line 772, in __aexit__
  |     raise BaseExceptionGroup(
  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)
  +-+---------------- 1 ----------------
    | Traceback (most recent call last):
    |   File "/home/ubuntu/15-Leafresh-AI/Text/LLM/router/chatbot_router.py", line 133, in event_generator
    |     raise ValueError("파싱된 데이터에 유효한 'data' 키가 없습니다.")
    | ValueError: 파싱된 데이터에 유효한 'data' 키가 없습니다.
    | 
    | During handling of the above exception, another exception occurred:
    | 
    | Traceback (most recent call last):
    |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/sse_starlette/sse.py", line 236, in wrap
    |     await func()
    |   File "/home/ubuntu/.venv/lib/python3.12/site-packages/sse_starlette/sse.py", line 221, in stream_response
    |     async for data in self.body_iterator:
    |   File "/home/ubuntu/15-Leafresh-AI/Text/LLM/router/chatbot_router.py", line 156, in event_generator
    |     raise HTTPException(
    | fastapi.exceptions.HTTPException: 500: {'status': 500, 'message': "파싱 실패: 파싱된 데이터에 유효한 'data' 키가 없습니다.", 'data': None}
```

- 토큰이 생성되다가 중간에 파싱 에러가 발생(모델이 JSON을 생성하다가 중간에 끊겼었음)
  - 이유? 간단했다ㅋ
        1. `max_new_tokens=128`로 설정되어 있어서:
            - 모델이 128개 토큰만 생성하고 중단됨
            - JSON이 완성되기 전에 생성이 끊김
            - 그래서 파싱이 실패했던 거
        1. 그래서 `max_new_tokens=1024`로 늘림:
            - 충분한 토큰 수를 확보
            - JSON이 완성될 때까지 생성 가능
            - 파싱도 정상적으로 됨
- 결과:

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/9FBB3B6F-146B-47B4-B2F7-8047A723264D_2/kly2zikg75KAEvctiuqZiCxSnpnUe8jgySpMdK6hm7Uz/Image.png)

- 잘된다잉ㅋ
- 아 그리고 아래와 같이 모델에 대한 설정도 있다.

```python
generation_kwargs = dict(
            inputs,
            streamer=streamer,
            max_new_tokens=1024,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
```

- 각 설정의 역할을 설명하면:
    1. inputs: 모델에 입력할 텐서들
        - input_ids: 토큰화된 입력 텍스트
        - attention_mask: 패딩 토큰 무시를 위한 마스크
    2. streamer: SSE 스트리밍을 위한 설정
        - 토큰을 하나씩 실시간으로 전송
        - 사용자에게 점진적으로 응답 표시
    3. max_new_tokens: 생성할 최대 토큰 수
        - JSON이 완성되도록 1024로 설정
        - 이전에 128로 설정했다가 JSON이 중간에 끊겼던 문제 해결
    4. temperature: 생성 다양성 조절
        - 0.7은 적당한 다양성
        - 0.0에 가까울수록 결정적(deterministic)
        - 1.0에 가까울수록 더 창의적
    5. do_sample: 확률적 샘플링
        - temperature와 함께 사용
            - 만약, `do_sample =False` 일 때:
                1. temperature 값은 무시됨
                2. 모델이 항상 가장 높은 확률의 토큰을 선택
                3. 확률 분포를 조절할 기회가 없음
        - 다양한 응답 생성 가능
    6. pad_token_id: 패딩 토큰 설정
        - Mistral 모델은 EOS 토큰을 패딩으로 사용
        - 배치 처리 시 필요
- 이 설정들이 모델의 응답 생성 방식을 결정한다고 한다~

## 여기서 잠깐, Mistral 모델은 “max_position_embeddings=2048” 인데 max_new_tokens(생성할 최대 토큰 수) = 1024로 낮춘 이유

- 프롬프팅도 있어서 그럼
- 내가 프롬프팅을 만약 500 토큰 이라고 한다면, "max_new_tokens =2048” 로 해버리면 총 2548 토큰으로 구성되므로, mistral의 총 “2048 토큰을 초과”해 버린다ㅇㅇ

## 4비트 양자화 했을 시 fp4 vs nf4?

| **항목**                        | **설명**                    | **현재 환경에 해당됨?**    |
| ----------------------------- | ------------------------- | ------------------ |
| **Mistral 7B**                | 원본 PyTorch 모델             | 예                |
| **bnb_4bit_quant_type=“fp4”** | GPU (CUDA)에서만 동작          | 아니요 (GGUF 모델 아님) |
| **bnb_4bit_quant_type=“nf4”** | CPU 호환 가능하나, 모델 구조에 따라 달림 | 예                |
| **GGUF 모델**                   | llama.cpp 등 CPU 최적화 전용 포맷 | 해당 안 됨           |
| **CPU에서 실행 가능?**              | ㅇㅇㅇ                       | 맞음               |
| **GPU에서 fp4 써도 되나?**          | ㄴㄴ 나는 오프로드                | 해당 안 됨           |

- 난 GGUF 모델도 GPTQ 모델도 아니니까 nf4 + offload 방식으로 간다

| **항목** | **fp4**             | **nf4**         |
| ------ | ------------------- | --------------- |
| 정식 명칭  | 4-bit float         | Normal Float 4  |
| 목적     | 일반적인 부동소수점 연산       | CPU 호환 및 안정성 강조 |
| 사용 환경  | GPU (CUDA) 중심       | CPU/GPU 가능      |
| 특징     | 더 빠르지만 일부 모델과 호환 제한 | 더 일반적이나 느림      |
| 성능     | 빠름, 메모리 효율 높음       | 약간 느림           |

---

- 4비트 양자화 사용하면서 굉장히 옵션이 많아졌다.
- 아래는 지피티가 제시해 준 것인데, v2 배포 끝나고 적용해볼까 한다.

```python
generation_kwargs = dict(
            **inputs,  # 입력 텐서 (input_ids, attention_mask 등)
            streamer=streamer,  # 스트리밍 응답을 위한 TextIteratorStreamer 객체
            max_new_tokens=512,  # 변경: 128에서 512로 증가 - JSON이 완성되도록 충분한 토큰 수 확보
            do_sample=False,  # 확률적 샘플링 비활성화 - 결정적(deterministic) 생성
            num_beams=1,  # 빔 서치 비활성화 - 단일 경로 생성으로 속도 향상
            pad_token_id=tokenizer.pad_token_id,  # 패딩 토큰 ID 설정
            eos_token_id=tokenizer.eos_token_id,  # 문장 종료 토큰 ID 설정
            use_cache=False,  # KV 캐시 사용 비활성화 - 메모리 사용량 감소
            return_dict_in_generate=True,  # 생성 결과를 딕셔너리 형태로 반환
            output_scores=False,  # 토큰별 확률 점수 출력 비활성화 - 메모리 절약
            output_hidden_states=False,  # 히든 스테이트 출력 비활성화 - 메모리 절약
            output_attentions=False,  # 어텐션 가중치 출력 비활성화 - 메모리 절약
            renormalize_logits=True,  # 로짓 재정규화 활성화 - 수치적 안정성 향상
            temperature=1.0,  # 샘플링 온도 설정 (do_sample=False이므로 영향 없음)
            repetition_penalty=1.0,  # 반복 패널티 설정 (기본값)
            no_repeat_ngram_size=0,  # n-gram 반복 제한 비활성화
            encoder_no_repeat_ngram_size=0,  # 인코더 n-gram 반복 제한 비활성화
            bad_words_ids=None,  # 금지 단어 목록 비활성화
            force_words_ids=None,  # 강제 생성 단어 목록 비활성화
            num_return_sequences=1,  # 단일 시퀀스 생성
            early_stopping=True  # 빔 서치 조기 종료 활성화 (num_beams=1이므로 영향 없음)
        )
```