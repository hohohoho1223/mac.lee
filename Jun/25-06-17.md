# 오늘 내가 배운 것들(Today I Learned)

- 참 답답한 날이었다.
- 왜 파싱이 안될까 계속 사투를 벌였던 하루였다.(free-text)

---

# SSE 연결 종료 처리 개선

### 문제점

- LLM 모델 로딩 및 응답 생성 과정에서 불필요하게 중복된 메모리 정리 로직이 존재
- `torch.cuda.empty_cache()`와 `gc.collect()`가 여러 곳에서 호출되어 오히려 성능에 영향을 줄 수 있음

### 원인 분석

- 모델 로드 시점에만 한 번 메모리 정리를 수행하면 충분함
- 스트리밍 응답 처리 중에는 모델이 이미 로드되어 있으므로 추가적인 메모리 정리가 대부분 불필요함

### 해결 과정

`Text/LLM/model/chatbot/LLM_chatbot_base_info_model.py` 파일의 모델 로드 부분에서 중복되거나 불필요한 메모리 정리 로직을 제거하고, 모델 로드 전에만 한 번 실행되도록 통합:


```python
# 모델 로드 전 메모리 정리
torch.cuda.empty_cache()
gc.collect()

# 모델 로드 시 메모리 최적화 옵션 추가
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.3",
    cache_dir=MODEL_PATH,
    device_map="auto",
    low_cpu_mem_usage=True,
    token=hf_token,
    torch_dtype=torch.float16,
    trust_remote_code=True,
    max_position_embeddings=2048,
    quantization_config=quantization_config,
    offload_folder="offload",
    offload_state_dict=True
)

# 메모리 최적화를 위한 설정 (유지)
model.config.use_cache = False
model.eval()

# ... 기존 코드 (get_llm_response 함수 내부에서 불필요한 메모리 정리 제거) ...

def get_llm_response(prompt: str, category: str) -> Generator[Dict[str, Any], None, None]:
    # ... 기존 코드 ...
    # 스트리밍 시작 전 메모리 정리 (이 부분 제거)
    # torch.cuda.empty_cache()
    # gc.collect()

    # ... 기존 코드 ...

    # 스레드 완료 대기
    thread.join()

    # 토큰 캐시 정리 (필요시 유지)
    if hasattr(streamer, 'token_cache'):
        streamer.token_cache = []

    # 메모리 정리 (이 부분 제거)
    # torch.cuda.empty_cache()
    # gc.collect()

    # ... 나머지 코드 ...
```

### 실무 팁

- `torch.cuda.empty_cache()`와 `gc.collect()`는 필요한 시점에 한 번만 호출하는 것이 효율적임.
- 모델 로드 시점에 메모리를 최적화하고, 이후 불필요한 호출을 자제하여 성능 오버헤드를 줄임.
- `model.config.use_cache = False`와 같은 모델 자체의 캐시 설정을 활용하여 메모리 사용량을 관리.

### 문제점

- /base-info의 FastAPI 서버에서 SSE 스트리밍 응답 처리 중 `RuntimeError: Unexpected ASGI message 'http.response.body' sent, after response already completed` 에러 발생
- 모델의 응답은 정상적으로 생성되고 파싱되었으나, 클라이언트로 전송하는 과정에서 문제 발생

### 원인 분석

- SSE 스트리밍이 완전히 종료되기 전에 연결이 끊어짐
- 응답이 이미 완료된 상태에서 추가 응답을 보내려고 시도
- 이는 주로 다음과 같은 상황에서 발생:
    1. 클라이언트가 연결을 일찍 종료
    2. 네트워크 연결 불안정
    3. 서버의 응답 처리 로직이 비정상적으로 종료

### 해결 과정

1. `response_completed` 플래그 추가: 응답이 완료되었는지 추적
2. 스트리밍 처리 시 `response_completed` 체크: 응답이 완료되지 않은 경우에만 처리
3. 응답 완료 시점에 `response_completed = True` 설정
4. 에러 발생 시에도 `response_completed = True` 설정

```python
full_response = ""
logger.info("스트리밍 응답 대기 중...")
response_completed = False  # 응답 완료 여부를 추적하는 플래그

try:
    # 스트리밍 응답 처리
    for new_text in streamer:
        if new_text and not response_completed:  # 응답이 완료되지 않은 경우에만 처리
            full_response += new_text
            logger.info(f"토큰 수신: {new_text[:20]}...")
            
            # ... 토큰 처리 로직 ...

            if not response_completed:
                response_completed = True
                yield {
                    "event": "close",
                    "data": json.dumps({
                        "status": 200,
                        "message": "모든 챌린지 추천 완료",
                        "data": parsed_data
                    }, ensure_ascii=False)
                }
```

## 코드 최적화 및 안정성 개선

### 1. 양자화 설정 변경 (fp4 → nf4)

- **증상**: fp4 양자화 타입이 CPU에서 지원되지 않는 문제 발생
- **원인**: fp4는 GPU 전용 양자화 타입으로, CPU 환경에서 오류 발생
- **해결**: nf4(Normal Float 4) 양자화 타입으로 변경

```python
quantization_config = BitsAndBytesConfig(
      load_in_4bit=True,
      bnb_4bit_compute_dtype=torch.float16,
      bnb_4bit_use_double_quant=True,
      bnb_4bit_quant_type="nf4"  # fp4에서 nf4로 변경
  )
```

### 2. 메모리 관리 개선

- **증상**: 메모리 누수 및 불필요한 메모리 정리로 인한 성능 저하
- **해결**: finally 블록 추가로 메모리 정리 로직 통합

```python
finally:
      # 요청 완료 후 메모리 정리
      try:
          if 'inputs' in locals():
              del inputs
          torch.cuda.empty_cache()
          gc.collect()
          logger.info("메모리 정리 완료")
      except Exception as e:
          logger.error(f"메모리 정리 중 에러 발생: {str(e)}")
```

### 3. /free-text 엔드포인트 비동기 전환

- **증상**: 동기 처리로 인한 응답 지연
- **해결**: async/await 패턴으로 비동기 처리 전환

```python
@router.get("/ai/chatbot/recommendation/free-text")
  async def freetext_rag(
      sessionId: Optional[str] = Query(None),
      message: Optional[str] = Query(None)
  ):
      # 비동기 처리 로직 구현
```

### 4. 파싱 로직 개선 및 스트리밍 안정화

- **증상**: JSON 파싱 오류 및 스트리밍 데이터 처리 문제, inf/nan 값 발생, event: token → challenge로 이벤트 타입 변경 필요
- **해결**:
    1. 스트리머 설정 최적화
    2. inf/nan 값 처리를 위한 로짓 프로세서 추가
    3. event: token → challenge로 이벤트 타입 변경

```python
# 로짓 프로세서 설정
  logits_processor = LogitsProcessorList([
      InfNanRemoveLogitsProcessor()
  ])
  
  # 스트리머 설정
  streamer = TextIteratorStreamer(
      tokenizer,
      skip_prompt=True,
      skip_special_tokens=True,
      timeout=None,
      decode_kwargs={
          "skip_special_tokens": True,
          "clean_up_tokenization_spaces": True,
          "errors": "ignore"
      }
  )
```

### 현재 상태

- 양자화 설정이 CPU/GPU 환경 모두에서 안정적으로 동작
- 메모리 관리가 효율적으로 이루어짐
- /free-text 엔드포인트의 응답 속도 개선
- JSON 파싱 및 스트리밍 데이터 처리 안정성 향상
- inf/nan 값으로 인한 오류 발생 빈도 감소