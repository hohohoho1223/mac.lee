# 오늘 내가 배운 것들(Today I Learned)

- `generation_kwargs` 모델 생성 설정 을 파헤쳐 보자.

---

- generation_kwargs

```python
# 모델 생성 설정
        generation_kwargs = dict(
            inputs,  # 입력 텐서 (input_ids, attention_mask 등)
            streamer=streamer,  # 스트리밍 응답을 위한 TextIteratorStreamer 객체
            max_new_tokens=512,  # 토큰 수를 줄여서 안정성 향상
            temperature=0.3,  # 더 낮은 temperature로 일관성 향상
            do_sample=True,  # 확률적 샘플링 활성화
            top_p=0.9,  # nucleus sampling 추가
            top_k=50,  # top-k sampling 추가
            repetition_penalty=1.1,  # 반복 방지
            pad_token_id=tokenizer.eos_token_id, # 패딩 토큰 ID 설정 (Mistral은 EOS 토큰을 패딩으로 사용)
            logits_processor=logits_processor,
            eos_token_id=tokenizer.eos_token_id,  # EOS 토큰 명시적 설정
            early_stopping=True  # 조기 중단 활성화
        )
```

- 여기서 `pad_token_id` & `eos_token_id`  이 뭘까.

## pad_token(패딩 토큰)이란?

> pad_token은 "padding token"의 줄임말로, 입력 데이터의 길이를 맞추기 위해 추가하는 "빈칸” 역할의 특수 토큰임

- 그럼 왜 입력 데이터의 길이를 맞추는가?
- 딥러닝 모델(특히 transformer 계열)은 여러 입력 시퀀스(문장 등)를 한번에 묶어서(batch) 처리 한다 -> 배치 연산
  - 배치연산 하면 속도도 빨라지고 당연히 GPU도 효율적으로 쓸 수 있음
- 하지만 실제 문장들은 길이가 다 다름
- 딥러닝 모델은 행렬(2차원 배열)을 매우 빠르게 처리하기 때문에, 짧은 문장에는 `pad_tokken` 을 추가해서 길이를 맞추는 것

## EOS(End Of Sequence)란?

> EOS는 "End Of Sequence"의 약자로써 시퀀스(문장)의 끝을 의미함
> 자연어 처리(NLP)에서 문장이나 텍스트의 끝을 표시하는 특수 토큰임

- 인공지능 언어 모델은 텍스트를 “토큰 단위”로 처리함
- 따라서 모델이 텍스트를 생성할 떄, 언제 멈춰야 할지를 알고 있어야 함
- 이때, <eos> 토큰이 나오면 “여기서 전체 응답이 끝났다”고 인식하고, 더 이상 텍스트 생성을 안함

### 추가 설정들

- `top_p=0.9`: 확률의 누적합이 0.9가 될 때까지 후보 토큰을 모아서, 그중에서 "무작위"로 하나를 선택하는 방식
- `top_k=3`: 더 좁은 후보군에서 단어 선택 (다양성↓, 일관성↑)
- `repetition_penalty=1.1`: 이미 나온 단어나 구절이 또 나오려고 할 떄, 그 확률에 패널티를 주는 방식

---

- (/free-text) 사용자가 헛소리를 해도 응답이 없는 이슈

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/981fd975-0a65-0d17-7ed4-47b0cd976a0d/ha8wxKsB6vGq8Sri2MuNnt7LUedNu9rDdXQLxjLp81gz/Image.png)

- 라우터 문제였을듯ㅇㅇ (25-06-23 에서 바라본 관점에서)

### "종료 후 추가 메시지 전송하는 오류”

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/afce0305-9af5-91f9-8ed0-d42f2f2595fb/70ulKcOScC3KmlRWe2yOiYB4ztXCRo08orAPc5CBjQ0z/Image.png)

- 분명 SSE 스트리밍이 종료되었는데, 서버가 추가로 데이터를 보내려고 할 때 발생함

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/5086fe41-66e5-dcc4-1bee-075df8976783/yQrs8Wjsxte9MkuCvtc89wS6lyAt5xR3vxXLC1pMwC0z/Image.png)

- FastAPI 또는 Starlette SSE 환경에서 흔히 발생하는 문제인데, **스트리밍 응답을 모두 마치지 않고 종료되었을 때** 발생
    - 클라이언트와의 연결이 이미 닫혔는데, 서버가 또 메세지를 보내려고 할때 생기는 것 같다.
    - 즉, 제너레이터가 끝났는데, 내부적으로 또 yield/send 시도함으로써 서버에서 에러를 준것
    - 그래서 제너레이터 함수의 정상 종료 여부를 확인하려고 예외 처리를 넣었다.

### 목적

- 모델 스트리밍이 잘 마무리 되지 않고 중간에 끊겼을 경우를 감지
- 명시적으로 error 이벤트를 yield하여 클라이언트에게 오류 알리고자

```python
 # 응답 완료 전에 함수가 종료된 경우, error 이벤트 강제 전송
        if 'response_completed' in locals() and not response_completed:
            logger.warning("응답 완료 전에 함수가 종료되어 error 이벤트를 강제 전송합니다.")
            yield {
                "event": "error",
                "data": json.dumps({
                    "status": 500,
                    "message": "응답이 중간에 종료되었습니다.",
                    "data": None
                }, ensure_ascii=False)
            }
```

- 근데 또 다른 에러가 발생했다...

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/a1665c09-2117-55e7-1adc-3074887102ba/HJXi2B7LL87l4M5DzYWFRP9jlUp2qyybXHqtGSk32l4z/Image.png)

> **FastAPI SSE 스트림이 정상 종료된 직후**,
> sse_starlette 라이브러리의 **자동 ping task**가 **추가 데이터를 보내려다 ASGI에 의해 차단된 상황**

- **ASGI 응답이 이미 종료되었는데도, SSE가 추가로 메시지를 보내려다 실패한 상황**
  - `**EventSourceResponse`** 가 이미 종료된 상태인데
  - SSE 내부(sse_starlette) 에서 ping 또는 이벤트 전송하려고 시도해서 ASGI서버에서 컷!  

### 해결방안

> sse_starlette에서 주기적으로 보내는 Ping을 비활성화 하면 됨

---

- 또 다른 에러 발생(다른 sessionId에서 동시에 요청시 메모리 터짐)
- 아래는 오류 내용이다.

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/7b29aaad-804b-bad1-1a49-2ce6a8ce8bd3/XYPEBtav8c81n2vKcHbQFSlYzxwlTeok5AUyRwnwzXoz/Image.png)

> “RuntimeError: CUDA error: an illegal memory access was encountered”

- **멀티 요청(CURL 병렬 호출)** 환경에서 **공유 모델(shared_model)** 을 동시에 사용하는 구조에서 생긴 **스레드 안전성 문제 또는 GPU 메모리 경합** 때문
  - 지금 FastAPI SSE 핸들러는 비동기 함수(async def)이지만,
  - 실제 model.generate()는 비동기 함수가 아님(동기 GPU/CPU 연산)   
  - 현재 구조는 “**서버는 비동기 SSE 방식이지만, 내부 모델 호출은 스레드 기반 동기 스트리밍 방식"이다.**
    - Hugging Face transformers 라이브러리의 generate()는 기본적으로 **동기 함수**
    - await model.generate() 처럼 비동기 호출은 불가

### 그럼 원인이 뭘까?

- 모델은 하나인데 요청은 여러개
- model.generate()는 동기 함수인데 스레드로 병렬 실행

```python
thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()
```

1. TextIteratorStreamer 는 각 요청 마다 생성하는데, 내부에서 generator & Queue 공유 -> 여러 스레드에서 한 모델을 동시에 호출하여 Queue에도 충돌 발생 가능성
2. EventSourceResponse는 요청마다 새로운 스레드 생성하여 `model.generate()` 를 실행
    - 동시 실행된 generate() 함수끼리 CUDA 메모리를 비정상적으로 공유

### 해결방안?

1. 요청 큐잉 처리
    - 동시에 하나의 요청만 model.generate() 가능하게 큐 처리
2. 비동기 generate 지원 모델로 교체
    - vLLM 또는 TGI 사용 -> 서버에서 여러 요청을 안전하게 비동기 처리 가능

---

### 오해 하고 있었던 것

1. `yield` 는 비동기에서만 사용 가능 한 것 아니었나?
    - 아니다. `yield` 는 비동기 뿐만 아니라 동기에서도 사용이 가능하다.
    - async def + yield  = 비동기 generator
        - async def 함수 안에서 yield를 사용하는 경우
        - 이떄 async for로 순회 해야함
    - def + yield = 동기 generator (현재 구조)

```python
def get_llm_response(prompt: str, category: str) -> Generator[Dict[str, Any], None, None]:
```

- def 로 정의 되어있고, 내부에서 yield를 사용했으니 동기 generator 함수다
- FastAPI SSE 응답처리에서는 동기  제네레이터도**`EventSourceResponse()`** 에 사용 가능하다

2. 아니 그럼 왜 SSE  스트리밍+ 동기 generator 조합이 가능했던거지?

- FastAPI 는 `EventSourceResponse(generator)` 를 받을 때:
  - 내부에서 이 동기 generator를 “백그라운드 스레드에서 실행”
  - SSE로 이벤트가 yield될 때마다 클라이언트에 점진적으로 전송

| **구분**             | **현재 코드 상태**                      |
| ------------------ | --------------------------------- |
| yield 사용하는 제네레이터   | **동기** 제네레이터 (def)                |
| SSE 서버 FastAPI 핸들러 | **비동기 함수 (async def)**            |
| 내부 스트리밍            | 동기 스트리밍 (TextIteratorStreamer) 기반 |

- 아래는 현재 시스템 흐름이다

```python
[프론트엔드 SSE 요청]
       ↓
[FastAPI 비동기 핸들러]
       ↓
[동기 제너레이터(event_generator)]
       ↓
[TextIteratorStreamer ← model.generate()]
```