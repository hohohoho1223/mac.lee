# 오늘 내가 배운 것들(Today I Learned)

- Redis 큐 적용하고 피드백모델 테스트를 했는데, 아래와 같이 메모리가 터졌다ㅋㅋ(챗봇에 이어서 너마저....)
- GPU 메모리 부족으로 모델 로딩에 실패

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/48A72FF0-26F9-4F2B-AC8D-179A86573418_2/nMHJQucb5gHVmt5MCfawfE7X0kZp8dbbJi5xzZ3j8Poz/Image.png)

1. 그래서 메모리 부족시 오프로드 허용하도록 `device_map` 설정을 했다.

```python
# 메모리 부족 시 CPU 오프로드를 허용하는 device_map 설정
            max_memory = {
                0: f"{available_memory}",
                "cpu": "8GB"  # CPU 메모리 제한
            }
```

2. 그리고 CPU 오프로드 허용을 마지막에 추가했다.

> `llm_int8_enable_fp32_cpu_offload=True`

```python
# 4비트 양자화 설정 - 안정성과 효율성을 위해 4비트 유지
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,  # 4비트 양자화 활성화
                bnb_4bit_compute_dtype=torch.float16,  # 계산은 16비트로 수행
                bnb_4bit_use_double_quant=True,  # 이중 양자화로 메모리 추가 절약
                bnb_4bit_quant_type="nf4",  # Normalized Float 4-bit
                llm_int8_enable_fp32_cpu_offload=True  # CPU 오프로드 허용
            )
```

- 근데도 여전히 피드백 요청을 보내면, OOM이 뜬다......뭐지
- `shared_model.py` 에서 모델이 이미 메모리에 로드되어 있어야 하는데, RQ worker가 실행될 때마다 새로운 프로세스에서 모델을 다시 로드하려고 해서 그런것임
- `nvidia-smi` 도 안먹힘 근데ㅋㅋ (메모리 현황 파악 불가...)
  > 이건 지금 와서 느끼는건데 CUDA 별도 설치가 안되서 그랬던듯! (회고중)