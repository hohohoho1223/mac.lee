# 오늘 내가 배운 것들(Today I Learned)

# Leafresh AI 피드백 서버 실행/종료 명령어 모음!

## 1. FastAPI 서버

| 구분 | 명령어 |
|--------|--------|
| 실행 | `uvicorn Text.LLM.main:app --host 0.0.0.0 --port 8000 --reload` |
| 종료 | `Ctrl + C` 또는 `kill <PID>` |

---

## 2. RQ 워커 (피드백 비동기 처리)

| 구분 | 명령어 |
|--------|--------|
| 실행 | `rq worker feedback --with-scheduler` |
| 종료 | `Ctrl + C` 또는 `kill <PID>` |

---

## 3. Redis 서버 (필요 시 직접 실행하는 경우)

| 구분 | 명령어 |
|--------|--------|
| 실행 | `sudo systemctl start redis-server` |
| 종료 | `sudo systemctl stop redis-server` |
| 자동 실행 설정 | `sudo systemctl enable redis-server` |

- 아니 근데 서버 다키고 피드백 curl 요청을 보내면:

<p align="center">
<img src="https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/A2372780-90BE-4A02-A98C-71878563FC13_2/GBtubrhJFxRQf1V31wXkxI1AmwXcPy5xDzgxDH0mclEz/%202025-07-01%20%2010.23.59.png" alt="Image.png"/>
</p>

- 이렇게 RQ 워커에서 피드백 요청이 들어올 때마다 모델이 **다시 로드됨**
- RQ 워커가 죽었다가 다시 시작되고 있음

### 왜 피드백 요청할 때마다 모델 로드 로그가 찍히는가?

- shared_model은 싱글톤 패턴으로 설계되어 있어서 정상적으로라면 FastAPI 서버가 처음 뜰 때 한 번만 모델을 로드합니다.
- 하지만 RQ 워커(rq worker feedback)는 FastAPI 서버와는 별도의 프로세스입니다.
- 즉, 워커를 새로 실행할 때마다 `shared_model = SharedMistralModel()` 이 새로 실행되고, 이때 모델이 다시 로드됩니다.
- 피드백 요청이 들어오면 워커가 작업을 처리하는데, 워커 프로세스가 처음 시작될 때 모델을 메모리에 올립니다.

→ 그래서 워커 로그에 "모델 로드" 메시지가 찍히는 것!

## **현재 구조 요약**

- generate_feedback_task() 함수에서 매번 FeedbackModel()을 **새로 생성**합니다.
- FeedbackModel 내부에서 shared_model.model을 참조하나, 이는 FastAPI 프로세스에 존재하는 인스턴스를 가져오는 것이 **아니고**, **다시 모델을 로드하게 됩니다**.
- 즉, RQ 워커 프로세스는 FastAPI 서버와 별개의 **독립된 Python 프로세스**이기 때문에 shared_model도 거기서 새로 메모리에 로드되는 것입니다.

## **요약:**

- **RQ 워커는 독립된 Python 프로세스이므로 모델을 처음부터 다시 로딩하게 됨**
- 이는 정상 동작이며, 워커에서 모델을 미리 로딩해놓는 구조를 따로 만들어야 줄일 수 있음!

### 그럼 워커가 스레드마냥 움직인다는건가?

- ㄴㄴ RQ워커는 "스레드"가 아니라 별도의 프로세스로 동작함.
- 즉, 워커는 FastAPI 서버와 완전히 독립적으로, 각자 메모리 공간에서 실행됨

## 병렬성(Concurrency)

- 워커를 여러 개 띄우면, 여러 개의 피드백 요청을 동시에 처리할 수 있다
- 이건 "멀티스레드"가 아니라 "멀티프로세스" 방식임
- 각 워커는 독립된 Python 프로세스이므로, 서로 다른 GPU/CPU 리소스를 쓸 수 있고, 서로의 메모리를 공유하지 않음

### 그럼 워커를 여려개 띄우면 멀티프로세스 -> 병렬처리가 된다는거임? 동시에?

- ㅇㅇ RQ 워커를 여러 개 띄우면, 각각이 독립적인 프로세스로 동작해서 동시에 여러 작업을 병렬로 처리할 수 있음

## 정리

- 워커 1개: 한 번에 하나의 작업만 처리 (직렬)
- 워커 여러 개: 여러 작업을 동시에 처리 (병렬, 멀티프로세스)
- 각 워커는 독립적으로 Redis 큐에서 작업을 꺼내 실행
- 예를 들어 워커 3개를 띄우면, 피드백 요청이 3개 동시에 들어와도 3개가 동시에 처리됨
- 근데 시발 서로다른 터미널에서 각자 RQ띄워야함ㅋㅋ 하

### 왜 FastAPI 서버에서 로드한 모델을 RQ 워커가 "그대로" 쓸 수 없는가?

1. FastAPI 서버와 RQ 워커는 "완전히 다른 프로세스"
    - FastAPI 서버는 하나의 Python 프로세스(메모리 공간)에서 동작
    - RQ 워커는 별도의 Python 프로세스(메모리 공간)에서 동작
    - 프로세스끼리는 메모리를 공유할 수 없음
    - 즉, FastAPI에서 이미 로드한 모델은 "다른 프로세스(RQ 워커)"에서는 접근 불가
2. 파이썬의 import, 싱글톤, 인스턴스는 "프로세스 단위"
    - 싱글톤이든, import든, 인스턴스든 모두 "프로세스 내부"에서만 공유됨
    - FastAPI에서 shared_model이 메모리에 올라가 있어도,

    RQ 워커 프로세스가 새로 뜨면 그 안에서 다시 shared_model을 import하고,

    그 프로세스 안에서만 모델이 메모리에 올라감

3. RQ 워커가 모델을 반드시 "자기 프로세스에서" 로드해야 하는 이유
    - 워커가 FastAPI와 같은 모델 인스턴스를 쓸 수 있는 방법은 없음
    > 파이썬 표준 환경에서는 프로세스 간 메모리 공유 불가
    - 그래서 워커가 뜰 때마다 "자기 메모리 공간"에 모델을 올려야 함