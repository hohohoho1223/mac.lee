# 오늘 내가 배운 것들(Today I Learned)

### RAG + Rangchain 조합이 SSE 스트리밍이 안된다?

> LangChain의 기본 LLMChain은 토큰 단위 스트리밍을 지원하지 않습니다(2024년 6월 기준).

- 그래서 두가지 선택지가 있다.(/free-text 기준)
    1. LangChain LLMChain은 이 과정을 자동화/구조화해서 관리하거나
    2. 직접 generate+streamer는 수동으로 프롬프트를 만들어서 넣기

| **방식**               | **실시간 스트리밍** | **프롬프트 조합** | **LangChain 기능** | **사용 위치**              |
| -------------------- | ------------ | ----------- | ---------------- | ---------------------- |
| generate+streamer    | O            | 직접          | X                | base-info, free-text 등 |
| LLMChain (LangChain) | X (최종만)      | 자동          | O                | RAG, 대화 그래프 등          |

## 1. generate + streamer 방식 (직접 모델 호출)

- base-info와 free-text 엔드포인트 모두에서
- HuggingFace 모델의 generate와 TextIteratorStreamer를 직접 사용해서 토큰이 생성될 때마다 실시간으로 SSE로 보내는 방식임
- 이 방식에서는
    1. 프롬프트(messages, context, query 등)를 직접 조합해서
    2. 모델에 넣고
    3. 토큰이 생성될 때마다 클라이언트로 전송함

## 2. LangChain LLMChain 방식

- LangChain의 LLMChain은 여러 입력(messages, context, query 등)을 프롬프트 템플릿에 넣어서 한 번에 LLM에 전달하고
- 최종 결과(한 번에 완성된 답변)만 반환
- 이 방식은 실시간 토큰 스트리밍이 아니라, 최종 결과만 반환함
- 랭체인은 SSE 스트리밍을 구현해야하는 서비스 관점에선 선택지가 아니다!

### 그래서  generate + streamer 방식으로 재설계 해보자

- ### LangChain 없이 직접 generate + streamer + SSE

  - RAG(검색): LangChain retriever 등으로 컨텍스트 생성
  - 프롬프트 조합: 직접 format
  - 모델 호출: HuggingFace 모델의 generate + TextIteratorStreamer
  - SSE: 토큰이 생성될 때마다 yield
- 아래는 free-text 결과이다.

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/5BDAFD63-1958-4A5D-B5C5-E40605E3A07F_2/BBxvNR5evdvDpbTchW8xlsOk03qCFkdyzEJN1Aeht8Uz/Image.png)

- 참나 이젠 스트리밍이 문단별로 된다ㅋㅋ

###  이유는?

- 문단별로 스트리밍되는 건 LLM의 생성 단위와 streamer의 버퍼링 특성 때문입니다.
- 더 세밀한 실시간 스트리밍을 원한다면

> “프롬프트를 “챌린지 한 개씩, title/description이 완성될 때마다 바로 출력”하도록 유도하는 것이 가장 효과적이다.”

- 이게 뭔 개소리지? 뭐 프롬프팅 한줄 달라졌다고 스트리밍 형식이 달리지나?

---

- 그 다 필요없고 RAG? 라우터에서 한줄 수정으로 구현이 됐다

```python
# RAG 검색 수행
        docs = retriever.get_relevant_documents(message)
        context = "\n".join([doc.page_content for doc in docs])
        logger.info(f"RAG 검색 완료. 문서 수: {len(docs)}")
        logger.info(f"컨텍스트 길이: {len(context)}")

        prompt = custom_prompt.format(
            context=context,  # RAG 활성화
            query=message,
            messages=messages_history,
            category=current_category
        )
```

- 하 나 어제 뭐한거지? 
- 아래는 Mermaid 다이어그램.

```other
graph TD
    A[사용자 요청] --> B{chatbot_router.py};
    B --> C[LangChain Retriever];
    C --> D[Qdrant DB];
    D --> C;
    C --> B;
    B --> E[Mistral 모델];
    subgraph 스트리밍
        E -- TextIteratorStreamer --> B;
    end
    B --> F[사용자 응답];
```

## 단계별 설명

1. chatbot_router.py: 사용자 요청을 받고
2. retriever.get_relevant_documents(): LangChain의 retriever를 사용하여 Qdrant DB에서 관련 문서를 검색함 (RAG)
3. prompt 생성: 검색된 문서(context)와 대화 기록을 조합하여 프롬프트를 생성
4. model.generate() + TextIteratorStreamer: transformers 라이브러리를 사용하여 모델 응답을 스트리밍
5. SSE 응답: 라우터에서 스트리밍된 토큰을 SSE 형식으로 클라이언트에 전송

## 결론

- LangChain의 RAG 기능과 transformers의 스트리밍 기능을 조합하여 구현한 것이 맞음

### 그럼 retrieve_context()는 라우터에서 왜 안쓰는가?

| **구조**                                   | **RAG 수행 위치**                                                     | **처리 방식**           | **비고**       |
| ---------------------------------------- | ----------------------------------------------------------------- | ------------------- | ------------ |
| SSE 방식 (GET /recommendation/free-text)   | ❗ FastAPI 라우터 내부에서 직접 retriever.get_relevant_documents() 호출       | 실시간 스트리밍용(SSE)      | 비동기 yield 처리 |
| JSON 방식 (POST /recommendation/free-text) | process_chat() → chat_graph.invoke() → retrieve_context() 내부에서 처리 | LangGraph 기반 흐름(동기) | 상태 추적 + 관리   |

## **구조 차이 상세 비교**

1. **POST 방식 (동기 방식)**

- **endpoint**: POST /ai/chatbot/recommendation/free-text
- **사용 흐름**:
  - process_chat() 함수 호출
  - LangGraph 기반 state machine 흐름:
  - validate_query → retrieve_context → generate_response

- **RAG 위치**: retrieve_context() 노드 안에서 실행
- **응답**: 모든 생성이 끝난 후 JSON 한 번 반환 (JSONResponse)
- **사용 예시**: 프론트가 응답을 기다렸다가 한 번에 화면 갱신할 때 적합

---

2. **GET 방식 (SSE 스트리밍 방식)**

- **endpoint**: GET /ai/chatbot/recommendation/free-text
- **사용 흐름**:
  - FastAPI 라우터 내 event_generator()에서 바로 실행
  - retriever.get_relevant_documents() 호출 → prompt 생성 → get_llm_response() 호출 → yield
- **RAG 위치**: **라우터 함수 내부**에서 직접 실행됨
- **응답**: 생성되는 토큰을 **한 토큰씩 클라이언트에 실시간으로 스트리밍** 전송 (EventSourceResponse)
- **사용 예시**: 프론트에서 event: message 받아서 실시간으로 응답 출력할 때 적합 (ex. 채팅창)

---

- 지금 현재 내코드는 모델 로드를 할때 양자화를 적용시켜서 메모리에 적재한다
- 아래는 shared_model.py 코드임.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
import os
import logging
from huggingface_hub import login
import gc
from fastapi import HTTPException
from dotenv import load_dotenv

# 로깅 설정
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SharedMistralModel:
    _instance = None
    _model = None
    _tokenizer = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(SharedMistralModel, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self._initialize_model()
            self._initialized = True
    
    def _initialize_model(self):
        """모델 초기화 - 한 번만 실행됨"""
        # 환경 변수 로드
        load_dotenv()
        
        # Hugging Face 로그인
        hf_token = os.getenv("HUGGINGFACE_API_KEYMAC")
        if hf_token:
            try:
                login(token=hf_token)
                logger.info("Hugging Face Hub에 성공적으로 로그인했습니다.")
            except Exception as e:
                logger.error(f"Hugging Face Hub 로그인 실패: {e}")
        else:
            logger.warning("HUGGINGFACE_API_KEYMAC 환경 변수를 찾을 수 없습니다. Hugging Face 로그인 건너뜜.")

        # 모델 경로 설정
        MODEL_PATH = "/home/ubuntu/mistral"
        logger.info(f"Model path: {MODEL_PATH}")

        # 모델 경로 확인
        if not os.path.exists(MODEL_PATH):
            logger.error(f"모델 경로를 찾을 수 없습니다: {MODEL_PATH}")
            raise HTTPException(
                status_code=500,
                detail={
                    "status": 500,
                    "message": f"모델 경로를 찾을 수 없습니다: {MODEL_PATH}",
                    "data": None
                }
            )

        # GPU 사용 가능 여부 확인
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"사용 가능한 디바이스: {device}")

        try:
            logger.info("Loading tokenizer...")
            self._tokenizer = AutoTokenizer.from_pretrained(
                "mistralai/Mistral-7B-Instruct-v0.3",
                cache_dir=MODEL_PATH,
                torch_dtype=torch.float16,
                token=hf_token
            )
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
                self._tokenizer.pad_token_id = self._tokenizer.eos_token_id
            
            logger.info("Loading model...")

            # 4비트 양자화 설정 - 안정성과 효율성을 위해 4비트 유지
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,  # 4비트 양자화 활성화
                bnb_4bit_compute_dtype=torch.float16,  # 계산은 16비트로 수행
                bnb_4bit_use_double_quant=True,  # 이중 양자화로 메모리 추가 절약
                bnb_4bit_quant_type="nf4"  # Normalized Float 4-bit
            )

            # GPU 메모리 사용량 계산
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            model_memory = 4 * 1024**3  # 4GB (4비트 양자화)
            available_memory = int((gpu_memory - model_memory) * 0.9)
            logger.info(f"GPU 메모리: {gpu_memory / 1024**3:.2f}GB, 모델 예상 메모리: {model_memory / 1024**3:.2f}GB, 사용 가능 메모리: {available_memory / 1024**3:.2f}GB")

            # 모델 로드 전 메모리 정리
            torch.cuda.empty_cache()
            gc.collect()

            self._model = AutoModelForCausalLM.from_pretrained(
                "mistralai/Mistral-7B-Instruct-v0.3",
                cache_dir=MODEL_PATH,
                device_map="auto",
                low_cpu_mem_usage=True,
                token=hf_token,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                max_position_embeddings=2048,
                quantization_config=quantization_config,
                offload_folder="offload",
                offload_state_dict=True,
            )
            
            # 메모리 최적화를 위한 설정
            self._model.config.use_cache = False
            self._model.eval()

        except Exception as e:
            logger.error(f"모델 로딩 실패: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail={
                    "status": 500,
                    "message": f"모델 로딩 실패: {str(e)}",
                    "data": None
                }
            )

        logger.info("Shared Mistral model loaded successfully!")
    
    @property
    def model(self):
        return self._model
    
    @property
    def tokenizer(self):
        return self._tokenizer
    
    def cleanup_memory(self):
        """메모리 정리"""
        try:
            torch.cuda.empty_cache()
            gc.collect()
            logger.info("Shared model memory cleanup completed")
        except Exception as e:
            logger.error(f"Memory cleanup error: {str(e)}")

# 전역 인스턴스 생성
shared_model = SharedMistralModel() 
```

- 여기서 8비트 양자화를 시켰더니 Out Of Memory(OOM)현상이 발생해 4비트 양자화를 시켰다.

> 왜 시켰냐?
> 원본 Pytorch모델을 그대로 쓰니까 한 답변 받는데에만 5분이 걸렸다..ㅋㅋ

- 그래서 모델은 4비트로 저장돼 있지만, 연산은 `torch_dtype=torch.float16` 로 지정하여 16비트로 정밀도를 수정했다.

## **왜 PyTorch에서는 멀티스레딩이 위험한가?**

- PyTorch 모델은 내부적으로 GPU/CPU 메모리 상태를 공유함
- model.generate()는 스레드-safe하지 않기 때문에, 여러 스레드에서 동시에 호출하면 **CUDA 충돌**, **context race**, **memory leak** 등이 발생할 수 있음
  - `model.generate()` 가 내부에서 무슨 일을 하는데?
  - transformers 라이브러리에서 다음과 같은 과정을 수행함
  1. 입력 토크나이저 처리 -> input_ids준비
   > input_ids: 토큰화된 입력 텍스트의 정수 인덱스 목록(모델에 문장을 입력하기 전에 텍스트를 숫자(토큰ID)로 바꾼 결과)

  2. 토큰 반복 생성: (for_in range(max_new_tokens) 형태)
  3. 매 토큰마다:
        - model.forward() 호출
        - GPU에서 matmul, softmax, argmax 연산 수행
        - past_key_values 업데이트
    4. 최종 출력 텐서를 디코딩 후 반환
        - 이러한 전체 과정은 "상태"이며, GPU 메모리 상에 attention_cache, past_key_values, logits 등이 임시로 저장되며 이것들이 동시에 호출되면 문제가 발생
- 따라서 PyTorch 모델은 멀티스레딩보다는 **병렬 프로세스 혹은 vLLM 같은 구조**로 처리해야 안전하다..!

### 멀티프로세스 적용?

- 세션마다 **별도 모델 로딩** 가능 → CUDA 연산도 완전 독립 실행 가능
- 하지만 → **VRAM 사용량이 N배로 증가** (7B 모델은 현실적으로 불가능)

## **비교: LoRA Adapter 분리 vs vLLM 도입**

| **항목**       | **LoRA Adapter 분리**                      | **vLLM 도입**                                            |
| ------------ | ---------------------------------------- | ------------------------------------------------------ |
| **속도 (처리량)** | 중간 (한 요청씩 처리됨)                           | **매우 빠름** (동시 요청 최적화)                                  |
| **VRAM 사용량** | 적음 (모델 하나만 유지)                           | 약간 높음 (캐시 & 병렬 처리용 버퍼)                                 |
| **구현 난이도**   | **낮음** (기존 코드에 adapter만 붙이면 됨)           | **중간 이상** (서버 구조 변경 필요)                                |
| **추론 정확도**   | 그대로 유지                                   | 그대로 유지                                                 |
| **장점**       | \- 기존 구조 재활용 가능- adapter 교체만으로 세션 분기 가능  | \- tokenizer 병렬화- Streaming 최적화- Prompt + Token 캐시 자동화 |
| **단점**       | \- 동시성 처리 직접 구현해야 함- adapter 로딩 지연 발생 가능 | \- 기존 FastAPI 구조와 충돌 가능- 비동기 세션 관리 필요                  |

- 현재 코드 로직을 가져가면서 업데이트를 해보자
- 여기서 궁금증이 생겼다.

> “세션별로 요청을 매핑해야 된다?”

- 그럼 세션 요청이 많아지면? 아래는 유사 상황이다.

| **상황**        | **추천 대응**                        |
| ------------- | -------------------------------- |
| 세션 수 적고 자주 반복 | LRU 캐시로 adapter 유지               |
| 세션 수 많고 요청 몰림 | 작업 큐 기반 직렬화 또는 병렬 분산처리           |
| VRAM 부족       | 사용 후 adapter 제거 및 캐시 초기화         |
| 속도와 메모리 모두 중요 | vLLM, GGUF, 또는 multi-GPU 아키텍처 고려 |

- 근데 어쩄든 직렬처리 밖에 할 수 없는 상황(세션 별 모델을 멀티프로세스로 분리 . 할수 없으므로)

### 그럼 왜 직렬 처리 밖에 하지 못하는가?

- 모델이 하나임
  - PyTorch 기반에서는 모델 인스턴스를 여러 세션에서 동시에 공유시 충돌
- LoRA가 변동성이 심함
  - `mode = PeftModel(model, lora_config)`처럼 기존 모델 위에 직접 adapter가 붙음 -> 한 번에 하나만 유지 가능
- PyTorch는 스레드에 불안정함
  - `.generate()` 는 병렬 요청시 race condition 발생 가능성 높음
  > race condition: 둘 이상의 프로세스나 스레드가 동시에 같은 자원에 접근할 때, "실행 순서에 따라 결과가 달라지는 문제"

- 그래서 사용자 입장에서 기다리기 넘 심심하다~

### 그럼 vLLM? 얘는 뭔데?

- 기존 PyTorch 모델과 vLLM의 구조차이

| **항목**           | **기존 PyTorch 구조**         | **vLLM 구조**                      |
| ---------------- | ------------------------- | -------------------------------- |
| **추론 방식**        | model.generate() 동기 함수 호출 | 비동기 토큰 단위 스케줄러                   |
| **동시 요청 처리**     | 불가능 (race condition 발생)   | 가능 (토큰 단위 병렬 실행)                 |
| **메모리 관리**       | GPU 할당 & 해제 반복            | 지속적인 KV 캐시 재사용                   |
| **응답 속도**        | 요청 수 증가 시 선형 지연           | 요청 수 증가해도 토큰 단위로 병렬화             |
| **Streaming 지원** | TextStreamer 필요           | 내장형 Streaming 지원 (OpenAI API 호환) |

### vLLM의 핵심 기술: PagedAttention & Token-level Scheduler

1. PagedAttention:
    - KV cache를 paging 구조로 관리해서 더 많은 context를 효율적으로 저장
2. Token-level scheduling:
    - 요청이 들어오면 문장 단위가 아닌 토큰 단위로 병렬 처리
    - N개의 요청이 있어도 1개의 모델만 사용해 토큰별로 순환 처리 가능
3. Continuous batching:
    - 각기 다른 길이의 문장 요청도 batch로 자동 처리
    - 예: 요청 A가 5토큰, B가 12토큰이면 A는 먼저 응답되고 B는 계속 진행

- KV Cache Paging 구조: PagedAttention

> KV Cache: GPT계열 모델에서 이전에 생성한 key,value 값을 저장해놓고 다음 토큰 생성에 재사용
> 기본구조 (Dense): 모든 시퀀스에 대해 고정된 크기의 메모리를 미리 할당(낭비)
> vLLM의 구조 (Paged): 요청마다 필요한 만큼만 페이지 단위로 메모리 할당

- Continuous Batching: 토큰 단위 스케줄링

> 기존 PyTorch는 요청마다 `model.generate()` 를 독립적으로 실행하지만 vLLM은 여러 요청을 실시간으로 끼워 넣어서 처리함 -> 마치 CPU 멀티태스킹처럼

- 근데 문제가 생겼다
  - vLLM서버는 POST로만 통신한댄다ㅋㅋ (망했)
  - 지금 현재 내 코드는 SSE 핸들러 떄문에 GET방식인데 그래서 아래와 같은 방법이 있다.  
  > **GET SSE 핸들러를 유지하려면**, 내부에서 httpx.AsyncClient 등을 이용해서 **vLLM POST 요청을 proxy 형태로 보내야** 함
  - 이 방식은 기존 구조를 유지하면서 vLLM의 streaming POST API와 연결할 수 있다