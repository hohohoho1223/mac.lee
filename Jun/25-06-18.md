# 오늘 내가 배운 것들(Today I Learned)

- 오늘은 메모리 문제 해결을 위해 모델 로드를 변경하였다.

---

- 챗봇에 요청을 각 엔드포인트 한번씩 보냈다.
- 아래는 `/base-info` curl 요청: 

```python
curl -N "http://35.216.82.57:8000/ai/chatbot/recommendation/base-info?sessionId=user123&location=%EB%8F%84%EC%8B%9C&workType=%ED%98%84%EC%9E%A5%EC%A7%81&category=%EB%B9%84%EA%B1%B4"                  
```

- 요청:

```python
sessionId: user123
•location: 도시
•workType: 현장직
•category: 비건
```

- 아래는 `/free-text` curl 요청:
    > 요청은 "아무거나 추천” 임

```python
curl -N "http://35.216.82.57:8000/ai/chatbot/recommendation/free-text?sessionId=user123&message=%EC%95%84%EB%AC%B4%EA%B1%B0%EB%82%98%20%EC%B6%94%EC%B2%9C"
```

- 근데 다음과 같은 메모리 초과 오류가 났다

![스크린샷 2025-06-18 오전 11.39.37.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/29A562D9-012A-4EEE-83CA-8AAE84E0B073_2/ShtcWhX7HF3fIePhhL3oyOFHT3RDqk3YCE3CuZ0SXhsz/%202025-06-18%20%2011.39.37.png)

- “CUDA out of memory”….
- 그래서 내 현재 코드를 검토해 보았는데,

### 문제점

- 두 엔드포인트(`/base-info`, `/free-text`)가 각각 독립적으로 모델을 로드
- 각각 4GB씩 사용하여 총 8GB 메모리 사용 (L4 GPU 24GB 중 33% 사용)
- 연속 요청 시 메모리 부족으로 CUDA Out of Memory 오류 발생
- 첫 번째 요청 후 메모리가 제대로 정리되지 않아 두 번째 요청 시 문제 발생
- `LLM_chatbot_base_info_mode.py` & `LLM_chatbot_free_text_model.py` 둘다 각자 모델을 로드 하고 있었다!

- 그래서 "shared_model.py" 를 만들었다.

- shared_model.py

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import torch
import os
import logging
from huggingface_hub import login
import gc
from fastapi import HTTPException
from dotenv import load_dotenv

# 로깅 설정
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class SharedMistralModel:
    _instance = None
    _model = None
    _tokenizer = None
    _initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(SharedMistralModel, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._initialized:
            self._initialize_model()
            self._initialized = True
    
    def _initialize_model(self):
        """모델 초기화 - 한 번만 실행됨"""
        # 환경 변수 로드
        load_dotenv()
        
        # Hugging Face 로그인
        hf_token = os.getenv("HUGGINGFACE_API_KEYMAC")
        if hf_token:
            try:
                login(token=hf_token)
                logger.info("Hugging Face Hub에 성공적으로 로그인했습니다.")
            except Exception as e:
                logger.error(f"Hugging Face Hub 로그인 실패: {e}")
        else:
            logger.warning("HUGGINGFACE_API_KEYMAC 환경 변수를 찾을 수 없습니다. Hugging Face 로그인 건너뜜.")

        # 모델 경로 설정
        MODEL_PATH = "/home/ubuntu/mistral"
        logger.info(f"Model path: {MODEL_PATH}")

        # 모델 경로 확인
        if not os.path.exists(MODEL_PATH):
            logger.error(f"모델 경로를 찾을 수 없습니다: {MODEL_PATH}")
            raise HTTPException(
                status_code=500,
                detail={
                    "status": 500,
                    "message": f"모델 경로를 찾을 수 없습니다: {MODEL_PATH}",
                    "data": None
                }
            )

        # GPU 사용 가능 여부 확인
        device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"사용 가능한 디바이스: {device}")

        try:
            logger.info("Loading tokenizer...")
            self._tokenizer = AutoTokenizer.from_pretrained(
                "mistralai/Mistral-7B-Instruct-v0.3",
                cache_dir=MODEL_PATH,
                torch_dtype=torch.float16,
                token=hf_token
            )
            if self._tokenizer.pad_token is None:
                self._tokenizer.pad_token = self._tokenizer.eos_token
                self._tokenizer.pad_token_id = self._tokenizer.eos_token_id
            
            logger.info("Loading model...")

            # 8비트 양자화 설정 - 더 나은 성능을 위해 4비트에서 8비트로 변경
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True,  # 8비트 양자화 활성화
                llm_int8_threshold=6.0,  # 8비트 양자화 임계값
                llm_int8_has_fp16_weight=True  # fp16 가중치 사용
            )

            # GPU 메모리 사용량 계산
            gpu_memory = torch.cuda.get_device_properties(0).total_memory
            model_memory = 8 * 1024**3  # 8GB (8비트 양자화)
            available_memory = int((gpu_memory - model_memory) * 0.9)
            logger.info(f"GPU 메모리: {gpu_memory / 1024**3:.2f}GB, 모델 예상 메모리: {model_memory / 1024**3:.2f}GB, 사용 가능 메모리: {available_memory / 1024**3:.2f}GB")

            # 모델 로드 전 메모리 정리
            torch.cuda.empty_cache()
            gc.collect()

            self._model = AutoModelForCausalLM.from_pretrained(
                "mistralai/Mistral-7B-Instruct-v0.3",
                cache_dir=MODEL_PATH,
                device_map="auto",
                low_cpu_mem_usage=True,
                token=hf_token,
                torch_dtype=torch.float16,
                trust_remote_code=True,
                max_position_embeddings=2048,
                quantization_config=quantization_config,
                offload_folder="offload",
                offload_state_dict=True,
            )
            
            # 메모리 최적화를 위한 설정
            self._model.config.use_cache = False
            self._model.eval()

        except Exception as e:
            logger.error(f"모델 로딩 실패: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail={
                    "status": 500,
                    "message": f"모델 로딩 실패: {str(e)}",
                    "data": None
                }
            )

        logger.info("Shared Mistral model loaded successfully!")
    
    @property
    def model(self):
        return self._model
    
    @property
    def tokenizer(self):
        return self._tokenizer
    
    def cleanup_memory(self):
        """메모리 정리"""
        try:
            torch.cuda.empty_cache()
            gc.collect()
            logger.info("Shared model memory cleanup completed")
        except Exception as e:
            logger.error(f"Memory cleanup error: {str(e)}")

# 전역 인스턴스 생성
shared_model = SharedMistralModel() 
```

![Image.png](https://resv2.craft.do/user/full/641ffdb9-6693-37da-6dbd-e78e1756c2de/doc/3c17d71c-25ef-2249-36c5-6ac2c9747d25/20090BC0-9D33-4238-B357-08C42A9C9124_2/ZdRhK7xi8r4XtnooqeFU1V9JVciKaTHqbOapkyzTOecz/Image.png)

- 모델 한번만 메모리에 적재해서 괜찮겠지 했는데, 아니나 다를까 모델 로드 실패했다
- 메모리 이슈..22.05 GiB 중 106.12 MiB만 남음 

### 해결방안: 4bit 양자화 모델로 싱글톤 패턴 적용

- **싱글톤 패턴의 공유 모델 구현**:
    1. `Text/LLM/model/chatbot/shared_model.py` 생성
    2. `SharedMistralModel` 클래스로 싱글톤 패턴 구현
    3. 챗봇의 두 엔드포인트& 피드백 엔드포인트 가 같은 모델 인스턴스를 공유

### 성능 개선 효과

- 메모리 사용량: 기존 12GB (3개 모델) → 4GB (1개 공유 모델)
- 절약: 67% 메모리 절약
- 안정성: 모델 로딩 중복 제거로 메모리 부족 오류 해결
- 유지보수성: 모델 설정 변경 시 한 곳에서만 수정