# -*- coding: utf-8 -*-
"""[ì¹´ì¹´ì˜¤ ë¶€íŠ¸ìº í”„] 4ì£¼ì°¨_ë¯¸ë‹ˆí€˜ìŠ¤íŠ¸(CNN).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K5FR4jwz7VceQ81nZFsJ6FrC40WFV0N0
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

"""#ë°ì´í„°ì „ì²˜ë¦¬

<aside>
ğŸ‘‰ ê°„ë‹¨í•œ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ì§„í–‰í•´ë³´ì„¸ìš”.

ê° ë‹¨ê³„ì—ì„œ í•„ìš”í•œ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.

### ë°ì´í„°ì…‹

- ê°€ìƒì˜ í•™ìƒ ì„±ì  ë°ì´í„°
- ì˜ˆì‹œë°ì´í„°
    
    ```python
    # ê°€ìƒì˜ ë°ì´í„°ì…‹ ìƒì„±
    data = {
        'í•™ìƒ': ['A', 'B', 'C', 'D', 'E'],
        'ìˆ˜í•™': [90, np.nan, 85, 88, np.nan],
        'ì˜ì–´': [80, 78, np.nan, 90, 85],
        'ê³¼í•™': [np.nan, 89, 85, 92, 80]
    }
    ```
    

### ë¬¸ì œ ì„¤ëª…

1. ë°ì´í„° ìˆ˜ì§‘
    - ê°€ìƒì˜ í•™ìƒ ì„±ì  ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
2. ê²°ì¸¡ê°’ ì²˜ë¦¬
    - ë°ì´í„°ì…‹ì— ëˆ„ë½ëœ ê°’ì´ ìˆì„ ë•Œ, ì´ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
3. ì´ìƒì¹˜ ì œê±°
    - ë°ì´í„°ì…‹ì— ì´ìƒì¹˜ê°€ ì—†ëŠ”ì§€ í™•ì¸í•˜ê³ , í•„ìš”í•˜ë©´ ì œê±°í•©ë‹ˆë‹¤.
4. ë°ì´í„° ì •ê·œí™”
    - ìˆ˜í•™, ì˜ì–´, ê³¼í•™ ì ìˆ˜ë¥¼ 0ê³¼ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§í•©ë‹ˆë‹¤.
5. ë°ì´í„° ë¶„í• 
    - ë°ì´í„°ì…‹ì„ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
</aside>
"""

#1 ë°ì´í„° ìˆ˜ì§‘

data = {
        'í•™ìƒ': ['A', 'B', 'C', 'D', 'E'],
        'ìˆ˜í•™': [90, np.nan, 85, 88, np.nan],
        'ì˜ì–´': [80, 78, np.nan, 90, 85],
        'ê³¼í•™': [np.nan, 89, 85, 92, 80]
}

df = pd.DataFrame(data)
df

#2 ê²°ì¸¡ê°’ ì²˜ë¦¬

# df['ìˆ˜í•™'].fillna(df['ìˆ˜í•™'].mean(), inplace=True)
# df['ì˜ì–´'].fillna(df['ì˜ì–´'].mean(), inplace=True)
# df['ê³¼í•™'].fillna(df['ê³¼í•™'].mean(), inplace=True)

df['ìˆ˜í•™'] = df['ìˆ˜í•™'].fillna(df['ìˆ˜í•™'].mean()) # ë®ì–´ ì”Œìš°ëŠ” ë°©ì‹ì´ ë” ì§ê´€ì ì´ë‹¤
df['ì˜ì–´'] = df['ì˜ì–´'].fillna(df['ì˜ì–´'].mean())
df['ê³¼í•™'] = df['ê³¼í•™'].fillna(df['ê³¼í•™'].mean())

df

df.dtypes

#3 ì´ìƒì¹˜ ì œê±°

score = df[(df.iloc[:,1:].values >= 0) & (df.iloc[:,1:].values <= 100)] # ì ìˆ˜ê°€ 0ì´ìƒë§Œ ì·¨ê¸‰
# score = df[(df.values[:,1:] >= 0) & (df.values[:,1:] <= 100)] # ì´ê²ƒë„ ê°€ëŠ¥
score

#3 ì´ìƒì¹˜ ì œê±°

# score = df[(df.iloc[:,1:] >= 0) & (df.iloc[:,1:] <= 100)] # ì ìˆ˜ê°€ 0ì´ìƒë§Œ ì·¨ê¸‰
# score

#4 ë°ì´í„° ì •ê·œí™”
# 0~1 ì‚¬ì´ ê°’ìœ¼ë¡œ ìŠ¤ì¼€ì¼ë§
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler() # ê°ë°ì´í„°ì˜ ìµœëŒ€,ìµœì†Œê°’ì„ ê¸°ë°˜ìœ¼ë¡œ 0~1 ì‚¬ì´ê°’ìœ¼ë¡œ ë³€í™˜í•¨

df[['ìˆ˜í•™','ì˜ì–´','ê³¼í•™']] = scaler.fit_transform(df[['ìˆ˜í•™','ì˜ì–´','ê³¼í•™']])
df

#5 ë°ì´í„° ë¶„í• 

from sklearn.model_selection import train_test_split


train , test = train_test_split(df, test_size = 0.2, random_state = 42) # `random_state`ë¥¼ ì„¤ì •í•´ì¤˜ì•¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ê³„ì† ì–»ì„ ìˆ˜ ìˆë‹¤.
print(train)
print(test)

"""<aside>
ğŸ‘‰ ê°€ìƒì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì „ì²˜ë¦¬ ê³¼ì •ì„ ì§ì ‘ êµ¬í˜„í•´ ë³´ì„¸ìš”.

ê° ë‹¨ê³„ì— ë§ì¶° ì½”ë“œë¥¼ ì‘ì„±í•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ í•™ìŠµìš©ê³¼ ê²€ì¦ìš© ë°ì´í„°ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì„ í•©ë‹ˆë‹¤.

### ë°ì´í„°ì…‹

- ê°€ìƒì˜ ì œí’ˆ íŒë§¤ ë°ì´í„°
    
    ```python
    # ê°€ìƒì˜ ë°ì´í„°ì…‹ ìƒì„±
    data = {
        'ì œí’ˆ': ['A', 'B', 'C', 'D', 'E'],
        'ê°€ê²©': [100, 150, 200, 0, 250],
        'íŒë§¤ëŸ‰': [30, 45, np.nan, 55, 60]
    }
    ```
    

### ë¬¸ì œ

1. ê²°ì¸¡ê°’ì„ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
2. ì´ìƒì¹˜ë¥¼ ì œê±°í•©ë‹ˆë‹¤. (ì˜ˆ: ê°€ê²©ì´ 0 ì´í•˜ì¸ ê²½ìš°)
3. ë°ì´í„°ë¥¼ í‘œì¤€í™”í•©ë‹ˆë‹¤. (í‰ê·  0, í‘œì¤€í¸ì°¨ 1)
4. ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. (í•™ìŠµìš© 70%, ê²€ì¦ìš© 30%)

### ë¬¸ì œ ì„¤ëª…

1. ë°ì´í„° ìˆ˜ì§‘
    - ê°€ìƒì˜ ì œí’ˆ íŒë§¤ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
2. ê²°ì¸¡ê°’ ì²˜ë¦¬
    - ë°ì´í„°ì…‹ì— ëˆ„ë½ëœ ê°’ì´ ìˆì„ ë•Œ, ì´ë¥¼ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.
3. ì´ìƒì¹˜ ì œê±°
    - ê°€ê²©ì´ 0 ì´í•˜ì¸ ë°ì´í„°ë¥¼ ì œê±°í•©ë‹ˆë‹¤.
4. ë°ì´í„° í‘œì¤€í™”
    - ê°€ê²©ê³¼ íŒë§¤ëŸ‰ ë°ì´í„°ë¥¼ í‰ê·  0, í‘œì¤€í¸ì°¨ 1ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
5. ë°ì´í„° ë¶„í• 
    - ë°ì´í„°ì…‹ì„ í•™ìŠµìš©ê³¼ ê²€ì¦ìš©ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤
</aside>
"""

#1 ë°ì´í„°ìˆ˜ì§‘

data = {
      'ì œí’ˆ': ['A', 'B', 'C', 'D', 'E'],
      'ê°€ê²©': [100, 150, 200, 0, 250],
      'íŒë§¤ëŸ‰': [30, 45, np.nan, 55, 60]
  }

df = pd.DataFrame(data)
df

#2 ê²°ì¸¡ê°’ ì²˜ë¦¬

df['íŒë§¤ëŸ‰'] = df['íŒë§¤ëŸ‰'].fillna(df['íŒë§¤ëŸ‰'].median())
df

#3 ì´ìƒì¹˜ ì œê±°

df = df[df['ê°€ê²©'].dropna() > 0]
# df[df.dropna()] -> ëª¨ë“  ê²°ì¸¡ì¹˜ ì œê±°ì¸ê±´ ì•Œì•˜ëŠ”ë°
# df[df.dropna() > 0] -> 0 ì´ˆê³¼ì¸ê°’ ì œì™¸ ì‚­ì œ í•œë‹¤ëŠ” ëœ»ì„!!
df

#4 ë°ì´í„° í‘œì¤€í™”

from sklearn.preprocessing import StandardScaler

df[['ê°€ê²©','íŒë§¤ëŸ‰']] = StandardScaler().fit_transform(df[['ê°€ê²©','íŒë§¤ëŸ‰']])
df

#5 ë°ì´í„° ë¶„í• 

from sklearn.model_selection import train_test_split

train,test = train_test_split(df, test_size= 0.2, random_state = 42)

print("\n íŠ¸ë ˆì´ë‹ ë°ì´í„°")
print(train) # íŠ¸ë ˆì´ë‹ ë°ì´í„°
print("\n í…ŒìŠ¤íŠ¸ ë°ì´í„°")
print(test) # í…ŒìŠ¤íŠ¸ ë°ì´í„°

"""#ë°ì´í„°ì¦ê°•

<aside>
ğŸ‘‰ Pythonê³¼ Pillowë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ íšŒì „, ë’¤ì§‘ê¸°, ì´ë™, í™•ëŒ€/ì¶•ì†Œ, ìƒ‰ìƒ ë³€í˜•, ë…¸ì´ì¦ˆ ì¶”ê°€ë¥¼ í•´ë³´ì„¸ìš”.

### ì´ë¯¸ì§€ ì˜ˆì‹œ (ì¶œì²˜: [ìœ„í‚¤í”¼ë””ì•„](https://ko.wikipedia.org/wiki/%EB%A0%88%EB%82%98_%28%EC%9D%B4%EB%AF%B8%EC%A7%80%29))

[Lenna.jpg](attachment:476de136-2de8-464b-91c2-dfa9c6865e61:Lenna.jpg)

### ë¬¸ì œ ì„¤ëª…

1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ë¶ˆëŸ¬ì˜¤ê¸°
2. ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°
3. ì´ë¯¸ì§€ ì¦ê°• ê¸°ë²• ì ìš©
</aside>
"""

#1 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° ë¶ˆëŸ¬ì˜¤ê¸°

from PIL import Image # ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  ì¡°ì‘í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import numpy as np # ë°°ì—´ ë° í–‰ë ¬ ì—°ì‚°ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
import matplotlib.pyplot as plt # ì‹œê°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬

#2 ì´ë¯¸ì§€ ë¶ˆëŸ¬ì˜¤ê¸°

image = Image.open("/content/drive/MyDrive/Colab Notebooks/[á„á…¡á„á…¡á„‹á…© á„‡á…®á„á…³á„á…¢á†·á„‘á…³]/Lenna.png")

def show_image(image, title = "image"):
        import matplotlib.pyplot as plt
        plt.imshow(image)
        plt.title(title)
        plt.axis('off')
        plt.show()

show_image(image)

#3 ì´ë¯¸ì§€ ì¦ê°• ê¸°ë²• ì ìš©

# ì´ë¯¸ì§€ íšŒì „

def rotate_image(image, angle):
    return image.rotate(angle)

rotated_image = rotate_image(image, 45)
show_image(rotated_image, "Rotated Image")

# 3 ì´ë¯¸ì§€ ì¦ê°• ê¸°ë²• ì ìš©

#ì´ë¯¸ì§€ ë’¤ì§‘ê¸°

from PIL import ImageOps

def flip_image(image,direction):
    if direction == 'horizontal':
        return ImageOps.mirror(image)
    elif direction == 'vertical':
        return ImageOps.flip(image)

flipped_image = flip_image(image, 'horizontal')
show_image(flipped_image, "Flipped Image")

#3 ì´ë¯¸ì§€ ì¦ê°• ê¸°ë²• ì ìš©

#ì´ë¯¸ì§€ ì´ë™

def translate_image(image,x,y):
    array = np.array(image) # np.array() ì…ë ¥ ë°ì´í„°ë¥¼ Numpy ë°°ì—´ë¡œ ë³€í™˜
    translated_array = np.roll(array, shift = (y,x), axis=(0,1))
    return Image.fromarray(translated_array)

translated_image = translate_image(image, 30, 50)
show_image(translated_image, "Translated Image")

#3 ì´ë¯¸ì§€ ì¦ê°• ê¸°ë²•

#ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •

def resize_image(image, scale):
    width , height = image.size
    return image.resize((int(width*scale), int(height*scale)))

resized_image = resize_image(image, 1.5) # 1.5ë°° í™•ëŒ€
resized_image

#3 ì´ë¯¸ì§€ ì¦ê°• ê¸°ë²•

#ì´ë¯¸ì§€ ë°ê¸° ì¡°ì ˆ

from PIL import ImageEnhance

def adjust_brightness(image, factor):
    enhancer = ImageEnhance.Brightness(image)
    return enhancer.enhance(factor)

brightened_image = adjust_brightness(image, 1.5) #1.5ë°° ë°ê¸°
brightened_image

#3 ì´ë¯¸ì§€ ì¦ê°• ê¸°ë²•

#ì´ë¯¸ì§€ì— ë…¸ì´ì¦ˆ ì¶”ê°€

def add_noise(image):
    array = np.array(image)
    noise = np.random.normal(loc = 0, scale = 10, size = array.shape)
    # ë…¸ì´ì¦ˆì˜ í‰ê· ê°’ = 0, ë…¸ì´ì¦ˆì˜ í‰ê· í¸ì°¨ = 10
    noisy_image = np.clip(array + noise , 0, 255).astype(np.uint8)
    return Image.fromarray(noisy_image)
    # ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ë°°ì—´ì„ ë‹¤ì‹œ ì´ë¯¸ì§€ ê°ì²´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜

noisy_image = add_noise(image)
#ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì´ë¯¸ì§€ë¥¼ í™”ë©´ì— í‘œì‹œí•œë‹¤.
show_image(noisy_image, "Noisy Image")

"""# ë°ì´í„°ì…‹ ë¶„í• 

<aside>
ğŸ‘‰ ë°ì´í„°ì…‹ì„ ëœë¤ìœ¼ë¡œ ë¶„í• í•˜ê³  ê° ë°ì´í„°ì…‹ì˜ í¬ê¸°ë¥¼ ì¶œë ¥í•˜ì—¬ ë¶„í• ì´ ì œëŒ€ë¡œ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”.

### ë¬¸ì œ ì„¤ëª…

1. ë°ì´í„° ë¡œë“œ
    - ìƒ˜í”Œ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.
    - ì˜ˆì œë°ì´í„°
    
    ```python
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    data = {
        'feature1': range(1, 101),
        'feature2': range(101, 201),
        'label': [1 if x % 2 == 0 else 0 for x in range(1, 101)]
    }
    df = pd.DataFrame(data)
    ```
    
2. ë°ì´í„°ì…‹ ë¶„í• 
    - ë°ì´í„°ë¥¼ í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
3. ê²°ê³¼ í™•ì¸
    - ê° ë°ì´í„°ì…‹ì˜ í¬ê¸°ë¥¼ ì¶œë ¥í•˜ì—¬ ë¶„í•  ê²°ê³¼ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
</aside>
"""

#1

#ë°ì´í„° ë¡œë“œ

data = {
    'feature1': range(1, 101),
    'feature2': range(101, 201),
    'label': [1 if x % 2 == 0 else 0 for x in range(1, 101)]
}

df = pd.DataFrame(data)
df

#2

#ë°ì´í„° ì…‹ ë¶„í• (íŠ¸ë ˆì´ë‹)

train_size = int(0.6 * len(df)) # 60%í•™ìŠµë°ì´í„° ë¹„ìœ¨

train_data = df[:train_size] # íŠ¸ë ˆì´ë‹ ë°ì´í„°

print("íŠ¸ë ˆì´ë‹ ë°ì´í„° í¬ê¸°:", len(train_data))

#2

#ë°ì´í„° ì…‹ ë¶„í• (ê²€ì •)

validate_size = int(0.2 * len(df)) # 20%í•™ìŠµë°ì´í„° ë¹„ìœ¨

validate_data = df[train_size: train_size + validate_size] # ê²€ì¦ ë°ì´í„°

print("ê²€ì¦ ë°ì´í„° í¬ê¸°:", len(validate_data))

#2

#ë°ì´í„° ì…‹ ë¶„í• (í•™ìŠµ)

test_size = int(0.2 * len(df)) # 40%í•™ìŠµë°ì´í„° ë¹„ìœ¨

test_data = df[train_size + validate_size:] # í…ŒìŠ¤íŠ¸ ë°ì´í„°

print("í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°:", len(test_data))

#3

#ê²°ê³¼ í™•ì¸

from sklearn.model_selection import train_test_split

train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)

print("íŠ¸ë ˆì´ë‹ ë°ì´í„°:",train_data)
print("í…ŒìŠ¤íŠ¸ ë°ì´í„°:",test_data)

"""#ë¨¸ì‹ ëŸ¬ë‹

<aside>
ğŸ‘‰ **ê°„ë‹¨í•œ ë¶„ë¥˜ ë¬¸ì œë¥¼ í•´ê²°í•´ë³´ì„¸ìš”.**

### ë¬¸ì œ ì„¤ëª…

1. **ë°ì´í„° ìˆ˜ì§‘**
    - ê°„ë‹¨í•œ ê½ƒ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    ê° ê½ƒì˜ íŠ¹ì„±(ì˜ˆ: ê¸¸ì´ì™€ ë„ˆë¹„)ê³¼ í´ë˜ìŠ¤(ì˜ˆ: ê½ƒì˜ ì¢…ë¥˜)ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
    - ì˜ˆì‹œ ë°ì´í„°
        
        ```python
        X = np.array([[1.4, 0.2], [1.3, 0.2], [1.5, 0.2], [4.5, 1.5], [4.1, 1.0], [5.1, 1.8]])
        y = np.array([0, 0, 0, 1, 1, 1])
        ```
        
2. **ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ**
    - k-ìµœê·¼ì ‘ ì´ì›ƒ(K-Nearest Neighbors, KNN) ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    - ê½ƒ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ê½ƒì˜ ì¢…ë¥˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
    
    <aside>
    â“ k-ìµœê·¼ì ‘ ì´ì›ƒ(K-Nearest Neighbors, KNN) ì•Œê³ ë¦¬ì¦˜ì´ë€?
    
        - K-ìµœê·¼ì ‘ ì´ì›ƒ(KNN)ì€ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ê¸°ì¡´ ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ Kê°œì˜ ë°ì´í„°ë¡œë¶€í„° ê·¸ë£¹ì´ë‚˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•
    </aside>
    
3. **í‰ê°€ ë° ê²€ì¦**
    - í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ê½ƒ ë°ì´í„°ì˜ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    - ì˜ˆì¸¡ê°’ì„ ì‹¤ì œê°’ê³¼ ë¹„êµí•˜ì—¬ ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
4. **ì˜ˆì¸¡ ë° ìµœì í™”**
    - ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ê°’ì„ ì¶œë ¥í•©ë‹ˆë‹¤.
</aside>
"""

#1
#ë°ì´í„°ìˆ˜ì§‘

x = np.array([[1.4, 0.2], [1.3, 0.2], [1.5, 0.2], [4.5, 1.5], [4.1, 1.0], [5.1, 1.8]])
y = np.array([0, 0, 0, 1, 1, 1])

#2
#ë°ì´í„° ë¶„í• 

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

#3
#ëª¨ë¸ì„ íƒ ë° í•™ìŠµ

knn = KNeighborsClassifier(n_neighbors=1) # k= 3->1ë¡œ ì¡°ì • : knnì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ê°€ ëœë‹¤!
#ë°ì´í„°ê°€ í¬ë©´ kì˜ ìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ ëœë‹¤. ê·¼ë° ì—¬ê¸°ì„œ kê°’ì„ í™€ìˆ˜ë¡œ í•œ ì´ìœ ê°€ ë”°ë¡œ ìˆë‹¤.
#'ë™ì íšŒí”¼' :  1:2 or 2:1ë¡œ ë‚˜ì˜¤ë¯€ë¡œ í•œìª½ í´ë˜ìŠ¤ë¡œ ê²°ì •í•˜ì—¬ ì˜ˆì¸¡ì˜ ì¼ê´€ì„±ì„ ë†’ì¸ë‹¤.
knn.fit(x_train, y_train) # ëª¨ë¸í•™ìŠµ

#4
#ì˜ˆì¸¡ ë° ìµœì í™”

y_pred = knn.predict(x_test) # ì˜ˆì¸¡
print("ì˜ˆì¸¡ê°’:", y_pred)

accuracy = accuracy_score(y_test, y_pred) #ì •í™•ë„ê³„ì‚°
print("ì •í™•ë„:", accuracy) # 0~1 ì‚¬ì´ ë²”ìœ„ì´ë©° 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ëª¨ë¸ì„±ëŠ¥ì´ ì¢‹ìŒ

# 5
# ìƒˆë¡œìš´ ë°ì´í„° ì˜ˆì¸¡

new_flower = np.array([[1.5, 0.2], [4.0, 1.5]])  # ìƒˆë¡œìš´ ê½ƒ ë°ì´í„°
predictions = knn.predict(new_flower)  # ì˜ˆì¸¡
print("ìƒˆë¡œìš´ ê½ƒì˜ ì˜ˆì¸¡ í´ë˜ìŠ¤:", predictions)

"""- knn : x,y í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ë¹„ìœ¨ì„ì •í•˜ê³  (ë°ì´í„° ë¶„í• ) -> x_testë°ì´í„°ë¡œ yì˜ ì˜ˆì¸¡ê°’ì„ ë„ì¶œí•´ë³¸ë‹¤ìŒ -> í•´ë‹¹ y ì˜ˆì¸¡ê°’ì´ ì‹¤ì œ y_testê°’ê³¼ ë¹„êµí•˜ì—¬ ëª¨ë¸ ì •í™•ë„ë¥¼ ë¹„êµí•œë‹¤.
- `Expected n_neighbors <= n_samples_fit, but n_neighbors = 5, n_samples_fit = 4, n_samples = 2` -> ë°ì´í„° ìƒ˜í”Œ ìˆ˜ë³´ë‹¤ kê°’ì´ ì»¤ì„œ ì˜¤ë¥˜ê°€ ìƒê¸´ë‹¤.(kê°’ì„ ë‚®ì¶”ì í™€ìˆ˜ê°’ìœ¼ë¡œ)

# ë”¥ëŸ¬ë‹_ë¯¸ë‹ˆí€˜ìŠ¤íŠ¸

<aside>
ğŸ‘‰ ì‹¤ìŠµ í”„ë¡œì íŠ¸ ì½”ë“œì—ì„œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µ ìˆ˜ì™€ ê° ì¸µì˜ ë‰´ëŸ° ìˆ˜ë¥¼ ë³€ê²½í•´ ë³´ê³ , ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ ë¹„êµí•´ë³´ì„¸ìš”.


XOR ë°ì´í„°ë¥¼ í™œìš©í•©ë‹ˆë‹¤.

```python
# XOR ë°ì´í„° ì •ì˜
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])
```

1. **ê¸°ë³¸ ëª¨ë¸ êµ¬ì¡°**
    - ì€ë‹‰ì¸µ 1ê°œ, ë‰´ëŸ° ìˆ˜ 2ê°œ (ì‚¬ìš© ì˜ˆì œ ì½”ë“œì˜ ê¸°ë³¸ êµ¬ì¡°)
2. **ë³€ê²½ ëª¨ë¸ êµ¬ì¡°**
    - ì€ë‹‰ì¸µ 2ê°œ, ì²« ë²ˆì§¸ ì¸µ ë‰´ëŸ° ìˆ˜ 4ê°œ, ë‘ ë²ˆì§¸ ì¸µ ë‰´ëŸ° ìˆ˜ 2ê°œ
        
        ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-02-12 10.52.18.png](attachment:0933d57b-977d-4563-a6e8-d3188005fd34:á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º_2025-02-12_10.52.18.png)
        
        ```python
        ì…ë ¥ì¸µ:                   ì€ë‹‰ì¸µ1:                 ì€ë‹‰ì¸µ2:           ì¶œë ¥ì¸µ:
           (x1) ----o--> [ ë‰´ëŸ°1 (ReLU) ] ----o--> [ ë‰´ëŸ°5 (ReLU) ] ----o---+
                                      ^                  ^                  |
                                      |                  |                  v
                                      o                  o              [ ë‰´ëŸ°7 (Sigmoid) ] ---> (y)
                                      |                  |                  ^
           (x2) ----o--> [ ë‰´ëŸ°2 (ReLU) ] ----o--> [ ë‰´ëŸ°6 (ReLU) ] ----o---+ |
                                      ^                  |                  
                                      |                  |                  
                                      o                  o                  
                                      |                  |                  
                          [ ë‰´ëŸ°3 (ReLU) ] ---------------+                  
                                      |                                    
                                      |                                    
                          [ ë‰´ëŸ°4 (ReLU) ]                                    
        
        ```
        
    - ì€ë‹‰ì¸µ 1ê°œ, ë‰´ëŸ° ìˆ˜ 4ê°œ
        
        ![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-02-12 10.53.05.png](attachment:f200f1ee-3668-42e6-b775-a22ed7c5e8d9:á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º_2025-02-12_10.53.05.png)
        
        ```python
        ì…ë ¥ì¸µ:                   ì€ë‹‰ì¸µ:               ì¶œë ¥ì¸µ:
        
          (x1) ----o--> [ ë‰´ëŸ°1 (ReLU) ] ----o---+
                                                 |
          (x2) ----o--> [ ë‰´ëŸ°2 (ReLU) ] ----o---+
                                                 |
          (x3) ----o--> [ ë‰´ëŸ°3 (ReLU) ] ----o---+
                                                 |
          (x4) ----o--> [ ë‰´ëŸ°4 (ReLU) ] ----o---+
                                                 |
                                                 v
                                             [ ë‰´ëŸ°5 (Sigmoid) ] ---> (y)
        
        ```
        
3. ì˜ˆì œ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹ ê²½ë§ ëª¨ë¸ì„ ë‘ ê°€ì§€ ë‹¤ë¥¸ êµ¬ì¡°ë¡œ ë³€ê²½í•´ ë´…ë‹ˆë‹¤.
4. ê°ê°ì˜ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  í‰ê°€í•˜ì—¬, ì„±ëŠ¥ ì°¨ì´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
</aside>

### ë‰´ëŸ°ìˆ˜ 2ê°œ, ì€ë‹‰ì¸µ 1ê°œ
"""

#ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë°‘ ë°ì´í„° ì •ì˜
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# XOR ë°ì´í„° ì •ì˜
x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

#Tensorë¡œ ë³€í™˜
x = torch.tensor(x, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

print("X:", x)
print("Y:", y)

class XORModel(nn.Module):
    def __init__(self):
        super(XORModel, self).__init__()
        self.layer1 = nn.Linear(2, 2) #ì€ë‹‰ì¸µì„ (ì…ë ¥+ì¶œë ¥ í†µí‹€ì–´ì„œã…‡ã…‡)
        self.layer2 = nn.Linear(2, 1) #ì¶œë ¥ì¸µ
        self.relu = nn.ReLU() #ReLu í™œì„±í™”
        self.sigmoid = nn.Sigmoid()

    def forward(self, k):
        k = self.relu(self.layer1(k))
        k = self.sigmoid(self.layer2(k)) #ë‘ë²ˆì§¸ ë ˆì´ì–´ì— ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜
        # ReLU í•¨ìˆ˜ëŠ” ì€ë‹‰ì¸µì—ì„œ ì„ í˜•ê´€ê³„ì˜ ì…ë ¥ê°’ì„ ë¹„ì„ í˜•ê²°í•©ìœ¼ë¡œ ì¬ê²°í•©í•´ì„œ ëª¨ë¸ë§ì„ í•¨
        # Sigmoid í•¨ìˆ˜ëŠ” ë³´í†µ ì¶œë ¥ì¸µì— ì ìš©í•˜ë©°, ì…ë ¥ ë°ì´í„°ì˜ í™•ë¥ ê°’ì´ ì¶œë ¥ë¨
        return k
    # ReLU í•¨ìˆ˜ëŠ” ì€ë‹‰ì¸µì—ì„œ ì„ í˜•ê´€ê³„ì˜ ì…ë ¥ê°’ì„ ë¹„ì„ í˜•ê²°í•©ìœ¼ë¡œ ì¬ê²°í•©í•´ì„œ ëª¨ë¸ë§ì„ í•¨
        # Sigmoid í•¨ìˆ˜ëŠ” ë³´í†µ ì¶œë ¥ì¸µì— ì ìš©í•˜ë©°, ì…ë ¥ ë°ì´í„°ì˜ í™•ë¥ ê°’ì´ ì¶œë ¥ë¨
model = XORModel() #ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

#ëª¨ë¸ ì»´íŒŒì¼
criterion = nn.BCELoss()  # Binary_Cross_Entropy_Loss : ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •
optimizer = optim.Adam(model.parameters(), lr=0.01) #Adaptive Moment Estimation,
#í•™ìŠµë¥ ì„ 0.01ë¡œ ì„¤ì • (í•œë²ˆ í• ë•Œ ê°€ì¤‘ì¹˜ë¥¼ ì–¼ë§Œí¼ ì¡°ì •í• ê±´ì§€ ìˆ˜ì¹˜í™”)

#ëª¨ë¸ í•™ìŠµ
num_epochs = 1000  # ì´ ì—í¬í¬ ìˆ˜

for epoch in range(num_epochs):
    model.train()  # ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
    optimizer.zero_grad()  # ì˜µí‹°ë§ˆì´ì €ì˜ ë³€í™”ë„(gradient)ë¥¼ ì´ˆê¸°í™”
    outputs = model(x)  # ëª¨ë¸ì— ì…ë ¥ ë°ì´í„°ë¥¼ ë„£ì–´ ì¶œë ¥ ê³„ì‚°
    loss = criterion(outputs, y)  # ì¶œë ¥ê³¼ ì‹¤ì œ ë ˆì´ë¸”ì„ ë¹„êµí•˜ì—¬ ì†ì‹¤ ê³„ì‚°
    loss.backward()  # ì—­ì „íŒŒë¥¼ í†µí•´ ì†ì‹¤ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    optimizer.step()  # ì˜µí‹°ë§ˆì´ì €ê°€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸

    if (epoch + 1) % 100 == 0:  # 100 ì—í¬í¬ë§ˆë‹¤ ì†ì‹¤ ì¶œë ¥
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

#ëª¨ë¸ í‰ê°€
model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
with torch.no_grad():  # í‰ê°€ ì¤‘ì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠìŒ
    outputs = model(x)  # ëª¨ë¸ì— ì…ë ¥ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ì—¬ ì¶œë ¥ê°’ ê³„ì‚°
    predicted = (outputs > 0.5).float()  # ì¶œë ¥ê°’ì´ 0.5ë³´ë‹¤ í¬ë©´ 1, ì•„ë‹ˆë©´ 0ìœ¼ë¡œ ë³€í™˜ (ì´ì§„ ë¶„ë¥˜)
    accuracy = (predicted == y).float().mean()  # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ ê³„ì‚°
    loss = criterion(outputs, y)  # ì†ì‹¤ í•¨ìˆ˜(í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ ê³„ì‚°
    print(f'Loss: {loss.item()}, Accuracy: {accuracy.item()}')  # ì†ì‹¤ê³¼ ì •í™•ë„ ì¶œë ¥
    print(f'Predicted: {predicted.squeeze().numpy()}')

"""### ì€ë‹‰ì¸µ 2ê°œ, ì²« ë²ˆì§¸ ì¸µ ë‰´ëŸ° ìˆ˜ 4ê°œ, ë‘ ë²ˆì§¸ ì¸µ ë‰´ëŸ° ìˆ˜ 2ê°œ"""

# ì€ë‹‰ì¸µ 2ê°œ, ì²« ë²ˆì§¸ ì¸µ ë‰´ëŸ° ìˆ˜ 4ê°œ, ë‘ ë²ˆì§¸ ì¸µ ë‰´ëŸ° ìˆ˜ 2ê°œ
#ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë°‘ ë°ì´í„° ì •ì˜
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# XOR ë°ì´í„° ì •ì˜
x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

#Tensorë¡œ ë³€í™˜
x = torch.tensor(x, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

print("X:", x)
print("Y:", y)

class XORModel(nn.Module):
    def __init__(self):
        super(XORModel, self).__init__()
        self.layer1 = nn.Linear(2, 4) #ì€ë‹‰ì¸µ1
        # xë°ì´í„°ê°€ 2ì°¨ì› ì´ë¯€ë¡œ `nn.Linear(2, 4)`ë¡œ ì„¤ì •
        self.layer2 = nn.Linear(4, 2) #ì€ë‹‰ì¸µ2
        self.layer3 = nn.Linear(2, 1) #ì¶œë ¥ì¸µ
        # ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ìˆ˜ê°€ 2ê°œì¸ ê²½ìš°, ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ 1ê°œì˜ ë‰´ëŸ°ì„ ê°–ëŠ” ê²ƒì´ ì¼ë°˜ì 
        self.relu = nn.ReLU() #ReLu í™œì„±í™”
        self.sigmoid = nn.Sigmoid()

    def forward(self, k):
        k = self.relu(self.layer1(k))
        k = self.relu(self.layer2(k))
        k = self.sigmoid(self.layer3(k)) #ì„¸ë²ˆì§¸ ë ˆì´ì–´ì— ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜
        # ReLU í•¨ìˆ˜ëŠ” ì€ë‹‰ì¸µì—ì„œ ì„ í˜•ê´€ê³„ì˜ ì…ë ¥ê°’ì„ ë¹„ì„ í˜•ê²°í•©ìœ¼ë¡œ ì¬ê²°í•©í•´ì„œ ëª¨ë¸ë§ì„ í•¨
        # Sigmoid í•¨ìˆ˜ëŠ” ë³´í†µ ì¶œë ¥ì¸µì— ì ìš©í•˜ë©°, ì…ë ¥ ë°ì´í„°ì˜ í™•ë¥ ê°’ì´ ì¶œë ¥ë¨
        return k
    # ReLU í•¨ìˆ˜ëŠ” ì€ë‹‰ì¸µì—ì„œ ì„ í˜•ê´€ê³„ì˜ ì…ë ¥ê°’ì„ ë¹„ì„ í˜•ê²°í•©ìœ¼ë¡œ ì¬ê²°í•©í•´ì„œ ëª¨ë¸ë§ì„ í•¨
        # Sigmoid í•¨ìˆ˜ëŠ” ë³´í†µ ì¶œë ¥ì¸µì— ì ìš©í•˜ë©°, ì…ë ¥ ë°ì´í„°ì˜ í™•ë¥ ê°’ì´ ì¶œë ¥ë¨
model = XORModel() #ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

#ëª¨ë¸ ì»´íŒŒì¼
criterion = nn.BCELoss()  # Binary_Cross_Entropy_Loss : ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •
optimizer = optim.Adam(model.parameters(), lr=0.01) #Adaptive Moment Estimation,
#í•™ìŠµë¥ ì„ 0.01ë¡œ ì„¤ì • (í•œë²ˆ í• ë•Œ ê°€ì¤‘ì¹˜ë¥¼ ì–¼ë§Œí¼ ì¡°ì •í• ê±´ì§€ ìˆ˜ì¹˜í™”)

#ëª¨ë¸ í•™ìŠµ
num_epochs = 1000  # ì´ ì—í¬í¬ ìˆ˜

for epoch in range(num_epochs):
    model.train()  # ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
    optimizer.zero_grad()  # ì˜µí‹°ë§ˆì´ì €ì˜ ë³€í™”ë„(gradient)ë¥¼ ì´ˆê¸°í™”
    outputs = model(x)  # ëª¨ë¸ì— ì…ë ¥ ë°ì´í„°ë¥¼ ë„£ì–´ ì¶œë ¥ ê³„ì‚°
    loss = criterion(outputs, y)  # ì¶œë ¥ê³¼ ì‹¤ì œ ë ˆì´ë¸”ì„ ë¹„êµí•˜ì—¬ ì†ì‹¤ ê³„ì‚°
    loss.backward()  # ì—­ì „íŒŒë¥¼ í†µí•´ ì†ì‹¤ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    optimizer.step()  # ì˜µí‹°ë§ˆì´ì €ê°€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸

    if (epoch + 1) % 100 == 0:  # 100 ì—í¬í¬ë§ˆë‹¤ ì†ì‹¤ ì¶œë ¥
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

#ëª¨ë¸ í‰ê°€
model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
with torch.no_grad():  # í‰ê°€ ì¤‘ì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠìŒ
    outputs = model(x)  # ëª¨ë¸ì— ì…ë ¥ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ì—¬ ì¶œë ¥ê°’ ê³„ì‚°
    predicted = (outputs > 0.5).float()  # ì¶œë ¥ê°’ì´ 0.5ë³´ë‹¤ í¬ë©´ 1, ì•„ë‹ˆë©´ 0ìœ¼ë¡œ ë³€í™˜ (ì´ì§„ ë¶„ë¥˜)
    accuracy = (predicted == y).float().mean()  # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ ê³„ì‚°
    loss = criterion(outputs, y)  # ì†ì‹¤ í•¨ìˆ˜(í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ ê³„ì‚°
    print(f'Loss: {loss.item()}, Accuracy: {accuracy.item()}')  # ì†ì‹¤ê³¼ ì •í™•ë„ ì¶œë ¥
    print(f'Predicted: {predicted.squeeze().numpy()}')

"""### ì€ë‹‰ì¸µ 1ê°œ, ë‰´ëŸ° ìˆ˜ 4ê°œ"""

#ì€ë‹‰ì¸µ 1ê°œ, ë‰´ëŸ°ìˆ˜ 4ê°œ
#ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ë°‘ ë°ì´í„° ì •ì˜
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# XOR ë°ì´í„° ì •ì˜
x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([[0], [1], [1], [0]])

#Tensorë¡œ ë³€í™˜
x = torch.tensor(x, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32)

print("X:", x)
print("Y:", y)

class XORModel(nn.Module):
    def __init__(self):
        super(XORModel, self).__init__()
        self.layer1 = nn.Linear(2, 4) #ì€ë‹‰ì¸µ1
        # xë°ì´í„°ê°€ 2ì°¨ì› ì´ë¯€ë¡œ `nn.Linear(2, 4)`ë¡œ ì„¤ì •
        self.layer2 = nn.Linear(4, 1) #ì¶œë ¥ì¸µ
        # ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ìˆ˜ê°€ 2ê°œì¸ ê²½ìš°, ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ 1ê°œì˜ ë‰´ëŸ°ì„ ê°–ëŠ” ê²ƒì´ ì¼ë°˜ì 
        self.relu = nn.ReLU() #ReLu í™œì„±í™”
        self.sigmoid = nn.Sigmoid()

    def forward(self, k):
        k = self.relu(self.layer1(k))
        k = self.sigmoid(self.layer2(k)) #ë‘ë²ˆì§¸ ë ˆì´ì–´ì— ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜
        # ReLU í•¨ìˆ˜ëŠ” ì€ë‹‰ì¸µì—ì„œ ì„ í˜•ê´€ê³„ì˜ ì…ë ¥ê°’ì„ ë¹„ì„ í˜•ê²°í•©ìœ¼ë¡œ ì¬ê²°í•©í•´ì„œ ëª¨ë¸ë§ì„ í•¨
        # Sigmoid í•¨ìˆ˜ëŠ” ë³´í†µ ì¶œë ¥ì¸µì— ì ìš©í•˜ë©°, ì…ë ¥ ë°ì´í„°ì˜ í™•ë¥ ê°’ì´ ì¶œë ¥ë¨
        return k
    # ReLU í•¨ìˆ˜ëŠ” ì€ë‹‰ì¸µì—ì„œ ì„ í˜•ê´€ê³„ì˜ ì…ë ¥ê°’ì„ ë¹„ì„ í˜•ê²°í•©ìœ¼ë¡œ ì¬ê²°í•©í•´ì„œ ëª¨ë¸ë§ì„ í•¨
        # Sigmoid í•¨ìˆ˜ëŠ” ë³´í†µ ì¶œë ¥ì¸µì— ì ìš©í•˜ë©°, ì…ë ¥ ë°ì´í„°ì˜ í™•ë¥ ê°’ì´ ì¶œë ¥ë¨
model = XORModel() #ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

#ëª¨ë¸ ì»´íŒŒì¼
criterion = nn.BCELoss()  # Binary_Cross_Entropy_Loss : ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •
optimizer = optim.Adam(model.parameters(), lr=0.01) #Adaptive Moment Estimation,
#í•™ìŠµë¥ ì„ 0.01ë¡œ ì„¤ì • (í•œë²ˆ í• ë•Œ ê°€ì¤‘ì¹˜ë¥¼ ì–¼ë§Œí¼ ì¡°ì •í• ê±´ì§€ ìˆ˜ì¹˜í™”)

#ëª¨ë¸ í•™ìŠµ
num_epochs = 1000  # ì´ ì—í¬í¬ ìˆ˜

for epoch in range(num_epochs):
    model.train()  # ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì •
    optimizer.zero_grad()  # ì˜µí‹°ë§ˆì´ì €ì˜ ë³€í™”ë„(gradient)ë¥¼ ì´ˆê¸°í™”
    outputs = model(x)  # ëª¨ë¸ì— ì…ë ¥ ë°ì´í„°ë¥¼ ë„£ì–´ ì¶œë ¥ ê³„ì‚°
    loss = criterion(outputs, y)  # ì¶œë ¥ê³¼ ì‹¤ì œ ë ˆì´ë¸”ì„ ë¹„êµí•˜ì—¬ ì†ì‹¤ ê³„ì‚°
    loss.backward()  # ì—­ì „íŒŒë¥¼ í†µí•´ ì†ì‹¤ì— ëŒ€í•œ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
    optimizer.step()  # ì˜µí‹°ë§ˆì´ì €ê°€ ë§¤ê°œë³€ìˆ˜ë¥¼ ì—…ë°ì´íŠ¸

    if (epoch + 1) % 100 == 0:  # 100 ì—í¬í¬ë§ˆë‹¤ ì†ì‹¤ ì¶œë ¥
        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

#ëª¨ë¸ í‰ê°€
model.eval()  # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
with torch.no_grad():  # í‰ê°€ ì¤‘ì—ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠìŒ
    outputs = model(x)  # ëª¨ë¸ì— ì…ë ¥ ë°ì´í„°ë¥¼ ì „ë‹¬í•˜ì—¬ ì¶œë ¥ê°’ ê³„ì‚°
    predicted = (outputs > 0.5).float()  # ì¶œë ¥ê°’ì´ 0.5ë³´ë‹¤ í¬ë©´ 1, ì•„ë‹ˆë©´ 0ìœ¼ë¡œ ë³€í™˜ (ì´ì§„ ë¶„ë¥˜)
    accuracy = (predicted == y).float().mean()  # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ ê³„ì‚°
    loss = criterion(outputs, y)  # ì†ì‹¤ í•¨ìˆ˜(í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì†ì‹¤ ê³„ì‚°
    print(f'Loss: {loss.item()}, Accuracy: {accuracy.item()}')  # ì†ì‹¤ê³¼ ì •í™•ë„ ì¶œë ¥
    print(f'Predicted: {predicted.squeeze().numpy()}')

"""- ì€ë‹‰ì¸µì´ ë§ì•„ì•¼ ë³µì¡í•œ ëª¨ë¸ë§ì¼ìˆ˜ë¡ ì„±ëŠ¥ì´ ì¢‹ì•„ì§„ë‹¤.
- í•˜ì§€ë§Œ ì€ë‹‰ì¸µì´ ê³¼ë‹¤í•˜ë©´ í•™ìŠµì´ ì–´ë ¤ì›Œì§ˆìˆ˜ ìˆë‹¤.(ê¸°ìš¸ê¸° ì†ì‹¤ ë¬¸ì œ)
    > ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ê¸°ìš¸ê¸°(gradient)ê°€ ì ì  ì‘ì•„ì§€ëŠ” í˜„ìƒ
    - ì‹œê·¸ëª¨ì´ë“œ(sigmoid) & Tanh í•¨ìˆ˜ì™€ ê°™ì€ 'ë¹„ì„ í˜• í™œì„±í™” í•¨ìˆ˜'ì—ì„œ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œê°€ ë°œìƒí•¨(ì…ë ¥ê°’ì´ ê·¹ë‹¨ì ì¼ë•Œ)

- ë‰´ëŸ° ìˆ˜ê°€ ì ìœ¼ë©´ ëª¨ë¸ì˜ í‘œí˜„ë ¥ì´ ê°ì†Œí•œë‹¤.
- ê·¸ëŸ¬ë‚˜ ë„ˆë¬´ ë§ì€ ë‰´ëŸ°ì˜ ìˆ˜ëŠ” ê³¼ì í•©(overfitting)ì˜ ìœ„í—˜ì´ ë°œìƒí• ìˆ˜ ìˆë‹¤.
> ë“œë¡­ ì•„ì›ƒì´ë‚˜, ë°°ì¹˜ ì •ê·œí™”ë¥¼ í†µí•´ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒ ì‹œí‚¬ìˆ˜ ìˆë‹¤.
-  ë“œë¡­ì•„ì›ƒ : 'ë¬´ì‘ìœ„'ë¡œ 'ë‰´ëŸ°'ì„ ì—†ì• ëŠ”ê²ƒ -> í•´ë‹¹ ë‰´ëŸ°ì˜ ì¶œë ¥ê°’ì„ 0ìœ¼ë¡œ ì§€ì •
        - ì¼ë°˜ì ìœ¼ë¡œ ê° í›ˆë ¨ë‹¨ê³„ì—ì„œ 0.2~0.5 í™•ë¥ ë¡œ ë“œë
- ë°°ì¹˜ ì •ê·œí™” : ê° ë¯¸ë‹ˆë² ì¹˜ì— ëŒ€í•´ 'ì…ë ¥ ë°ì´í„°'ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ê³„ì‚°í•˜ì—¬ ì •ê·œí™” í•¨.(í‰ê· 0, ë¶„ì‚°1ì¸ ì •ê·œë¶„í¬)
    - ë‚´ë¶€ ê³µë³€ëŸ‰ ë³€í™”(Internal Covariate Shift)ë¥¼ ì¤„ì—¬ì¤€ë‹¤
    - ë‚´ë¶€ ê³µë³€ëŸ‰ ë³€í™”ë€?
    > í•™ìŠµì¤‘ ê°€ì¤‘ì¹˜ê°€ ì—…ë°ì´íŠ¸ ë˜ë©´, ê° ì¸µì˜ ì…ë ¥ ë°ì´í„°(=ì§ì „ ì¸µì˜ ì¶œë ¥ê°’) ë¶„í¬ê°€ ë³€í•˜ê²Œ ëœë‹¤. ì´ë ‡ê²Œ ë˜ë©´ ê° ëª¨ë¸ì¸µì´ í•™ìŠµí•´ì•¼í•˜ëŠ” ë¶„í¬ê°€ ê³„ì† ë³€í™”í•˜ì—¬ í›ˆë ¨ì†ë„ì™€ ìµœì í™”ê°€ ì–´ë ¤ì›Œì§ˆìˆ˜ ìˆë‹¤.
    - ì‹ ê²½ë§ í•™ìŠµ ê³¼ì •ì¤‘ 'ì—­ì „íŒŒê³¼ì •'ì—ì„œ ë°œìƒ. -> ê³„ì† ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸ í•˜ë¯€ë¡œ
"""

